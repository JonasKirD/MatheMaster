% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{Der multivariate zentrale Grenzwertsatz (ZGWS) für Dreiecksschemata} %6
Wir betrachen zunächst den \underline{univariaten} Fall. 
Es gelte
\begin{align}\label{eq6.1}\tag{6.1}
	X_{n,1},X_{n,2},\ldots,X_{n,n}\text{ sind unabhängige \ul{reelle} ZV }\forall n\in\N
\end{align}
Die Kollektion
\begin{align*}
	\big\lbrace X_{n,k}:1\leq k\leq n,n\in\N\rbrace
\end{align*}
heißt \textbf{Dreiecksschema / $\Delta$-Schema}.
\begin{align*}
	\begin{matrix}
		X_{1,1}\\
		X_{2,1} & X_{2,2}\\
		X_{3,1} & X_{3,2} & X_{3,3}\\
		\vdots & \vdots & \vdots & \ddots\\
		X_{n,1} & X_{n,2} & \hdots & \hdots & X_{n,n}\\
		\vdots &&&&\vdots & \ddots
	\end{matrix}
\end{align*}

Sei 
\begin{align}\label{eq6.2}\tag{6.2}
	\E[X_{n,k}]=0,~\sigma_{n,k}^2:=\E\left[X_{n,k}^2\right]<\infty~\forall n,k\in\N\text{ und }
	s_n^2:=\sum\limits_{k=1}^n\sigma_{n,k}^2~\forall n\in\N
\end{align}

\begin{satz}[Lindeberg, 1922]\label{satz6.1Lindeberg1922}\enter
	Es gelten \eqref{eq6.1} und \eqref{eq6.2} sowie
	\begin{align}\label{eqSatz6.1LindebergLB}\tag{LB}
		\sum\limits_{k=1}^n\E\left[X_{n,k}^2\cdot\indi_{\big\lbrace|X_{n,k}|>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0.
	\end{align}
	Falls zusätzlich
	\begin{align*}
		s_n^2\stackrel{n\to\infty}{\longrightarrow}\sigma^2\in(0,\infty)
	\end{align*}
	gilt, so gilt:
	\begin{align*}
		\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)
	\end{align*}
\end{satz}

\begin{bemerkung}
	\begin{align*}
		s_n^2=\Var\left(\sum\limits_{k=1}^n X_{n,k}\right)
	\end{align*}
\end{bemerkung}

\begin{proof}
	So ähnlich wie Beweis von klassischem zentralen Grenzwertsatz in der Vorlesung Wahrscheinlichkeitstheorie (Bachelor), nur technisch etwas komplizierter. 
	Vergleiche auch Billingsley (1995), \textit{Probability and Measure}, Seite 359 ff.
\end{proof}

Im Folgenden betrachten wir die Verallgemeinerung auf den \underline{multivariaten} Fall.\\
Sei $\lbrace X_{n,k}:k\leq n,n\in\N\big\rbrace$ ein $\Delta$-Schema von Zufallsvariablen.
\begin{align*}
	X_{n,k}=\left(X_{n,k}^{(1)},\ldots,X_{n,k}^{(d)}\right)\text{ in }\R^d
\end{align*}
Es gelte die \textbf{zeilenweise Unabhängigkeit}:
\begin{align}\label{eq6.3}\tag{6.3}
	X_{n,1},\ldots,X_{n,n}\text{ sind unabhängig}\qquad\forall n\in\N
\end{align}
%Ferger arbeitet auch am Buß- und Bettag in der Uni. Er ist evangelisch getauft worden und sogar konfirmiert. 
Also die Vektoren seien unabhängig. Daraus folgt nicht, dass deren Komponenten unabhängig sind.
\begin{align}\label{eq6.4}\tag{6.4}
	\E\big[X_{n,k}\big]:=\left(\E\left[X_{n,k}^{(j)}\right]\right)_{1\leq j\leq d}&=0:=(0,\ldots,0)
	&\forall& k,n\in\N\\
	\label{eq6.5}\tag{6.5}
	\E\left[\left(X_{n,k}^{(j)}\right)\right]&<\infty
	&\forall& 1\leq j\leq d,\forall n,k\in\N
\end{align}
Wegen \eqref{eq6.4} und \eqref{eq6.5} ist die so genannte \textbf{Kovarianzmatrix}
\begin{align*}
	\Cov\left(X_{n,k}\right):=\Big(\underbrace{\Cov\left(X_{n,k}^{(i)},X_{n,k}^{(j)}\right)}_{
		=\E\left[X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]
	}\Big)_{1\leq i,j\leq d}\in\R^{d\times d}
\end{align*}

\begin{satz}[Multivariater Zentraler Grenzwertsatz (ZGWS)]\label{satz6.2MultivariaterZGWS}\enter
	Es gelten \eqref{eq6.3}, \eqref{eq6.4}, \eqref{eq6.5} sowie
	\begin{align}\label{eqSatz6.2LB}\tag{LB}
		\sum\limits_{k=1}^n\E\left[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
	\end{align}
	(Hierbei ist $\Vert\cdot\Vert$ die euklidische Norm auf $\R^d$.)\\
	Falls die Normierungsbedingung
	\begin{align}\label{eqSatz6.2NB}\tag{NB}
		\sum\limits_{k=1}^n\Cov\left(X_{n,k}\right)\stackrelnew{\text{komponentenweise}}{n\to\infty}{\longrightarrow}\Gamma\qquad\mit\Gamma\in\R^{d\times d}\text{ positiv definit}
	\end{align}
	erfüllt ist, so gilt:
	\begin{align*}
		\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)\text{ in }\R^d
	\end{align*}
\end{satz}
%Ferger kennt die Dichte die Normalverteilung im Mehrdimensionalen nicht und versucht sie trotzdem aus dem Gedächtnis an die Tafel zu schreiben. Wieder 5 Minuten weg :D
%"Mir persönlich reicht es, dass das Ding hat eine Dichte!" :D
%"Manchmal ist weniger mehr!"
\begin{proof}
	Sei $N\sim\mathcal{N}_d(0,\Gamma)$. Mit \ref{satz5.4CramerWoldDevice} bleibt zu zeigen:
	\begin{align}
		\label{eqProof6.2Stern}\tag{$\ast$}
		\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle\stackrel{\L}{\longrightarrow}\langle t,N\rangle\text{ in }\R\qquad\forall t\in\R^d
	\end{align}
	Sei $t\in\R^d\setminus\lbrace0\rbrace$ (für $t=0$ ist \eqref{eqProof6.2Stern} trivialerweise erfüllt). Es folgt
	\begin{align*}
		\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle
		\overset{\text{Lin}}&=
		\sum\limits_{k=1}^n\underbrace{\left\langle t,X_{n,k}\right\rangle}_{=:Y_{n,k}=:Y_{n,k}(t)}
		=\sum\limits_{k=1}^n Y_{n,k}\\
		Y_{n,k}
		&=\sum\limits_{k=1}^d t_j\cdot X_{n,k}^{(j)}
	\end{align*}
	Folglich erfüllt $\big\lbrace Y_{n,k}:1\leq k\leq n,n\in\N\big\rbrace$ die Bedingung \eqref{eq6.1} wegen Blockungslemma und \eqref{eq6.2} wegen \eqref{eq6.4} und Linearität sowie Cauchy-Schwarz-Ungleichung.
	%Hauptsatz des Mannschaftssport: "Jede Kombination von Nullen ist immer Null."
	\begin{align*}
		\big|Y_{n,k}^2\big|&=\big|\langle t,X_{n,k}\rangle\big|^2
		\leq\Vert t\Vert^2\cdot\Vert X_{n,k}\Vert^2
		=\Vert t\Vert^2\cdot\sum\limits_{j=1}^d\left(X_{n,k}^{(j)}\right)^2
	\end{align*}
	
	Da $\langle t,N\rangle=t\cdot N\sim\mathcal{N}(0,t\cdot\Gamma)$ (vgl. Klaus Schmidt \textit{Wahrscheinlichkeit und Maß}, Bsp. 13.2.2), folgt \eqref{eqProof6.2Stern}. 
	Der Nachweis von 
	\begin{align}\label{eqProof6.2SternStern}\tag{$\ast\ast$}
		\sum\limits_{k=1}^n Y_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,t'\cdot\Gamma\cdot t)
	\end{align}
	erfolgt mit Satz \ref{satz6.1Lindeberg1922}. 
	Es gilt:
	\begin{align*}
		s_n^2
		&=\sum\limits_{k=1}^n\E\left[Y_{n,k}^2\right]\\
		&=\sum\limits_{k=1}^n\E\left[\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
		&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot\E\left[X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
		&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot\Big(\Cov(X_{n,k}\Big)_{i,j}\cdot t_j\\
		&=\sum\limits_{k=1}^n t'\cdot\Cov(X_{n,k})\cdot t\\
		\overset{\text{Distri}}&=
		t'\cdot\underbrace{\sum\limits_{k=1}^n\Cov(X_{n,k})\cdot t}_{\stackrel{n\to\infty}{\longrightarrow}\Gamma\text{ wg. \eqref{eqSatz6.2NB}}}\stackrel{n\to\infty}{\longrightarrow} t'\cdot \Gamma\cdot t=:\sigma^2\stackrel{\Gamma\text{ p.d.}}{>}0
	\end{align*}
	Gemäß Cauchy-Schwarz-Ungleichung (CSU) gilt:
	\begin{align}\label{eqProof6.2t}\tag{t}
		&|Y_{n,k}|=\big|\langle t,X_{n,k}\rangle\big|\stackeq{\text{CSU}}{\leq}\Vert t\Vert\cdot\Vert X_{n,k}\Vert\\\nonumber
		&\implies\sum\limits_{k=1}^n\E\Big[\underbrace{|Y_{n,k}|^2}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}\cdot\indi_{\big\lbrace\underbrace{|Y_{n,k}|}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}>\varepsilon\big\rbrace}\Big]
		\stackrel{\eqref{eqProof6.2t}}{\leq}
		\Vert t\Vert^2\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\left\lbrace\Vert X_{n,k}>\frac{\varepsilon}{\Vert t\Vert}\right\rbrace}\Big]\stackrel{n\to\infty}{\longrightarrow}0
	\end{align}
	Dies gilt gemäß \ref{satz6.1Lindeberg1922} für alle $\varepsilon>0$. 
	Somit folgt aus Satz \ref{satz6.1Lindeberg1922} dann \eqref{eqProof6.2SternStern} und dadurch \eqref{eqProof6.2Stern} und somit die Behauptung.
\end{proof}

\begin{korollar}\label{korollar6.3}
	Sei $(X_i)_{i\in\N}$ mit $X_i$ iid Zufallsvariable in $\R^d$ mit
	\begin{align*}
		&\E\left[\left(X_1^{(j)}\right)^2\right]<\infty\qquad\forall 1\leq j\leq d\\
		\mu&:=\E\big[X_1\big]=\left(\E\left(X_1^{(1)}\right),\ldots,\E\left(X_1^{(d)}\right)\right)\in\R^d,\\
		\Gamma&:=\Cov(X_1)
		=\left(\Cov\left(X_1^{(i)},X_1^{(j)}\right)\right)_{i,j=1}^d
		=\left(\E\left[\left(X_1^{(i)}-\mu_i\right)\cdot\left(X_1^{(j)}-\mu_j\right)\right]\right)_{i,j=1}^d\\&\text{ positiv definit}
	\end{align*}
	Dann gilt:
	\begin{align*}
		\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n(X_i-\mu)\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)
	\end{align*}
\end{korollar}

\begin{proof}
	\begin{align}\label{eqProof6.3Stern}\tag{$\ast$}
		\frac{1}{\sqrt{n}}\cdot\sum\limits_{k=1}^n(X_k-\mu)
		&=\sum\limits_{k=1}^n\underbrace{\frac{1}{\sqrt{n}}\cdot(X_k-\mu)}_{=:X_{n,k}}\qquad\forall 1\leq k\leq n,\forall n\in\N
	\end{align}
	Dann sind \eqref{eq6.3}, \eqref{eq6.4} und \eqref{eq6.5} erfüllt und es gilt
	\begin{align*}
		\Cov(X_{n,k})
		&=\frac{1}{n}\cdot\Gamma\implies\sum\limits_{k=1}^n\Cov(X_{n,k})=\Gamma
	\end{align*}
	Somit ist \eqref{eqSatz6.2NB} erfüllt. Zu \eqref{eqSatz6.2LB}:
	\begin{align*}
		\sum\limits\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\Big]
		&\stackeq{\eqref{eqProof6.3Stern}}
		\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_k-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_k-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
		&\stackeq{\text{iid}}
		\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
		&=\E\Big[\underbrace{\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace \Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}}_{\stackrel{n\to\infty}{\longrightarrow}0~\forall\varepsilon>0}\Big]
		\stackrelnew{\text{domKonv}}{n\to\infty}{\longrightarrow}0
	\end{align*}
	Hierbei geht der Satz der dominierten Konvergenz ein, denn $\Vert X_1-\mu\Vert^2$ ist Dominante und integrierbar, da 
	\begin{align*}
		\Vert X_1-\mu\Vert^2\leq\big(\Vert X_1\Vert+\Vert\mu\Vert\big)^2\leq 2\cdot\left(\Vert X_1\Vert^2+\Vert\mu\Vert^2\right)
	\end{align*}
	und 
	\begin{align*}
		\E\left[\Vert X_1\Vert^2\right]
		&=\E\left[\sum\limits_{j=1}^d\left(X_1^{(j)}\right)^2\right]
		=\sum\limits_{j=1}^d\underbrace{\E\left[\left(X_1^{(j)}\right)^2\right]}_{<\infty~\forall j}<\infty
	\end{align*}
	Somit ist \eqref{eqSatz6.2LB} erfüllt und es folgt mit \ref{satz6.2MultivariaterZGWS} die Behauptung.
\end{proof}

Für $d=1$ liefert Korollar \ref{korollar6.3} den klassischen ZGWS.