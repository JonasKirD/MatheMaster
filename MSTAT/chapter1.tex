% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\addchap{Mathematische Statistik}
Viele Schätzer in der Statistik sind definiert als Minimal- oder Maximalstelle von bestimmten \textit{Kriteriumsfunktionen}, z. B. der \textit{Maximum-Likelihood-Schätzer (MLS)} oder \textit{Minimum-Qudrat-Schätzer (MQS, KQS)} oder \textit{Bayes-Schätzer}. Allgemein nennt man solche Schätzer \textbf{M-Schätzer}.\\
Ziel: Untersuchung des asymptotischen Verhaltens ($n\to\infty$) von M-Schätzern über einen \textit{funktionalen Ansatz}. Als Beispiel:

\section{Der Median}
Sei $X:(\Omega,\A,\P)\to\R$ eine reelle Zufallsvariable mit Verteilungsfunktion\\ $F_X:\R\to[0,1],~F_X(x):=\P[X\leq x]$, also $X\sim F_X$. Definiere

\begin{align}
	Y(t)&:=\E\left(|X-t|\right)\label{DefY}\tag{1.0}\\ \nonumber
	&=\int\limits_\Omega |X(\omega)-t|\d\P(\omega)\\ \nonumber
	\overset{\text{Trafo}}&=
	\int\limits_\R|x-t|\Big(\P\circ X\d x\Big)\\ \nonumber
	&=\int\limits_\R|x-t|(F\d x) \nonumber
	\qquad\forall t\in\R\\
	m&:=\argmin\limits_{t\in\R}Y(t):=\text{ (irgendeine) Minimalstelle der Funktion}\nonumber
\end{align} 

\begin{notation}
	$F(m-):=F(m-0):=\lim\limits_{t\uparrow m} F(t)$
\end{notation}

Charakterisierung der Menge aller Mediane in folgendem kleinen Lemma:

\begin{lemma}\label{lemmaMedian}
	Sei $X\sim F_X$ integrierbar und $m\in\R$. Dann äquivalent:
	\begin{enumerate}[label=(\alph*)]
		\item $F(m-)\leq\frac{1}{2}\leq F(m)$
		\item $\E[|X-t|]\geq\E[|X-m|]\qquad\forall t\in\R$
		\item $m$ ist Median
	\end{enumerate}
\end{lemma}

\begin{proof}
	\underline{Zeige (a) $\Rightarrow$ (b):}\\
	Setze $h(t):=\E[|X-t|-|X-m|]\stackeq{\text{Lin}}Y(t)-Y(m)$. Dann ist 2. äquivalent zu $h(t)\geq0~\forall t\in\R$. Dies ist noch zu zeigen.\nl
	\underline{Fall 1: $t<m$}
	\begin{align*}
		&h(t)\\
		\overset{\text{Trafo}}&=
		\int\limits_{\R}|x-t|-|x-m| Q_F(\d x)\\
		&=\int\limits_{(-\infty,t]}\underbrace{|x-t|-|x-m|}_{=t-x-(m-x)=-(m-t)} F(\d x)
		+\int\limits_{(t,m)}\underbrace{\underbrace{|x-t|-|x-m|}_{\underbrace{x-t}_{\geq0}-\underbrace{(m-x)}_{\leq m-t}}}_{\geq-(m-t)} F(\d x)\\
		&\qquad
		+\int\limits_{[m,\infty)}\underbrace{|x-t|-|x-m|}_{x-t-(x-m)=m-t} F(\d x)\\
		&\geq-(m-t)\cdot \underbrace{Q((-\infty,t])}_{F(t)}+
		\Big(-(m-t)\cdot F(m-)-F(t)\Big)
		+(m-t)\cdot\underbrace{Q([m,\infty))}_{1-\underbrace{Q((-m,m))}_{F(m-)}}\\
		&=-\underbrace{(m-t)}_{\geq0}\cdot(\underbrace{1-2\cdot F(m-)}_{\stackrel{1.}{\geq}0})\\
		&\geq0
	\end{align*}

	\underline{Fall 2: $t> m$}
	\begin{align*}
		h(t)&=\int\limits_{(-\infty,m]}\ldots F(\d x)+\int\limits_{(m,t]}\ldots+\int\limits_{(t,\infty)}\ldots F(\d x)\\
		&\ldots\\
		&\geq(t-m)\cdot(\underbrace{2\cdot F(m)-1}_{\stackrel{1.}{\geq}0})\\
		&\geq0
	\end{align*}

	\underline{Fall 3: $t=m$} ist trivial. $\#$\nl
	\underline{Zeige (b) $\Rightarrow$ (a):}\\
	Nach Annahme ist $h(t)\geq0~\forall t\in\R$.\nl
	\underline{Fall 1: $t<m$} Die obige Rechnung im Fall 1 bei 1. $\Rightarrow$ 2. zeigt:
	\begin{align*}
		0\leq h(t)&=-(m-t)\cdot F(t)+\int\limits_{(t,m)}\underbrace{\underbrace{x}_{<m}-t-(m-x) }_{=2x-t-m\leq m-t}F(\d x)+(m-t)\cdot(1-F(m-))\\
		&\leq-(m-t)\cdot\Big(F(t)-1\underbrace{+F(m-)-F(m-)}_{=0}+F(t)\Big)\\
		&=\underbrace{(m-t)}_{>0}\cdot(1-2\cdot F(t))\\
		&\Longrightarrow\forall t<m:0\leq(m-t)\cdot(1-2\cdot F(t))\\
		&\Longrightarrow\forall t<m:0\leq 1-2\cdot F(t)\\
		&\Longrightarrow\forall t<m:F(t)\leq\frac{1}{2}\\
		&\stackrel{t\uparrow m}{\Longrightarrow}F(m-)\leq\frac{1}{2}
	\end{align*}
	\underline{Fall 2: $t> m$} Siehe 2. Fall, analog.\nl
	\underline{Zeige (a) $\gdw$ (c):}
	(b) ist offensichtlich äquivalent zur Definition des Medians.
\end{proof}

\begin{bemerkungnr}\
	\begin{enumerate}
		\item Lemma \ref{lemmaMedian} (a) besagt, dass $\lbrace m\in\R: m\text{ erfüllt } 1.\rbrace$ die Menge aller Mediane von $F$ ist.
		\item Im Allgemeinen gibt es mehrere Mediane. Üblicherweise \underline{wählt} man $m:=F^{-1}(\frac{1}{2})$, wobei
		\begin{align*}
			F^{-1}(u):=\inf\left\lbrace x\in\R:F(x)\geq u\right\rbrace\quad\forall u\in (0,1)
		\end{align*}
		die \textbf{Quantilfuntion / die verallgemeinerte Inverse} ist. Da
		\begin{align*}
			F\left(F^{-1}(u)-\right)\leq u\leq F\left(F^{-1}(u)\right)\qquad\forall u\in (0,1),
		\end{align*}
		erfüllt $m=F^{-1}\left(\frac{1}{2}\right)$ die Bedingung (a) in Lemma \ref{lemmaMedian} und ist somit ein Median, nämlich der kleinste.
		\item Die obige Funktion \eqref{DefY}, also
		\begin{align*}
			Y:\R\to\R,\qquad Y(t)=\int\limits|x-t|~F(\d x)\qquad\forall t\in\R,
		\end{align*}
		ist stetig (nutze Folgenkriterium + dominierte Konvergenz bzw. Satz von Lebesgue), aber im Allgemeinen nicht differenzierbar, z. B. falls $F\sim X$ 		eine diskrete Zufallsvariable ist. In diesem Fall ist somit die Minimierung über Differentiation nicht möglich!
	\end{enumerate}
\end{bemerkungnr}

Zur Schätzung von $m$ seien $X_1,\ldots, X_n$ i.i.d.$\sim F$ mit zugehöriger \textbf{empirischer Verteilungsfunktion}
\begin{align*}
	F_n(x):=\frac{1}{n}\cdot\sum\limits_{i=1}^n\indi_{\lbrace X_i\leq x\rbrace}\qquad\forall x\in\R.
\end{align*}
Tatsächlich ist $F_n$ die Verteilungsfunktion zum \textbf{empirischen Maß}
\begin{align*}
	Q_n:=\frac{1}{n}\cdot\sum\limits_{i=1}^n\delta_{X_i}\text{ wobei }\delta_x\text {das Dirac-Maß in}x\in\R
\end{align*}

Gemäß dem Satz von Gliwenko-Cantelli gilt:
\begin{align*}
	\sup\limits_{x\in\R}|F_n(x)-F(x)|\stackrel{n\to\infty}{\longrightarrow}0\text{ konvergiert $\P$-fast sicher für alle Vereteilungsfunktionen }F
\end{align*}

\begin{erinnerung}
	Für das Dirac-Maß $\delta_x:\A\to\R_+,\qquad\delta_x(A):=\indi_A(x)$ gilt:
	\begin{align*}
		\int\limits f(t)~\delta_x(\d t)=f(x)
	\end{align*}
\end{erinnerung}

Ein vages Stetigkeitsargument motiviert folgenden Schätzer für $m$:
\begin{align*}
	\hat{m}_n&:=\argmin\limits_{t\in\R}Y_n(t):=\text{ (irgendeine) Minimalstelle der Funktion}\\
	Y_n(t)&:=\int\limits_\Omega |x-t|F_n(\d x)\\ 
	&=\int\limits_\Omega |x-t|Q_n(\d x)\\ 
	&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\int\limits|x-t|~\delta_{X_i}(\d x)\\
	&=\frac{1}{n}\cdot\sum\limits_{i=1}^n|X_i-t|
\end{align*} 

$\hat{m}_n$ heißt \textbf{empirischer Median} von $X_1,\ldots,X_n$ mit üblicher Auswahl $\hat{m}_n=F_n^{-1}\left(\frac{1}{2}\right)$ gemäß Lemma \ref{lemmaMedian} (da empirische Verteilungsfunktion eine Verteilungsfunktion ist).

%TODO Abbildung 1

\begin{bemerkung}\
	\begin{itemize}
		\item Wenn man eine ungerade Anzahl von Daten hat, ist der Median der mittlere Wert, nachdem man die Daten der Größe nach geordnet hat.
		\item Hat man hingegen eine gerade Anzahl an Daten, dann ist der Median der kleinere der beiden mittleren Werte.
	\end{itemize}
\end{bemerkung}

Mit dem starken Gesetz der großen Zahlen (SGGZ) gilt
\begin{align}\label{eq1.1}
	\forall t\in\R: \Big(Y_n(t)\stackrel{n\to\infty}{\longrightarrow}
	\E[|X_1-t|]=Y(t)\text{ fast sicher}\Big)
\end{align}
Problem: Folgt aus \eqref{eq1.1} bereits, dass
\begin{align*}
	\argmin\limits_{t\in\R} Y_n(t)
	\stackrel{n\to\infty}{\longrightarrow}
	\argmin\limits_{t\in\R}
	Y(t)\text{ fast sicher?}
\end{align*}
Dann folgte:
\begin{align*}
	\hat{m}_n
	\stackrel{n\to\infty}{\longrightarrow}
	m\text{ fast sicher (\textbf{starke Konvergenz})}
\end{align*}

Wir formalisieren und verallgemeinern:
\begin{align*}
	&X_i:(\Omega, \A,P)\to(\R,\B(\R))\text{ messbar},\qquad\omega\mapsto X_i(\omega)\\
	&\Longrightarrow
	Y_n(t):=Y_n(t,\omega)=
	\frac{1}{n}\cdot\sum\limits_{i=1}^n\left|X_i(\omega)-t\right|\\
	&\Longrightarrow
	Y_n(t,\cdot):(\Omega,\A)\to(\R,\B(\R))\text{ messbar }\forall t\in\R
\end{align*}

\begin{defi}
	Die \textbf{Kollektion}
	\begin{align*}
		Y_n:=\lbrace Y_n(t,\cdot):t\in\R\rbrace
		=\lbrace Y_n(t):t\in\R\rbrace
	\end{align*}
	heißt \textbf{stochastischer Prozess (SP)}. Die Abbildung
	\begin{align*}
		X_n(\cdot,\omega):\R\to\R,\qquad t\mapsto Y_n(t,\omega)
	\end{align*}
	heißt \textbf{Trajektorie / Pfad} des SP $Y_n$ zu festem $\omega\in\Omega$.
\end{defi}

In unserem Beispiel sind für \underline{alle} $\omega\in\Omega$ die Pfade stetig auf $\R$. Die Abbildung
\begin{align*}
	Y_n:\Omega\to X C(\R,\R),\qquad\omega\mapsto Y_n(\cdot,\omega)
\end{align*}
heißt \textbf{Pfadabbildung} des SP $Y_n$. Wir identifizieren also den SP $Y_n$ mit seiner Pfadabbildung. Damit ist $Y_n$ eine Abbildung von $\Omega$ in den Funktionenraum 
\begin{align*}
	C(\R):=C(\R,\R):=\lbrace f:\R\to\R: f\text{ ist stetig }\rbrace.
\end{align*}

Sei $d:C(\R)\times C(\R\to\R$ die Metrik der gleichmäßigen Konvergenz der Kompakta auf $\C(\R)$ (formale Definition kommt später) und sei
\begin{align*}
	\B(C(\R)):=\B_d\big(C(\R)\big)=\sigma\big(\lbrace G\subseteq C(\R):G\text{ ist offen bzgl. }d\rbrace\big)
\end{align*}
die von $d$ induzierte \textbf{Borel-$\sigma$-Algebra}.\\
Wir werden sehen, dass die Abbildung
\begin{align*}
	Y_n:(\Omega,\A,\P)\to\Big(C(\R),\B\big(C(\R)\big)\Big)
\end{align*}
messbar ist. $Y_n$ ist also eine Zufallsvariable mit Werten im metrischen Raum $\big(C(\R),d\big)$.

Formulierung des Problems im allgemeinen Rahmen:
Seien $Y_n,~n\in\N$ mit $Y$ SP mit stetigen Pfaden (stetige SP).
Was lässt sich sagen über die Gültigkeit der folgenden Implikationen?
\begin{align}
	Y
	\stackrel{n\to\infty}{\longrightarrow}
	Y\text{ fast sicher }
	\Longrightarrow
	\argmin\limits_{t\in\R} Y(t)
	\stackrel{n\to\infty}{\longrightarrow}
	\argmin\limits_{t\in\R} Y(t)\text{ fast sicher}
\end{align}
Ziel: Welche Art der Konvergenz $Y_n\stackrel{n\to\infty}{\longrightarrow} Y$ reicht für obige Implikation aus? Gleichmäßige Konvergenz, gleichmäßige Konvergenz auf Kompakta punktweise Konvergenz oder sogar nur \eqref{eq1.1}?\\
$Y$ besitzt womöglich (unter positiven Wahrscheinlichkeiten) keine eindeutige Minimalstelle. Und dann?\nl
Für die Konstruktion von (asymptotischen) Konfidenzintervallen für $m$ benötigt man \textbf{Verteilungskonvergenz}:
\begin{align}\label{eq1.3}
	a_n(\hat{m}_n-m)
	\stackrel{\mathcal{L}}{\longrightarrow}\xi\text{ in }\R
\end{align}
wobei $a_n\to\infty$ in $\xi$ Grenzvariable, die es zu identifizieren gilt. Für die Herleitung von \eqref{eq1.3} favorisiere wieder einen \textit{funktionalen Ansatz}. Sei
\begin{align*}
	Z_n(t):=\beta_n\cdot\left( Y_n\left(m+\frac{t}{a_n}\right)-Y_n(m)\right)\qquad t\in\R
\end{align*}
der sogenannte \textbf{reskalierte Prozess zu $Y_n$}, wobei $\beta_n$ deine geeignete positive Folge ist. Damit folgt
\begin{align}\label{1.4}
	a_n(\hat{m}_n-m)=\argmin\limits_{t\in\R} Z_n(t)
\end{align}
Klar: $Z_n$ ist wieder ein stetiger stochastischer Prozess und damit $(Z_n)_{n\in\N}$ eine Folge von Zufallsvariablen in $\big(C(\R),d\big)$.
Wünschenswert auch hier wäre die Gültigkeit folgender Implikation:
\begin{align}\label{eqSternchen}\tag{$\ast$}
	Z_n
	\stackrel{\mathcal{L}}{\longrightarrow}
	Z\text{ in } \big(C(\R),d\big)
	\Longrightarrow
	\argmin\limits_{t\in\R} Z_n(t)
	\stackrel{\mathcal{L}}{\longrightarrow}
	\argmin\limits_{t\in\R} Z(t)
\end{align}
Dazu erforderlich ist das Konzept der \textbf{Verteilungskonvergenz} von Zufallsvariablen in metrischen Räumen, damit \eqref{eqSternchen} eine wohldefinierte Bedeutung erhält.
Dies folgt später.
Natürlich auch hier wieder das Problem: $Z$ besitzt mit positiver Wahrscheinlichkeit mindestens 2. Minimalstellen. 
Und dann?\nl
Im Falle einer fast sicher eindeutigen Minimalstelle von $Z$ würde aber aus (1.4) und (1.5) folgen:
\begin{align*}
	a_n(\hat{m}_n-m)
	\stackrel{\mathcal{L}}{\longrightarrow}
	\argmin\limits_{t\in\R} Z(t)
\end{align*}

\section{Konzepte aus metrischen Räumen}
Sei $(\mathcal{S},d)$ metrischer Raum.

\begin{beispiel}[Supremums-Metrik] %2.1
	\begin{align*}
		\mathcal{S}=C([0,1]):=\big\lbrace f:[0,1]\to\R: f\text{ stetig}\big\rbrace\\
		d(f,g):=\sup\limits_{t\in[0,1]}\big|f(t)-g(t)\big|,\qquad\forall f,g\in C([0,1])
	\end{align*}
\end{beispiel}

\begin{definition}\
	\begin{enumerate}[label={(\arabic*)}]
		\item Für $x\in\mathcal{S},~r>0$ ist
		\begin{align*}
			B(x,r):=B_d(x,r):=\lbrace y\in\mathcal{S}:d(x,y)<r\rbrace
		\end{align*}
		die offene Kugel um Mittelpunkt $x$ und Radius $r$.
		\item Sei $A\subseteq\mathcal{S}$. Dann:
		\begin{align*}
			\stackrel{\circ}{A}&:=\inner(A):=\text{ das Innere von }A\\
			\overline{A}&:=\text{ Abschluss von }A\\
			\partial A&:=\overline{A}\cap\overline{A^C}=\overline{A}\setminus\stackrel{\circ}{A}\text{ ist der Rand von }A\\
			A^C&:=\mathcal{S}\setminus A
		\end{align*}
		\item \begin{align*}
			\mathcal{G}:=\mathcal{G}(\mathcal{S})&:=\big\lbrace G\subseteq\mathcal{S}: G\text{ ist offen bzgl. }d\big\rbrace\\
			&=\big\lbrace G\subseteq\mathcal{S}:\forall x\in G:\exists r>0:B_d(x,r)\subseteq G\big\rbrace
		\end{align*}
		ist die durch $d$ induzierte Topologie.
		\begin{align*}
			\mathcal{F}:=\mathcal{F}(\mathcal{S}):=\big\lbrace F\subseteq\mathcal{S}:F\text{ ist abgeschlossen}\big\rbrace
		\end{align*}
		\item Sei $\emptyset\neq A\subseteq\mathcal{S},~x\in\mathcal{S}$. Dann ist
		\begin{align*}
			d(x,A):=\inf\lbrace d(x,a):a\in A\rbrace\geq0
		\end{align*}
		der Abstand von $x$ zu $A$.
		\item $C(\mathcal{S}):=\lbrace f:S\to\R:f\text{ stetig}\rbrace$
		\begin{align*}
			C^b(\mathcal{S}):=\lbrace f\in C(\mathcal{S}):f\text{ beschränkt}\rbrace\\
			\Vert f\Vert:=\Vert f\Vert_\infty:=\sup\limits_{x\in\mathcal{S}}|f(x)|
		\end{align*}
	\end{enumerate}
\end{definition}

\begin{lemma}\label{lemma2.3}\ %2.3
	\begin{enumerate}[label={(\arabic*)}]
		\item $\begin{aligned}
			x\in\overline{A}\Longleftrightarrow d(x,A)=0
		\end{aligned}$
		\item $\begin{aligned}
			\big| d(x,A)-d(y,A)\big|\leq d(x,y)\qquad\forall x,y\in\mathcal{S}
		\end{aligned}$
		\item $\begin{aligned}
			d(\cdot, A):\mathcal{S}\to\R,\qquad x\mapsto d(x,A)
		\end{aligned}$ ist gleichmäßig stetig ($A\neq\emptyset$).
	\end{enumerate}
\end{lemma}

\begin{proof}
	\underline{Zeige (1) ``$\Rightarrow$'':} Sei $x\in\overline{A}$. Dann gilt:
	\begin{align*}
		&\forall\varepsilon>0:\exists a\in A: d(x,a)<\varepsilon\\
		&\implies d(x,A)\leq d(x,a)<\varepsilon~\forall\varepsilon>0\\
		&\stackrel{\varepsilon\to0}{\implies}
		d(x,A)=0
	\end{align*}
	\underline{Zeige (1) ``$\Leftarrow$'':}
	Sei $d(x,A)=0$. Dann folgt aus der Infimumseigenschaft:
	\begin{align*}
		&\forall\varepsilon>0:\exists a\in A;0\leq d(x,a)\leq0+\varepsilon=\varepsilon\\
		&\implies x\in\overline{A}
	\end{align*}
	\underline{Zeige (2):} Seien $x,y\in\mathcal{S}$. Dann gilt:
	\begin{align*}
		&d(x,a)
		\stackrel{\Delta\text{Ungl}}{\leq}
		d(x,y)+d(y,a)\qquad\forall a\in A\\
		&\implies
		d(x,A)\leq d(x,y)+d(y,A)\implies d(x,A)-d(y,A)\leq d(x,y)
	\end{align*}
	Vertauschen von $x$ und $y$ liefert:
	\begin{align*}
		d(y,A)-d(x,A)\leq d(y,x)=d(x,y)\implies\text{ Behauptung}
	\end{align*}
	\underline{Zeige (3):} Folgt aus (2) da, die Funktion $d(\cdot,A)$ Lipschitz-stetig und damit gleichmäßig stetig ist.
\end{proof}

\begin{satz}\label{Satz2.4} %2.4
	Zu $A\subseteq\mathcal{S}$ und $\varepsilon>0$ existiert ein gleichmäßig stetige Funktion 
	\begin{align*}
		f:\mathcal{S}\to[0,1]\text{ mit der Eigenschaft} f(x)=\left\lbrace\begin{array}{cl}
			1, & \falls x\in A\\
			0, & \falls d(x,A)\geq\varepsilon
		\end{array}\right.
	\end{align*}
\end{satz}

\begin{proof}
	Setze
	\begin{align*}
		\varphi:\R\to[0,1],\qquad \varphi(t):=\left\lbrace\begin{array}{cl}
			1 , & \falls t\leq0\\
			1-t, & \falls 0<1<1\\
			0, & \falls t\geq1
		\end{array}\right.
	\end{align*}
	Dann ist $\varphi$ gleichmäßig stetig auf $\R$. Sei
	\begin{align*}
		f(x):=\varphi\left(\frac{1}{\varepsilon}\cdot d(x,A)\right)\qquad\forall x\in\S
	\end{align*}
	Dann hat dieses $f$ die gewünschte Eigenschaft wegen Lemma \ref{lemma2.3}.
\end{proof}

\begin{definition} %2.5
	Ein metrischer Raum $(\S,d)$ heißt \textbf{separabel}
	\begin{align*}
		&:\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:\S\subseteq\overline{S_0}\\
		&\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:\S=\overline{S_0}\\
		&\Longleftrightarrow\exists\text{ abzählbares } S_0\subseteq\S:S_0\text{ liegt dicht in }\S
	\end{align*}
\end{definition}

\begin{beispiel}\label{beisp2.6} %2.6
	$C([0,1])$ mit Supremums-Metrik ist separabal.
	
	\begin{proof}
		\begin{align*}
			S_0:=\big\lbrace P:P\text{ ist Polynom mit \underline{rationalen} Koeffizienten}\big\rbrace
		\end{align*}
		$S_0$ ist abzählbar. Aus dem \textit{Approximationssatz von Weierstraß} und der Dichtheit von $\Q$ folgt die Behauptung.
	\end{proof}
\end{beispiel}

\begin{definition} %2.7
	$\G_0\subseteq\G$ heißt \textbf{Basis} von $\G:\Longleftrightarrow\forall G\in\G:G$ ist Vereinigung von Mengen aus $\G_0$,
	so genannte \textbf{$\G_0$-Mengen}.
\end{definition}

\begin{beispiel} %2.8
	Die Menge
	\begin{align*}
		\big\lbrace B(x,r):x\in\S,0<r\in\Q\big\rbrace
	\end{align*}
	ist Basis von $\G$, denn:
	\begin{proof}
		Sei $G\in\G$. Dann gilt:
		\begin{align*}
			&\forall x\in G:\exists 0<r_x\in\Q:B(x,r_x)\subseteq G\\
			&\implies
			G=\bigcup\limits_{x\in G}\underbrace{\lbrace x\rbrace}_{\subseteq B(x,r_x)}\subseteq\bigcup\limits_{x\in G} \underbrace{B(x,r_x)}_{\subseteq G}\subseteq G\implies G=\bigcup\limits_{x\in G} \underbrace{B(x,r_x)}_{\in\G_0}
		\end{align*}
	\end{proof}
\end{beispiel}

\begin{satz}\label{satz2.9}
	$\S$ separabel $\Longleftrightarrow\G$ hat abzählbare Basis
\end{satz}

\begin{proof}
	\underline{Zeige ``$\Rightarrow$'':}\\
	Sei $S_0\subseteq\S$ abzählbar und dicht in $\S$. Zeige:
	\begin{align*}
		\G_0:=\big\lbrace B(x,r):x\in S_0,0<r\in\Q\big\rbrace\subseteq\G\text{ ist Basis.}
	\end{align*}
	Sei also $G$ offen. Dann folgt aus Beispiel 2.8:
	\begin{align}\label{proof2.9Sternchen}\tag{$\ast$}
		G=\bigcup\limits_{x\in G} B(x,r_x),\qquad 0<r_x\in\Q,\forall x\in G
	\end{align}
	Da $\overline{S_0}=\S$ gilt:
	\begin{align*}
		&\forall x\in G:\exists y_x\in S_0: d(x,y_x)<\frac{r_x}{2}\\
		&\implies d(x,y)
		\stackrel{\Delta\text{Ungl}}{\leq}
		d(x,y_x)+d(y_x,x)< \underbrace{\frac{r_x}{2}+\frac{r_x}{2}}_{=r_x}\qquad\forall y\in B\left(y_x,\frac{r_x}{2}\right)\\
		&\implies B\left(y_x,\frac{r_x}{2}\right)\subseteq B(x,r_x)\qquad\forall x\in G\\
		&\implies G\stackrel{\eqref{proof2.9Sternchen}}{\supseteq}
		\bigcup\limits_{x\in G}\underbrace{B\left(y_x,\frac{r_x}{2}\right)}_{\supseteq\lbrace x\rbrace}
		\supseteq\bigcup\limits_{x\in G}\lbrace x\rbrace=G\\
		&\implies G=\bigcup	\limits_{x\in G}\underbrace{B\left(y_x,\frac{r_x}{2}\right)}_{\in\G_0}
	\end{align*}
	Also ist $\G_0$ einen Basis. Da $S_0$ abzählbar ist $\G_0$ abzählbar.\nl
	\underline{Zeige ``$\Leftarrow$'':}\\
	Sei $\G_0$ abzählbare Basis von $\G$ und sei o.B.d.A. $\emptyset\notin\G_0$. Wähle für jedes $G\in\G_0$ ein $x_G\in G$ fest aus. Setze
	\begin{align*}
		S_0:=\lbrace x_G:G\in\G_0\rbrace.
	\end{align*}
	$S_0$ ist auch abzählbar. Bleibt Dichtheit zu zeigen.\\
	Sei $x\in\S$ und $\varepsilon>0$. Da $B(x,\varepsilon)$ offen und $\G_0$ Basis, gilt: 
	\begin{align*}
		&\exists\G_{x,\varepsilon}\subseteq\G_0\mit B(x,\varepsilon)=\bigcup\limits_{G\in\G_{x,\varepsilon}} G\\
		&\implies G\subseteq B(x,\varepsilon)\qquad\forall G\in\G_{x\varepsilon}
	\end{align*}
	Wähle ein $G$ von diesen aus. Dann gilt:
	\begin{align*}
		x_G\in G\subseteq B(x,\varepsilon)
		\implies x_G\in B(x,\varepsilon)
		\implies d(\underbrace{x_G}_{\in S_0},x)<\varepsilon
	\end{align*}
\end{proof}

\begin{satz}\label{Satz2.10} %2.10
	Seien $(\S,d)$ und $(\S',d')$ metrische Räume.
	\begin{enumerate}[label={(\arabic*)}]
		\item Auf $\S\times\S'$ sind Metriken definiert durch
		\begin{align*}
			d_1\Big((x,x'),(y,y')\Big)&:=\left( \big(d(x,y)\big)^2+\big(d'(x',y')\big)^2\right)^{\frac{1}{2}} &\forall(x,x'),(y,y')\in \S\times\S'\\
			d_2\Big((x,x'),(y,y')\Big)&:=\max \left\lbrace d(x,y),d'(x',y')\right\rbrace &\forall(x,x'),(y,y')\in \S\times\S'\\
			d_3\Big((x,x'),(y,y')\Big)&:=d(x,y)+d'(x',y') &\forall(x,x'),(y,y')\in \S\times\S'
		\end{align*}
		\item Die Metriken $d_1,d_2,d_3$ induzieren dieselbe Topologie $\mathcal{G}(\S\times \S')$ auf $\S\times\S'$, 
		die sogenannte \textbf{Produkttopologie} von $\mathcal{G}(\S)$ und $\mathcal{G}(\S')$.
		\item $\begin{aligned}
			\mathcal{G}(\S\times\S')=\left\lbrace\bigcup\limits_{\begin{subarray}{c}
				G\in\mathcal{O}\\ G'\in\mathcal{O}'
			\end{subarray}}G\times G':\mathcal{O}\subseteq\mathcal{G}(\S),\mathcal{O}'\subseteq\mathcal{G}(\S')\right\rbrace
		\end{aligned}$\\
		d.h.
		\begin{align*}
			\big\lbrace G\times G':G\in\mathcal{G}(\S),G'\in\mathcal{G}(\S')\big\rbrace
		\end{align*}
		bildet eine Basis von $\mathcal{G}(S\times\S')$.
	\end{enumerate}
\end{satz}

\begin{proof}\enter
	\underline{Zu (1):} Überprüfung der Eigenschaften einer Metrik (zur Übung).\\
	\underline{Zu (2):} Punktweise gelten die Beziehungen:
	\begin{align*}
		d_2\leq d_1\leq\sqrt{2}\cdot d_2,\qquad
		\frac{1}{\sqrt{2}}\cdot d_3\leq d_1\leq d_3,\qquad
		d_2\leq d_3\leq 2\cdot d_2
	\end{align*}
	Beachte beim Nachweis, dass die $d_i$'s als Metriken größer Null sind. Aus obigen Beziehungen folgt u. a.:
	\begin{align*}
		B_{d_2}\left(x,\frac{r}{\sqrt{2}}\right)\subseteq B_{d_1}(x,r)
	\end{align*}
	denn:
	\begin{align*}
		r>\sqrt{2}\cdot d_2(y,x)\geq d_1(y,x)
	\end{align*}
	\underline{Zu (3), zeige ``$\subseteq$'':}\\
	Sei $G^\ast\in\mathcal{G}(\S\times\S')$. Dann gilt:
	\begin{align*}
		\forall x^\ast=(x,y)\in G^\ast:\exists r=r_{x^\ast}>0:
		G^\ast=\bigcup\limits_{x^\ast\in G^\ast} B\big(x^\ast,r_{x^\ast}\big)
	\end{align*}
	Wegen Teil (2) sei o.B.d.A. $\S^\ast:=\S\times\S'$ versehen mit der Metrik $d_2$. Dann gilt:
	\begin{align*}
		B_{d_2}\big(x^\ast,r_{x^\ast}\big)&=\Big\lbrace(y,y')\in \S\times\S':\max\big\lbrace d(x,y),d'(x',y')\big\rbrace<r_{x^\ast}\Big\rbrace\\
		&=\Big\lbrace(y,y')\in\S\times\S':d(x,y)<r_{x^\ast}\wedge d'(x',y')<r_{x^\ast}\Big\rbrace\\
		&= \underbrace{B_d\big(x,r_{x^\ast}\big)}_{\in\mathcal{G}(\S)}\times \underbrace{B_{d'}\big(x', r_{x^\ast}\big)}_{\in\mathcal{G}(\S')}
	\end{align*}
	\underline{Zu (3), zeige ``$\supseteq$'':}\\
	Sei zunächst $G\times G'\mit G,G'$ offen und $x^\ast=(x,x')\in G\times G'$. Also ist $x\in G$ und $x'\in G'$ und somit
	\begin{align*}
		\exists r,r'>0:B_d(x,r)\subseteq G\wedge B_{d'}(x',r')\subseteq G'
	\end{align*}
	Setze $r^\ast:=\min\lbrace r,r'\rbrace>0$. Damit folgt

	\begin{align*}
		B_{d_2}\big( x^\ast,r^\ast\big)&\subseteq B_d(x,r)\times B_{d'}\big(x',r'\big)\\
		&\subseteq
		G\times G'=G^\ast\\
		&\implies
		G\times G'\in\mathcal{G}(\S\times\S')\\
		&\implies
		\bigcup\limits_{\begin{subarray}{c} G\in\mathcal{O}\\G'\in\mathcal{O}'\end{subarray}}G\times G'\subseteq\mathcal{G}(\S\times\S')
		\qquad\forall\mathcal{O}\subseteq\mathcal{G}(\S),\mathcal{O}'\subseteq\mathcal{G}(\S')
	\end{align*}
	da die Produkttopologie vereinigungsstabil ist.
\end{proof}

\begin{defi}
	Die Metriken $d_1,d_2,d_3$ heißen \textbf{Produktmetriken}.
	Daher alternative\\ Schreibweise $d\times d'$, also z. B. $d\times d':=\max\lbrace d,d'\rbrace$ usw.
\end{defi}

\begin{bemerkungnr} %2.11
	Analog lassen sich Produktmetriken für \underline{endlich viele} metrische Räume $(S_i,d_i)_{i\in\lbrace1,\ldots,k\rbrace}$ definieren, z. B.
	\begin{align*}
		d_1\times\ldots\times d_k:=\left(\sum\limits_{i=1}^k d_i^2\right)^{\frac{1}{2}},
	\end{align*}
	die wiederum dieselbe Produkttopologie induzieren.
\end{bemerkungnr}

\section{Zufallsvariablen in metrischen Räumen}
\begin{definition} %3.1
	Die \textbf{Borel-$\sigma$-Algebra} auf dem metrischen Raum $(\S,d)$ ist %definiert als
	\begin{align*}
		\B(\S):=\sigma\big(\mathcal{G}(\S)\big).
	\end{align*}
	Elemente $B\in\B(\S)$ heißen \textbf{Borel-Mengen} in $\S$.\\
	Beachte: $\B(\S)=\B_d(\S)$ hängt i. A. von der Metrik $d$ ab.
\end{definition}

\begin{lemma}\label{Lemma3.2} %3.2
	Es gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item 
		$\begin{aligned}
			\B(\S)=\sigma\big(\mathcal{F}(\S)\big)
		\end{aligned}$
		\item $\begin{aligned}
			f:(\S,d)\to(\S',d)
		\end{aligned}$ ist stetig und damit $\B_d(\S)-\B_d(\S')$ %TODO
		\item Sei $\mathcal{G}_0$ abzählbare Basis von $\mathcal{G}(\S)$. Dann gilt:
		\begin{align*}
			\sigma(\mathcal{G}_0)=\B(\S)
		\end{align*}
	\end{enumerate}
\end{lemma}

\begin{proof}\enter
	\underline{Zu (1), zeige ``$\subseteq$'':}
	\begin{align*}
		G^C\in\mathcal{F}(\S)\subseteq\sigma\big(\mathcal{F}(\S)\big)
		\implies
		G=\big(G^C\big)^C\in\mathcal{F}(\S)
	\end{align*}
	da $\sigma\big(\mathcal{F}(\S)\big)$ Komplement-stabil ist. Also folgt
	\begin{align*}
		\mathcal{G}\subseteq\sigma(\mathcal{F})
		\implies\sigma(\mathcal{G})\subseteq\sigma(\mathcal{F})
	\end{align*}
	\underline{Zu (1), zeige ``$\supseteq$'':} Analog.\nl
	\underline{Zeige (2):}
	\begin{align*}
		f^{-1}\big(\B_{d'}(\S')\big)&=f^{-1}\Big(\sigma\big(\mathcal{G}(\S')\big)\Big)\\
		&=\sigma\Big(\underbrace{f^{-1}\big(\mathcal{G}(\S')\big)}_{\stackrel{f\text{ stetig}}{\subseteq}\mathcal{G}(S)}\Big)\\
		&\subseteq\sigma\big(\mathcal{G}(\S)\big)\\
		&=\B(\S)
	\end{align*}
	\underline{Zu (3), zeige ``$\subseteq$'':}\\
	Klar wegen $\mathcal{G}_0\subseteq\mathcal{G}$ und $\sigma$ monoton.\\
	\underline{Zu (3), zeige ``$\supseteq$'':} Sei $G\in\mathcal{G}$. Dann:
	\begin{align*}
		G&=\bigcup\limits_{i\in\N} G_i\mit\text{geeigneten }G_i\in\mathcal{G}_0\subseteq\sigma(\mathcal{G}_0)\\
		&\implies
		G\in\sigma(\mathcal{G}_0)
	\end{align*}
	Aus der Stabilität unter Vereinigungen folgt die Behauptung.
\end{proof}

\begin{satz}\label{satz3.3} %3.3
	Sei $(\S,d)$ separabler metrischer Raum. Dann gilt:
	\begin{align*}
		\B_{d\times d}(\S\times\S)=\B(S)\otimes\B(\S)
	\end{align*}
\end{satz}

\begin{proof}
	Seien
	\begin{align*}
		&\pi_1:\S\times\S\to\S,\qquad \pi_1(x,y):=x\qquad\forall(x,y)\in\S\times S\\
		&\pi_2:\S\times\S\to\S,\qquad \pi_2(x,y):=y\qquad\forall(x,y)\in\S\times S
	\end{align*}
	die \textbf{Projektionsabbildungen}. Dann gilt
	\begin{align*}
		\B(\S)\otimes\B(\S) 
		\overset{\text{Def}}&=
		\sigma(\pi_1,\pi_2)\\
		\overset{\text{Def}}&=
		\sigma\Big(\pi_1^{-1}\big(\sigma(\mathcal{G})\big)\cup\pi_2^{-1}\big(\sigma(\mathcal{G})\big)\Big)\\
		\overset{(+)}&=
		\sigma\Big(\sigma\big(\pi_1^{-1}(\mathcal{G})\big)\cup\sigma\big(\pi_2^{-1}(\mathcal{G})\big)\Big)\\
		&=
		\sigma\Big(\pi_1^{-1}(\mathcal{G})\big)\cup\pi_2^{-1}(\mathcal{G})\Big)\\
		&=\sigma\Big(\big\lbrace G\times S,S\times G':G,G'\in\mathcal{G}\big\rbrace\Big)\\
		&=
		\sigma\Big(\big\lbrace \overbrace{G\times G'}^{=(G\times S)\cap(S\times G')}:G,G'1\in\mathcal{G}\big\rbrace\Big)\\
		\overset{\text{($\ast$)}}&=
		\sigma\left(\left\lbrace\bigcup\limits_{\begin{subarray}{c}
			G\in\mathcal{O}\\
			G'\in\mathcal{O}'
		\end{subarray}}G\times G':\mathcal{O},\mathcal{O}'\subseteq\mathcal{G}\right\rbrace\right)\\
		\overset{2.10~(3)}&=
		\sigma\Big(\mathcal{G}(\S\times\S)\Big)\\
		\overset{\text{Def}}&=
		\B(\S\times\S)
	\end{align*}
	Zum Nachweis von (+):\\
	Zeige ``$\supseteq$'': Setze
	\begin{align*}
		\xi&:=
		\underbrace{\sigma\big(\pi_1^{-1}(\mathcal{G})\big)}_{\supseteq \pi_1^{-1}(\mathcal{G})}\cup\underbrace{\sigma\big(\pi_2^{-1}(\mathcal{G})\big)}_{\pi_2^{-1}(\mathcal{G})}\\
		&\supseteq
		\pi_1^{-1}(\mathcal{G})\cup\pi_2^{-1}(\mathcal{G})\\
		&=:\mathcal{H}\\
		&\implies\sigma(\xi)\supseteq\sigma(\mathcal{H})
	\end{align*}
	Zeige ``$\subseteq$'': Es gilt
	\begin{align*}
		&\pi_1^{-1}(\mathcal{G})\subseteq\big(\pi_1^{-1}(\mathcal{G})\cup\pi_2^{-1}(\mathcal{G})\big)=\mathcal{H}\\
		&\implies
		\sigma\big(\pi_1^{-1}(\mathcal{G})\big)\subseteq\sigma(\mathcal{H})\text{ und analog }\\
		&\implies
		\sigma\big(\pi_2^{-1}(\mathcal{G})\big)\subseteq\sigma(\mathcal{H})\\
		&\implies
		\xi=\underbrace{\sigma\big(\pi_1^{-1}(\mathcal{G})\big)}_{\subseteq\sigma(\mathcal{H})}\cup\underbrace{\sigma\big(\pi_2^{-1}(\mathcal{G})\big)}_{\subseteq\sigma(\mathcal{H})}\subseteq\sigma(\mathcal{H})\\
		&\implies
		\sigma(\xi)\subseteq\sigma(\mathcal{H})
	\end{align*}

	Bleibt Nachweis von ($\ast$):\\
	``$\subseteq$'': ist klar (gilt auch ohne Separabilität)\\
	``$\supseteq$'': Gemäß 2.9 existiert abzählbare Basis $\mathcal{G}_0$  von $\mathcal{G}$. Sei
	\begin{align*}
		G^\ast&=\bigcup\limits_{\begin{subarray}{c}
			G\in\mathcal{O}\\
			G'\in\mathcal{O}'
		\end{subarray}}G\times G'\text{ und }\mathcal{O},\mathcal{O}'\subseteq\mathcal{G}\\
		&\stackeq{(!)}
		\bigcup\limits_{\begin{subarray}{c}
			G,G'\text{ offen}\\
			G,G'\subseteq G^\ast
		\end{subarray}}
		G\times G'\\
		&\stackeq{(!)}
		\bigcup\limits_{\begin{subarray}{c}
			G_0,G_0'\in\mathcal{G}_0\\
			G\times G_0'\subseteq G^\ast
		\end{subarray}}
		G_0\times G_0'\\
		&=\text{ abzählbare Vereinigung, da $\mathcal{G}_0$ abzählbare Basis }\\
		&\implies
		G^\ast\in\sigma\Big(\big\lbrace G\times G':G,G'\in\mathcal{G}\big\rbrace\Big)
	\end{align*}
\end{proof}

\begin{definition} %3.4
	Sei $(\Omega,\A)$ ein Messraum.
	Eine Abbildung
	$X:\Omega\to\S$, die $\A$-$\B(\S)$-messbar ist, heißt \textbf{Zufallsvariable (ZV)} in den metrischen Raum $(\S,d)$ über $(\Omega,\A)$.\nl
	Sei $\P$ ein Wahrscheinlichkeitsmaß auf $(\Omega,\A)$, also $(\Omega,\A,\P)$ ein Wahrscheinlichkeitsraum. 
	Das Bildmaß
	\begin{align*}
		\P\circ X^{-1}&:=:\P_X:=:\mathcal{L}:=:\mathcal{L}(X~|~\P)\\
		(\P\circ X^{-1})(B)&:=\P\left(X^{-1}(B)\right)=\P\Big(\big\lbrace\omega\in\Omega:X(\omega)\in B\big\rbrace\Big)
		=: \P[X\in B]
		\qquad\forall B\in\B(\S)
	\end{align*}
	heißt \textbf{Verteilung} von $X$ unter $\P$.
\end{definition}

\begin{satz}\label{Satz3.5} %3.5
	Sei $(\S,d)$ separabler metrischer Raum und seien $X,Y$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A)$.\\
	Dann ist $d(X,Y)$ eine reelle Zufallsvariable.
\end{satz}

\begin{proof}
	\begin{align*}
		X,Y:(\Omega,\A)\to(\S,\B(\S))\text{ sind messbar }\\
		\stackrel{\text{MINT}}{\Longleftrightarrow}
		(X,Y):(\Omega,\A)\to\big(\S\times\S,\underbrace{\B(\S)\otimes\B(\S)}_{\stackeq{\ref{satz3.3}}\B(\S\times\S)}\big)\text{ ist messbar}\\
	\end{align*}
	Jede Metrik ist bekanntlich stetig, also auch
	\begin{align*}
		d:\big(\S\times\S,\G(\S\times\S)\big)\to\R.
	\end{align*}
	Dann folgt aus Lemma \ref{Lemma3.2}, dass
	\begin{align*}
		d:\B(\S\times\S)\to\B(\R)
	\end{align*}
	messbar ist.
	Damit folgt die Behauptung, denn $d(X,Y)=d\circ(X,Y)$ ist messbar als Komposition von messbaren Abbildungen.
\end{proof}

\subsection*{Fast sichere Konvergenz} %NoNumber
\begin{definition} %3.6
	Seien $X,X_n,n\in\N$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A,\P)$. 
	Dann:
	\begin{align*}
		X_n\stackrel{n\to\infty}{\longrightarrow} X\quad\P\text{-fast sicher }:\Longleftrightarrow
		\P\Big(\underbrace{\big\lbrace\omega\in\Omega:d\big(X_n(\omega),X(\omega)\big)\stackrel{n\to\infty}{\longrightarrow} 0\big\rbrace}_{=:M}\Big)=1
	\end{align*}
	Beachte: Die Definition von Konvergenz mengentheoretisch aufgeschrieben (Schnitt $\hat{=}$ ``für alle''; Vereinigung $\hat{=}$ ``Es gibt''):
	\begin{align*}
		\bigcap\limits_{0<\varepsilon\in\Q}
		\bigcup\limits_{m\in\N}
		\bigcap\limits_{n\geq m}
		\big\lbrace\underbrace{d(X_n,X)}_{=:\xi_n}<\varepsilon\big\rbrace\stackrel{\ref{Satz3.5}}{\in}\A\\
		\text{denn }\xi_n^{-1}\big((-\infty,\varepsilon)\big)\in\A
	\end{align*}
\end{definition}

Die bekannten Regeln (Ergebnisse) für \underline{reelle} Zufallsvariablen lassen sich mühelos verallgemeinern. Dazu z. B.:

\begin{satz}\label{Satz3.7} %3.7
	\begin{align*}
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\quad\P\text{-fast sicher }\wedge
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X'\quad\P\text{-fast sicher }
		\implies
		X=X'\quad\P\text{-fast sicher}
	\end{align*}
\end{satz}

\begin{proof}
	\begin{align*}
		\lbrace X\neq X'\rbrace
		&\subseteq\lbrace X_n
		\stackrel{n\to\infty}{\not\longrightarrow}
		X\rbrace
		\cup\lbrace X_n
		\stackrel{n\to\infty}{\not\longrightarrow}
		X'\rbrace\\
		&\implies
		\P[X_n\not\to X]+\P[X_n\not\to X']=0+0\\
		&\implies
		\P[X\neq X']=0
	\end{align*}
\end{proof}

\begin{satz}\label{Satz3.8} %3.8
	Seien $X,X_n,n\in\N$ Zufallsvariablen im metrischen Raum $(\S,d)$ und sei $f:(\S,d)\to(\S',d')$ messbar und stetig in $X$ $\P$-fast sicher.
	Dann gilt:
	\begin{align*}
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\quad\P\text{-fast sicher }
		\implies
		f(X_n)
		\stackrel{n\to\infty}{\longrightarrow}
		f(X)\quad\P\text{-fast sicher}
	\end{align*}
\end{satz}

\begin{proof}
	\begin{align*}
		\lbrace X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\rbrace\cap\lbrace f\text{ stetig in }X\rbrace\rbrace
		\stackrel{\text{Folgen-Stetigkeit}}{\subseteq}
		\lbrace f(X_n)
		\stackrel{n\to\infty}{\longrightarrow}
		f(X)\rbrace
	\end{align*}
	Damit folgt die Behauptung, denn zur Erinnerung:
	\begin{align*}
		\big(\forall i\in\N:\P(E_i)=1\big)\implies\P\left(\bigcap\limits_{i\in\N} E_i\right)=1
	\end{align*}
\end{proof}

\begin{satz}[Konvergenz-Kriterium]\label{Satz3.9} %3.9
	\begin{align*}
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\quad\P\text{-fast sicher}
		\Longleftrightarrow
		\forall\varepsilon>0:\limn\P\left(\sup\limits_{m\geq n} d(X_m,X)>\varepsilon\right)=0
	\end{align*}
\end{satz}

\begin{proof}
	Man ersetze im Beweis für den Fall reeller Zufallsvariablen $|X_n-X|$ durch $d(X_n,X)$. 
	Und beachte, dass alle Schlussfolgerungen bestehen bleiben.
\end{proof}

Ein sehr nützliches Kriterium ist Folgendes:
\begin{satz}\label{Satz3.10} %3.10
	\begin{align*}
		\sum\limits_{n\in\N_{>0}}\P\big(d(X_n,X)>\varepsilon\big)<\infty\qquad\forall\varepsilon>0
		\implies
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\quad\P\text{-fast sicher}
	\end{align*}
\end{satz}

\begin{proof}
	Setze
	\begin{align*}
		A_n(\varepsilon):=\big\lbrace d(X_n,X)>\varepsilon\big\rbrace\stackrel{\ref{Satz3.5}}{\in}\A
	\end{align*}
	Dann folgt aus dem \textit{ersten Borel-Cantelli-Lemma}:
	\begin{align*}
		&\P\left(\limsup\limits_{n\to\infty} A_n(\varepsilon)\right)=0\qquad\forall\varepsilon>0
	\end{align*}
		Mit
	\begin{align*}
		\liminf\limits_{n\to\infty}\big(A_n(\varepsilon)\big)
		\stackeq{\text{Def}}
		\bigcup\limits_{m\in\N}\bigcap\limits_{n\geq m}\big(A_n(\varepsilon)\big)^C
		=
		\bigcup\limits_{m\in\N}\bigcap\limits_{n\geq m}\big\lbrace d(X_n,X)\leq\varepsilon\big\rbrace
	\end{align*}
	folgt dann
	\begin{align*}
		1=\P\left(\left(\limsup\limits_{n\to\infty} A_n(\varepsilon)\right)^C\right)
		=\P\left(\liminf\limits_{n\to\infty}\big(A_n(\varepsilon)\big)\right)\qquad\forall\varepsilon>0
	\end{align*}
	Da Abzählbare Durchschnitte von Eins-Mengen (also Mengen mit $\P$-Maß 1) wieder Eins-Mengen sind, folgt schließlich:
	\begin{align*}
		\P\Bigg(\underbrace{\bigcap\limits_{0<\varepsilon\in\Q}\bigcup\limits_{n\geq m}\big\lbrace d(X_n,X)\leq\varepsilon\big\rbrace}_{\lbrace X_n\to X\rbrace=\lbrace d(X_n,X)\to0\rbrace}\Bigg)=1
	\end{align*}
\end{proof}

Weitere Eigenschaften der fast sicheren Konvergenz von Zufallsvariablen in metrischen Räumen finden sich z. B. in \textit{``Wahrscheinlichkeitstheorie'' Gäussler u. Stute (1977), Kapitel 8.2}

\subsection*{Stochastische Konvergenz} %NoNumber
\begin{definition} %3.11
	\begin{align*}
		X_n
		\stackrelnew{n\to\infty}{\P}{\longrightarrow}
		X:\Longleftrightarrow\forall\varepsilon>0:
		\P\Big(\big\lbrace d(X_n,X)>\varepsilon\big\rbrace\Big)
		\stackrel{n\to\infty}{\longrightarrow}
		0
	\end{align*}
\end{definition}

\begin{satz}\label{Satz3.12}
	\begin{align*}
		X_n
		\stackrel{n\to\infty}{\longrightarrow}
		X\quad\P\text{-fast sicher }
		\implies X_n
		\stackrelnew{n\to\infty}{\P}{\longrightarrow}
		X
	\end{align*}
\end{satz}

\begin{proof}
	\begin{align*}
		\forall\varepsilon>0:
		0\leq\P\big(d(X_n,X)>\varepsilon\big)
		\leq\P\left(\sup\limits_{m\geq n}d(X_m,X)>\varepsilon\right)
		\stackrelnew{n\to\infty}{\P}{\longrightarrow}
		0
	\end{align*}
	gemäß Satz \ref{Satz3.9}.
\end{proof}

Die Umkehrung in 3.12 gilt i. A. \underline{nicht}, aber es gilt das folgende Teilfolgenkriterium:

\begin{satz}[Teilfolgenkriterium für stochastische Konvergenz]\label{satz3.13}\enter
	Folgende Aussagen sind äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X
		\end{aligned}$
		\item Zu jeder Teilfolge (TF) $(X_{n'})$ von $(X_n)_{n\in\N}$ existiert eine Teilfolge $(X_{n''})$ von $(X_{n'})$ derart, dass $X_{n''}
		\stackrel{n''\to\infty}{\longrightarrow} X$ $\P$-fast sicher.
	\end{enumerate}
\end{satz}

\begin{proof}
	Wie im Reellen.
\end{proof}

Mit dem Teilfolgenkriterium lassen sich Rechenregeln für fast sichere Konvergenz auf stochastische Konvergenz übertragen.

\begin{korollar}\label{Korollar3.14}\
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X\wedge X_n
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X'
			\implies X=X'\quad\P\text{-fast sicher}
		\end{aligned}$
		\item $\begin{aligned}
			X_n
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X\text{ in }(\S,d),~f:(\S,d)\to(\S',d')\text{ messbar mit $f$ stetig in $X$ $\P$-fast sicher }
		\end{aligned}$
		\begin{align*}
			\implies f(X_n)
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			f(X)
		\end{align*}
	\end{enumerate}
\end{korollar}

\begin{proof}
	\underline{Zeige (1):}
	\begin{align*}
		X_n
		\stackrelnew{n\to\infty}{\P}{\longrightarrow}
		X
		\stackrel{\ref{satz3.13}}{\implies}
		\exists\text{ TF }(X_{n'})\subseteq(X_n)_{n\in\N}\mit X_{n'}
		\stackrel{n'\to\infty}{\longrightarrow}
		X\text{ fast sicher}
	\end{align*}
	Zu $(X_{n'})$ existiert (wegen $X_n\stackrelnew{n\to\infty}{\P}{\longrightarrow} X'$ und Satz \ref{satz3.3}) eine Teilfolge
	$(X_{n''})\subseteq(X_{n'})$ mit
	\begin{align*}
		X_{n''}
		\stackrel{n\to\infty}{\longrightarrow} X'\text{ fast sicher}
		\stackrel{\ref{Satz3.7}}{\implies}
		X=X'\text{ fast sicher }
	\end{align*}

	\underline{Zeige (2):} Zur Übung.
\end{proof}

\subsection*{Konvergenz in Produkträumen} %noNumber
Seien $(\S,d)$ und $(\S',d')$ separable metrische Räume. Dann ist auch $(\S\times\S',d\times d')$ ein metrischer Raum. 
Dies folgt z. B. aus dem \textit{Satz von der koordinatenweise Konvergenz}:

\begin{align}\label{eq3.1KoordinatenweissKonvergenz}\tag{3.1}
	\big(a_n,a_n'\big)
	\stackrelnew{d\times d'}{n\to\infty}{\longrightarrow}
	(a,a')
	\Longleftrightarrow
	(a_n)
	\stackrelnew{d}{n\to\infty}{\longrightarrow}
	a
	\wedge
	(a_n')
	\stackrelnew{d'}{n\to\infty}{\longrightarrow}
	(a')
\end{align}

Es ``stochastische Versionen'' dieses Satzes.

\begin{satz}\label{satz3.15}\
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			(X_n,X_n')
			\stackrelnew{}{n\to\infty}{\longrightarrow}
			(X,X')~\P\text{-f.s.}
			\Longleftrightarrow
			X_n
			\stackrelnew{}{n\to\infty}{\longrightarrow}
			X~\P\text{-f.s. }\wedge
			X_n'
			\stackrelnew{}{n\to\infty}{\longrightarrow}
			X'~\P\text{-f.s.}
		\end{aligned}$
		\item $\begin{aligned}
			(X_n,X_n')
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			(X,X')
			\Longleftrightarrow
			X_n
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X\wedge
			X_n'
			\stackrelnew{n\to\infty}{\P}{\longrightarrow}
			X'
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zu (1):}
	\begin{align*}
		(1)\text{, linke Seite }
		&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
		X_n\to X,~X_n'\to X'\quad\P\text{-fast sicher}\\
		&\stackrel{\cap\text{ Eins-Mengen}}{\Longleftrightarrow}
		(1),\text{ rechte Seite}
	\end{align*}

	\underline{Zu (2):}
	\begin{align*}
		(2)\text{, linke Seite}
		&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
		\forall\text{ TF }(X_{n'},X_{n'}')\subseteq(X_n,X_n'):\\
		&\qquad\exists\text{ TTF }(X_{n''},X_{n''}')\subseteq(X_{n'},X_{n'}'):
		(X_{n''},X_{n''}')\stackrel{n\to\infty}{\longrightarrow}\text{ f.s.}
	\end{align*}
	Also wegen Teil (1) mit
	\begin{align*}
		X_{n''}\to X\text{ f.s. und }X_{n''}'\to X'\text{ f.s.}
	\end{align*}
	Somit:
	\begin{align*}
		\forall\text{ TF }(X_{n'})\subseteq(X_n):\exists\text{ TTF }(X_{n''})\subseteq(X_{n'}):X_{n''}\to X\text{ f.s. }
		&\stackrel{\eqref{eq3.1KoordinatenweissKonvergenz}}{\Longleftrightarrow}
		X_n\stackrelnew{n\to\infty}{\P}{\longrightarrow} X
	\end{align*}
	Und es gilt analog: $X_n'\to X'$.
\end{proof}

\subsection*{Gleichheit in Verteilung} %NoNumber
\begin{definition}\label{def3.16}
	Zufallsvariablen $X,Y$ in $(\S,d)$ über $(\Omega,\A,\P)$ heißen \textbf{gleich in Verteilung}, in Zeichen $X\stackeq{\mathcal{L}} Y$
	\begin{align*}
		:\Longleftrightarrow \P\circ X^{-1}\equiv\P\circ Y^{-1}
	\end{align*}
\end{definition}

\begin{bemerkung} %noNomber
	Def 3.16 kann erweitert werden auf Zufallsvariablen $X:(\Omega,\A,\P)\to(\S,d)$ und $Y:(\tilde{\Omega},\tilde{\A},\tilde{\P})\to(\S,d)$ durch
	\begin{align*}
		X\stackeq{\mathcal{L}} Y:\Longleftrightarrow\P\circ X^{-1}\equiv\tilde{\P}\circ Y^{-1}
	\end{align*}
\end{bemerkung}

Charakterisierung von Verteilungsgleichheit in folgendem Satz:

\begin{satz}\label{satz3.17}\
	\begin{enumerate}[label=(\arabic*)]
		\item Seien $\P,Q$ Wahrscheinlichkeitsmaße auf $\B(\S)$. Dann gilt:
		\begin{align*}
			\P\equiv Q\Longleftrightarrow
			\int\limits f\d\P=\int\limits f\d Q\qquad\forall f\in C^b(\S)\text{ glm. stetig}
		\end{align*}
		\item $\begin{aligned}
			X\stackeq{\mathcal{L}} Y\Longleftrightarrow
			\E\big[f(X)\big]=\E\big[f(Y)\big]\qquad\forall f\in C^b(\S)\text{ glm. stetig}
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zu (1) zeige ``$\implies$'':} Klar.\nl
	\underline{Zu (1) zeige ``$\Longleftarrow$'':}
	\begin{align*}
		\B(\S)\stackeq{\text{3.2 (1)}}\sigma\big(\mathcal{F}(\S)\big)
	\end{align*}
	und $\F$ ist Durchschnittsstabil. Wegen dem Maßeindeutigkeitssatz reicht es zu zeigen:
	\begin{align*}
		\P(F)=Q(F)\qquad\forall F\in\mathcal{F}(\S)
	\end{align*}
	Sei $F\subseteq\S$ abgeschlossen. Setze
	\begin{align*}
		f_k(x):=\varphi\Big(k\cdot d(x,F)\Big)
	\end{align*}
	(vgl. Satz 2.4).
	Aus dem Lemma \ref{lemma2.3} folgt, dass die $f_k$ beschränkt und gleichmäßig stetig sind mit $f_k\stackrel{k\to\infty}{\downarrow}\indi_F$. 
	Also gilt:
	\begin{align*}
		\P(F)
		&\stackeq{\text{Def}}
		\int\limits\indi_F\d\P
		=\int\limits\lim\limits_{k\to\infty} f_k\d\P
		\stackeq{\text{Mono-Konv}}
		\lim\limits_{k\to\infty}\int\limits f_k\d\P
		\stackeq{\text{Vor}}
		\lim\limits_{k\to\infty}\int\limits f_k\d Q\\
		&\stackeq{\text{Mono-Konv}}
		\int\limits\lim\limits_{k\to\infty} f_k\d Q
		=\int\limits\indi_F\d Q
		\stackeq{\text{Def}} Q(F)
	\end{align*}
	Da $F$ beliebig war, folgt die Behauptung.\nl
	\underline{Zu (2):} folgt aus (1) mit dem Transformationssatz \eqref{eqTrafo}:
	\begin{align*}
		X\stackeq{\mathcal{L}} Y
		&\stackrel{\ref{def3.16}}{\Longleftrightarrow}
		\underbrace{\P\circ X^{-1}}_{}=\underbrace{\P\circ Y^{-1}}_{}\\
		&\stackrel{(1)}{\Longleftrightarrow}
		\int\limits_{\S} f\d(\P\circ X^{-1})=\int\limits_{\S} f\d(\P\circ Y^{-1})
	\end{align*}
	und
	\begin{align*}
		\int\limits_{\S} f\d(\P\circ X^{-1})
		\stackeq{\text{Trafo}}
		\int\limits_{\Omega}\underbrace{f\circ X}_{=:f(X)}\d\P
		\stackeq{\text{Def}}
		\E\big[f(X)\big]
	\end{align*}
\end{proof}

\section{Verteilungskonvergenz von Zufallsvariablen in metrischen Räumen}
Seien $X,X_n,n\in\N$ Zufallsvariablen in $(\S,d)$ über $(\Omega,\A,\P)$. 
Dann sind
%st die Verteilung von $X$, 
\begin{align*}
	P:=\P\circ X^{-1},\qquad P_n:=\P\circ X_n^{-1},\qquad n\in\N
\end{align*}
sind Wahrscheinlichkeitsmaße auf $\B(\S)$.

\begin{definition}[Verteilungskonvergenz]\label{def4.1}\
	\begin{enumerate}[label=(\arabic*)]
		\item Seien $P,P_n,n\in\N$ Wahrscheinlichkeitsmaße auf $\B(S)$. 
		Dann \textbf{konvergiert $P_n$ schwach gegen $P$}, in Zeichen
		\begin{align*}
			P_n\stackrelnew{w}{n\to\infty}{\longrightarrow} P
			:\Longleftrightarrow
			\int\limits f\d P_n\stackrel{n\to\infty}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)
		\end{align*}
		Das $w$ steht für "weakly".
		\item $X_n$ \textbf{konvergiert in Verteilung gegen $X$ in Raum $(\S,d)$}, in Zeichen
		\begin{align*}
			X_n\stackrel{\mathcal{L}}{\longrightarrow} X\text{ in }(\S,d)
			:\Longleftrightarrow
			\P\circ X_n^{-1}\stackrelnew{w}{n\to\infty}{\longrightarrow}\P\circ X^{-1}
		\end{align*}
		Alternative Schreibweise: $X_n\stackrel{\d}{\longrightarrow} X$. Das $\L$ steht für "law".
	\end{enumerate}
\end{definition}

Äquivalente Charakterisierung von $\stackrelnew{w}{}{\longrightarrow}$ bzw. $\stackrel{\L}{\longrightarrow}$ in folgendem Satz:

\begin{satz}[Portmanteau-Theorem]\enter\label{satz4.2}
	Folgende Aussagen sind äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			P_n\stackrelnew{w}{}{\longrightarrow} P
		\end{aligned}$
		\item $\begin{aligned}
			\int\limits f\d P_n\stackrel{}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)\text{ glm. stetig}
		\end{aligned}$
		\item $\begin{aligned}
			\limsup\limits_{n\to\infty} P_n(F)\leq P(F)\qquad\forall F\in\F(\S)
		\end{aligned}$
		\item $\begin{aligned}
			\liminf\limits_{n\to\infty} P_n(G)\geq P(G)\qquad\forall G\in\G(\S)
		\end{aligned}$
		\item $\begin{aligned}
			\limn P_n(B)=P(B)\qquad\forall B\in\B(\S)\mit P(\underbrace{\partial B}_{\in\F(\S)})=0
		\end{aligned}$\\
		Mengen $B\in\B(\S)$ mit $P(\partial B)=0$ heißen \textbf{$P$-randlos}.
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zeige (1) $\implies$ (2):}\\
	Folgt aus der Definition \ref{def4.1} (1).\nl
	\underline{Zeige (2) $\implies$ (3):}\\
	Sei $F\in\F(\S)$ (also abgeschlossen), Der Beweis von Satz \ref{satz3.17} zeigt: 
	Es gibt eine Folge $(f_k)_{k\in\N}$ von gleichmäßig stetigen, beschränkten Funktionen auf $\S$ mit $f_k\downarrow\indi_F$. 
	Dann gilt:
	\begin{align*}
		\limsup\limits_{n\to\infty} P_n(F)
		=\limsup\limits_{n\to\infty}\int\limits\underbrace{\indi_F}_{\leq f_k~\forall k\in\N}\d P_n
		\overset{\text{Mono}}&{\leq}
		\limsup\limits_{n\to\infty}\int\limits f_k\d P_n
		\stackrel{\text{Vor (2)}}{=}
		\int\limits f_k\d P~~\forall k\in\N\\
		\overset{\text{Mono Konv}}&{\implies}
		\int\limits f_k\d P\stackrel{k\to\infty}{\longrightarrow}
		\int\limits\indi_F\d P=P(F)\\
		\overset{k\to\infty}&{\implies}
		~(3)
	\end{align*}

	\underline{Zeige (3) $\Longleftrightarrow$ (4):}\\
	Nutze Übergang zum Komplement sowie Rechenregeln für $\liminf$ und $\limsup$:
	\begin{align*}
		\liminf\limits_{n\to\infty} P_n(G)
		&=\liminf\limits_{n\to\infty} \big(1-P_n(G^C)\big)\\
		&=1-\underbrace{\limsup\limits_{n\to\infty} P_n(\underbrace{G^C}_{\in\F})}_{\leq P(G^C)}\\
		&\geq 1-P(G^C)\\
		&=P(G)
	\end{align*}

	\underline{Zeige (3) $\implies$ (1):}\\
	Sei $f\in C^b(\S)$ beliebig. Zu zeigen:
	\begin{align}\label{eqProof1.4.2Sternchen}
		\limsup\limits_{n\to\infty}\int\limits f\d P_n\leq\int\limits f\d P
	\end{align}
	\underline{1. Schritt:} Sei $0\leq f<1$. Setze
	\begin{align*}
		F_i:=\left\lbrace f\geq\frac{i}{k}\right\rbrace=\left\lbrace x\in\S:f(x)\geq\frac{i}{k}\right\rbrace,\qquad \forall 0\leq i\leq k,k\in\N
	\end{align*}
	Dann gilt $F_i\in\F~\forall i$, da $f$ stetig. Da 
	\begin{align*}
		\int\limits_{\S}f\d P
		\stackeq{\text{Lin}}
		\sum\limits_{i=1}^k\int\limits\indi_{\left\lbrace\frac{i-1}{k}\leq f<\frac{i}{k}\right\rbrace}\cdot f\d P
	\end{align*}
	folgt wegen Monotonie
	\begin{align}\label{eqProof1.4.2Plus}
		\sum\limits_{i=1}^k\underbrace{\frac{i-1}{k}}_{=\frac{i}{k}-\frac{1}{k}}\cdot P\left(\frac{i-1}{k}\leq f<\frac{i}{k}\right)
		\leq
		\int\limits f\d P
		\leq
		\sum\limits_{i=1}^k \frac{1}{k}\cdot P\Big(\underbrace{\frac{i-1}{k}\leq f<\frac{i}{k}}_{F_{i-1}\setminus F_i}\Big)
	\end{align}
	Die rechte Summe in \eqref{eqProof1.4.2Plus} ist gleich
	\begin{align*}
		&\frac{1}{k}\cdot\sum\limits_{i=1}^k i\cdot\big( P(F_{i-1}-P(F_i)\big)\\
		&=\frac{1}{k}\cdot\Big(P(F_0)-P(F_1)+2\cdot P(F_1-2\cdot P(F_2)+3\cdot P(F_2)-3\cdot P(F_3)+\\
		&\qquad+\ldots+(k-1)\cdot P(F_{k-2})-(k-1)\cdot P(F_{k-1})+k\cdot P(F_{k-1})-k\cdot P(F_k)\Big)\\
		&=\frac{1}{k}\cdot\Big(\underbrace{P(F_0)}_{=1}+P(F_1)+P(F_2)+\ldots+P(F_{k-1})-k\cdot k\cdot \underbrace{P(F_k)}_{=0}\Big)\\
		&=\frac{1}{k}+\frac{1}{k}\cdot\sum\limits_{i=1}^{k-1} P(F_i)
	\end{align*}
	Da die linke Summe in \eqref{eqProof1.4.2Plus} gleich der rechten Summe minus $\frac{1}{k}$, folgt
	\begin{align}\label{eqProof1.4.2DoppelSternchen}
		\sum\limits_{i=1}^{k-1} P(F_i)
		\leq\int\limits f\d P
		\leq\frac{1}{k}+\sum\limits_{i=1}^{k-1} P(F_i)
	\end{align}
	Beachte, \eqref{eqProof1.4.2DoppelSternchen} gilt für \ul{jedes} Wahrscheinlichkeitsmaß $P$, also auch für $P_n$. 
	Damit folgt:
	\begin{align*}
		\limsup\limits_{n\to\infty}\int\limits f\d P_n
		&\leq\frac{1}{k}+\sum\limits_{i=1}^{k-1}\underbrace{\limsup\limits_{n\to\infty} P_n(F_i)}_{\stackrel{(3)}{\leq}P(F_i)~\forall i}\\
		&\leq\frac{1}{k}+\underbrace{\sum\limits_{i=1}^{k-1} P(F_i)}_{\stackrel{\eqref{eqProof1.4.2DoppelSternchen}}{\leq}\int\limits f\d P}\\
		&\leq\frac{1}{k}+\int\limits f\d P\qquad\forall k\in\N
	\end{align*}
	Grenzwertbildung $k\to\infty$ liefert \eqref{eqProof1.4.2Sternchen}.\nl
	\ul{2. Schritt:} Da $f\in C^b(\S)$ beliebig, gilt:
	\begin{align*}
		\exists a<b:a\leq f<b
		\implies g(x):=\frac{f(x)-a}{b-a}\text{ ist stetig und } 0\leq g<1
	\end{align*}
	Daraus folgt
	\begin{align*}
		\limsup\limits_{n\to\infty}\int\limits f\d P_n
		&=\limsup\limits_{n\to\infty}\int\limits (b-a)\cdot g+a\d P_n\\
		&=\limsup\limits_{n\to\infty}\left((b-a)\cdot\int\limits g\d P_n+a\right)\\
		&\leq(b-a)\cdot\underbrace{\limsup\limits_{n\to\infty}\int\limits g\d P_n}_{\leq\int\limits g\d P\text{, wg. 1. Schritt}}+a\\
		&\leq(b-a)\cdot\int\limits g\d P+0\\
		\overset{\text{Lin}}&=
		\int\limits f\d P
	\end{align*}
	Damit ist \eqref{eqProof1.4.2Sternchen} gezeigt. Übergang zu $-f$ in \eqref{eqProof1.4.2Sternchen} liefert
	\begin{align*}
		\int\limits f\d P
		&\leq
		\liminf\limits_{n\to\infty}\int\limits f\d P_n\\
		&=\liminf\limits_{n\to\infty}-\int\limits -f\d P_n\\
		&=-\limsup\limits_{n\to\infty}-\int\limits \underbrace{-f}_{\in C^b(\S)}\d P\\
		\overset{\eqref{eqProof1.4.2Sternchen}}&{\geq}
		-\int\limits -f\d P\\
		\overset{\text{Lin}}&=
		\int\limits f\d P
		\qquad\forall f\in C^b(\S)\\
		&\implies(1)
	\end{align*}

	\underline{Zeige (3) $\implies$ (5):}\\
	Sei $B\in\B(\S)\mit P(\partial B)=0$. Dann gilt:
	\begin{align*}
		P(\overline{B})
		\overset{(3)}&{\geq}
		\limsup\limits_{n\to\infty} \underbrace{P_n(\overbrace{\overline{B}}^{\supseteq B})}_{\geq P_n(B)}\\
		&\geq\limsup\limits_{n\to\infty} P_n(B)\\
		\overset{\text{stetig}}&{\geq}
		\liminf\limits_{n\to\infty} \underbrace{P_n(\overbrace{B}^{\supseteq\stackrel{\circ}{B}})}_{P_n(\stackrel{\circ}{B}}\\
		\overset{(3)\gdw(4)}&{\geq}
		P(\stackrel{\circ}{B})\\
		&=P(\overline{B})\\
		&=P(B),
	\end{align*}
	denn:
	\begin{align}\label{eqProof1.4.2SternchenZwei}
		0
		=P(\overbrace{\partial B}^{\overline{B}\setminus\stackrel{\circ}{B}})
		=P(\overline{B})-P(\stackrel{\circ}{B})
		\implies
		P(\stackrel{\circ}{B})\leq P(B)\leq P(\overline{B})=P(\stackrel{\circ}{B})
	\end{align}
	Da $\liminf=\limsup$ folgt $\limn P_n(B)=P(B)$.\nl
	\underline{Zeige (5) $\implies$ (3):}\\
	Sei $F\in\F$ (abgeschlossen) beliebig. Dann gilt $\forall\varepsilon>0:$
	\begin{align}\label{eqProof1.4.2SternchenUnten}
		\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)
		\subseteq\big\lbrace x\in\S:d(x,F)=\varepsilon\big\rbrace
	\end{align}
	denn: Sei $x\in\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)$. Dann gilt:
	\begin{align*}
		&\exists (x_n)_{n\in\N}:\forall n\in\N:d(x_n,F)\leq\varepsilon\wedge \limn x_n=x\\
		&\exists (\xi_n)_{n\in\N}:\forall n\in\N:d(\xi_n,F)>\varepsilon\wedge\limn\xi_n=x
	\end{align*}
	Da $d(\cdot,F)$ stetig gemäß \ref{lemma2.3} (3), folgt
	\begin{align*}
		\varepsilon\leq d(x,F)\leq\varepsilon.
	\end{align*}
	Wegen \eqref{eqProof1.4.2SternchenUnten} sind 
	\begin{align*}
		A_\varepsilon:=\partial\Big(\big\lbrace x\in\S:d(x,F)\leq\varepsilon\big\rbrace\Big)\qquad\forall\varepsilon>0
	\end{align*}
	paarweise disjunkt, da bereits die Obermengen paarweise disjunkt sind. Dann folgt
	\begin{align}\label{eqProof1.4.2DoppelSternchenUnten}
		E:=\big\lbrace\varepsilon>0:P(A_\varepsilon)>0\big\rbrace\text{ ist höchstens abzählbar},
	\end{align}
	denn:
	\begin{align*}
		E=\bigcup\limits_{n\in\N}\underbrace{\left\lbrace\varepsilon>0:P(A_\varepsilon)\geq\frac{1}{m}\right\rbrace}_{=:E_m}
	\end{align*}
	Es gilt $|E_m|\leq m$, weil: Angenommen es existieren $0<\varepsilon_1<\ldots<\varepsilon_{m+1}$ mit 
	\begin{align*}
		&P(A_{\varepsilon_i})\geq\frac{1}{m}\qquad\forall 1\leq i\leq m+1\\
		&\implies
		1\geq P\left(\bigcup\limits_{i=1}^{m+1} A_{\varepsilon_i}\right)
		\stackeq{\text{pw. disj.}}
		\sum\limits_{i=1}^{m+1}\underbrace{P\big(A_{\varepsilon_i}\big)}_{\geq\frac{1}{m}}\geq(m+1)\cdot\frac{1}{m}>1
	\end{align*}
	Das ist ein Widerspruch. 
	Damit ist $E$ höchstens abzählbar unendlich. 
	Damit liegt das Komplement
	\begin{align*}
		E^C=\big\lbrace\varepsilon>0: P(A_\varepsilon)=0\big\rbrace
	\end{align*}
	dicht in $[0,\infty)$.
	(dies kann man auch durch Widerspruch zeigen)\\
	Daraus folgt insbesondere:
	\begin{align*}
		\exists(\varepsilon_k)_{k\in\N}\subseteq\R\mit\varepsilon_k\downarrow0:\forall k\in\N:
		F_k:=\big\lbrace x\in\S:d(x,F)\leq\varepsilon_k\big\rbrace\text{ ist $P$-randlos}
	\end{align*}
	Beachte $A_{\varepsilon_k}=\partial F_k$. Da $F\subseteq F_k~\forall k\in\N$,gilt:
	\begin{align*}
		&\limsup\limits_{n\to\infty} P_n(F)
		\leq\limsup\limits_{n\to\infty} \underbrace{P_n(F_k)}_{\text{konv.}}
		\stackeq{(5)}
		P(F_k)\qquad\forall k\in\N\\
		&\stackrel{k\to\infty}{\implies}
		\limsup\limits_{n\to\N} P_n(F)
		\leq\lim\limits_{k\to\infty} P(F_k)
		=P(F)
	\end{align*}
	Die letzte Gleichheit gilt, weil $P$ $\sigma$-stetig von oben ist und $F_k\downarrow F$. 
	$F_k\downarrow F$, denn $F_1\supseteq F_2\supseteq\ldots$, da $\varepsilon_k$ monoton fallende Folge ist und
	\begin{align*}
		\bigcap\limits_{k\in\N}F_k=F, 
	\end{align*}
	denn: 
	\begin{align*}
		x\in\bigcap\limits_{k\in\N}F_k
		&\Longleftrightarrow
		x\in F_k\qquad\forall k\in\N\\
		&\Longleftrightarrow
		d(x,F)\leq\varepsilon_k\qquad\forall k\in\N\\
		&\implies
		d(x,F)=0\\
		\overset{\ref{lemma2.3}~(1)}&{\Longleftrightarrow}
		x\in \overline{F}\stackeq{F\in\F}F
	\end{align*}
\end{proof}

Mitunter folgt schwache Konvergenz aus $P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)$ für eine spezielle Klasse von Mengen $A$.

\begin{theorem}\label{theorem4.3}
	Sei $\U\subseteq\B(\S)$ mit
	\begin{enumerate}[label=(\roman*)]
		\item $\begin{aligned}
			A,B\in \U\implies A\cap B\in\U
		\end{aligned}$, also $\U$ ist endlich $\cap$-stabil
		\item Jedes offene $G$ ist abzählbare Vereinigung von Mengen aus $\U$.
	\end{enumerate}
	Dann gilt:
	\begin{align*}
		\Big(\forall A\in\U:P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)\Big)\implies P_n\stackrelnew{w}{}{\longrightarrow} P
	\end{align*}
\end{theorem}

\begin{proof}
	Seien $A_1,\ldots,A_m\in\U$. Dann gilt:
	\begin{align*}
		P_n\left(\bigcup\limits_{i=1}^m A_i\right)
		\overset{\text{allg. Add-Formel}}&=
		\sum\limits_{k=1}^m (-1)^{k-1}\cdot\sum\limits_{1\leq i_1<\ldots<i_k\leq m}\underbrace{P_n\big(\underbrace{A_{i_1}\cap\ldots\cap A_{i_k})}_{\in\U}}_{\stackrel{n\to\infty}{\longrightarrow} P(A_{i_1}\cap\ldots\cap A_{i_k})}\\
		\overset{n\to\infty}&{\longrightarrow}
		\sum\limits_{k=1}^m\sum\limits_{1\leq i_1<\ldots<i_k\leq m} P\left(A_{i_1}\cap\ldots\cap A_{i_k}\right)\\
		\overset{\text{all. Add.}}&=
		P\left(\bigcup\limits_{i=1}^m A_i\right)
	\end{align*}
	Sei $G\in\G$. Dann gilt wegen Voraussetzung (ii):
	\begin{align*}
		&\exists(A_i)_{i\in\N}\subseteq\U:G=\bigcup\limits_{i\in\N} A_i\\
		&\implies
		G_m:=\bigcup\limits_{i=1}^m A_i\uparrow G,~m\to\infty\\
		\overset{P~\sigma\text{-stetig}}&{\implies}
		\forall \varepsilon>0:\exists m_0\in\N:P(G)-P(G_{m_0})\leq\varepsilon\\
		&\implies
		P(G)-\varepsilon\leq P(G_{m_0})
		\stackeq{\text{s. o.}}
		\limn P_n(\underbrace{G_{m_0}}_{\subseteq G})\leq\liminf\limits_{n\to\infty} P_n(G)\qquad\forall\varepsilon>0\\
		\overset{\varepsilon\to0}&{\implies}
		\liminf\limits_{n\to\infty} P_n(G)\geq P(G)\qquad\forall G\in\G
	\end{align*}
	Nun folgt die Behauptung aus dem Theorem \ref{satz4.2}.
\end{proof}

\begin{korollar}\label{korollar4.4}
	Sei $\U$ endlich Durschnittsstabil mit
	\begin{enumerate}[label=(\roman*)]
		\item $\begin{aligned}
			\forall x\in S,\forall\varepsilon>0:\exists A\in\U:x\in\stackrel{\circ}{A}\subseteq A\subseteq B(x,\varepsilon)
		\end{aligned}$
	\end{enumerate}
	Ist $(\S,d)$ separabel, so gilt
	\begin{align*}
		\Big(\forall A\in\U:P_n(A)\stackrel{n\to\infty}{\longrightarrow} P(A)\Big)
		\implies P_n\stackrelnew{w}{}{\longrightarrow} P
	\end{align*}
\end{korollar}

\begin{proof}
	Gemäß Satz \ref{satz2.9} hat $\G$ abzählbare Basis. 
	Nach dem Satz von Lindelöf:
	\begin{align}\label{eqSatzVonLindelöf}\tag{L}
		\text{Für jede offene Überdeckung einer beliebigen Teilmenge von $\S$ existiert}\\\nonumber\text{eine abzählbare Teilüberdeckung.}
	\end{align}
	Sei nun $G\in\G$ beliebig. 
	Für alle $x\in G$ existiert ein $\varepsilon_x>0$ mit $B(x,\varepsilon_x)\subseteq G$.
	Gemäß (i) findet man ein $A_x\in\U$ mit $x\in\stackrel{\circ}{A}_x\subseteq A_x\subseteq B(x,\varepsilon_x)\subseteq G$. 
	Also folgt
	\begin{align*}
		G=\bigcup\limits_{x\in G}\lbrace x\rbrace\subseteq\bigcup\limits_{x\in G}\stackrel{\circ}{A}_x\subseteq G
	\end{align*}
	Somit ist $\left\lbrace\stackrel{\circ}{A}_x:x\in G\right\rbrace$ eine offene Teilüberdeckung von $G$. 
	Aus \eqref{eqSatzVonLindelöf} folgt nun: 
	Es existieren $A_{x_i}\in\U,~i\in\N$ mit
	\begin{align*}
		G\subseteq\bigcup\limits_{i\in\N}\stackrel{\circ}{A}_{x_i}\subseteq 
		\bigcup\limits_{i\in\N}A_{x_i}\subseteq 
		G
		\implies
		G=\bigcup\limits_{i\in\N}A_{x_i}
	\end{align*}
	Also erfüllt $\U$ die Voraussetzung (i) und (ii) in Theorem \ref{theorem4.3} und es folgt die Behauptung.
\end{proof}

Als Anwendung / Beschreibung der schwachen Konvergenz im $\S=\R$. 
Erinnere an \textbf{schwache Konvergenz von Verteilungsfunktionen (VF)} $(F_n)_{n\in\N}$ gegen $F$, in Zeichen
\begin{align*}
	F_n\rightharpoonup F
	:\Longleftrightarrow
	F_n(x)\stackrel{n\to\infty}{\longrightarrow} F(x)\qquad\forall x\in C_F
	\mit C_F:=\big\lbrace x\in\R:F\text{ ist stetig in }x\big\rbrace
\end{align*}

\begin{korollar}\label{korollar4.5}
	Seien $P,P_n,n\in\N$ Wahrscheinlichkeitsmaße auf $\B(\R)$ mit zugehörigen Verteilungsfunktionen $F$ und $F_n,n\in\N$. 
	Dann gilt:
	\begin{align*}
		P_n\stackrelnew{w}{}{\longrightarrow}P
		\Longleftrightarrow
		F_n\rightharpoonup F
	\end{align*}
\end{korollar}

\begin{proof}
	\underline{Zeige ``$\implies$'':}\\
	Sei $B:=(-\infty,x],~x\in\R$. 
	Dann gilt:
	\begin{align*}
		P(\underbrace{\partial B}_{=\lbrace x\rbrace})=0
		&\Longleftrightarrow P(\lbrace x\rbrace)=F(x)-\underbrace{F(x-0)}{\text{Grenzwert}}=0\\
		& x\in C_F
	\end{align*}
	Somit folgt für $x\in C_F$:
	\begin{align*}
		F_n(x)&\stackeq{\text{Def}}
		P_n\big(\underbrace{(-\infty,x]}_{=B}\big)=P_n(B)\stackrel{n\to\infty}{\longrightarrow} P(B)\stackeq{\text{Def}} F(x)\qquad\forall x\in C_F
	\end{align*}
	gemäß Satz \ref{satz4.2}.\nl
	\underline{Zeige ``$\Longleftarrow$'':} Sei
	\begin{align*}
		\U:=\big\lbrace (a,b]:a,b\in C_F\big\rbrace.
	\end{align*}
	Dann ist $\U$ endlich durchschnittsstabil. Ferner: Die Menge 
	\begin{align*}
		D_F:=\big\lbrace x\in\R: F\text{ \underline{nicht} stetig in }x\big\rbrace
		=\big\lbrace x\in\R:P(\lbrace x\rbrace)>0\big\rbrace
	\end{align*}
	ist höchstens abzählbar (vgl. \eqref{eqProof1.4.2DoppelSternchen} 
	%keine Ahnung welches DoppelSternchen er hier meint mit der Referenz %TODO
	im Beweis von Satz \ref{satz4.2}). Also folgt
	\begin{align*}
		\forall x\in\R:\forall\varepsilon>0:\exists A=(a,b]\in\U:
		x\in (a,b)=\stackrel{\circ}{A}\subseteq A=(a,b]\subseteq B(x,\varepsilon)=(x-\varepsilon,x+\varepsilon)
	\end{align*}
	denn: 
	In $(x-\varepsilon,x+\varepsilon)$ muss ein $a\in C_F$ existieren, denn sonst wäre $(x-\varepsilon, x)\subseteq D_F$. 
	Analog findet man ein $b\in(x,x+\varepsilon)$. 
	Somit erfüllt $\U$ die Voraussetzungen von Korollar \ref{korollar4.4}. 
	Klar: $\S=\R$ ist separabel, da $\Q$ abzählbar und dicht in $\R$. 
	Schließlich gilt:
	\begin{align*}
		P_n\big((a,b]\big)&=
		F_n(\underbrace{b}_{\in C_F})-F_n(\underbrace{a}_{\in C_F})\stackrel{n\to\infty}{\longrightarrow} F(b)-F(a)
		=P\big((a,b]\big)\qquad\forall a,b\in C_F\\
		&\stackrel{\ref{korollar4.4}}{\implies}
		P_n\stackrelnew{w}{}{\longrightarrow} P
\end{align*}
\end{proof}

\begin{bemerkung}\ %4.6
	\begin{enumerate}[label=(\arabic*)]
		\item Seien $X,X_n,n\in\N$ reelle Zufallsvariablen über $(\Omega,\A,\P)$. Dann:
		\begin{align}\label{eqBemerkung4.6}\tag{$\ast$} 
			X_n\stackrel{\L}{\longrightarrow} X
			\stackrel{\text{Def}}{\Longleftrightarrow}
			\underbrace{\P\circ X_n^{-1}}_{\hat{=}P_n}
			\stackrelnew{w}{}{\longrightarrow} \underbrace{\P\circ X^{-1}}_{\hat{=}P}
			\stackrel{\ref{korollar4.5}}{\Longleftrightarrow}
			\underbrace{\P(X_n\leq x)}_{\hat{=}F_n(x)}
			\stackrel{n\to\infty}{\longrightarrow}
			\underbrace{\P(X\leq x)}_{\hat{=}F(x)}
		\end{align}
		für alle $x$, die Stetigkeitsstellen der Verteilungsfunktion von $X$ sind.
		\item Es gibt Verallgemeinerung von \ref{korollar4.5} bzw \eqref{eqBemerkung4.6} auf $\S=\R^k$:\\
		Seien 
		\begin{align*}
 		   X=\left(X^{(1)},\ldots, X^{(k)}\right),X_n=\left(X^{(1)}_n,\ldots,X^{(k)}_n\right),\qquad(\Omega,\A)\to\left(\R^k,\B(\R^k)\right)
		\end{align*}
		Zufallsvariablen in $\R^k$. Dann gilt:
		\begin{align*}
			X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^k
			\Longleftrightarrow
			\P(X_n\leq x)\stackrel{n\to\infty}{\longrightarrow}\P(X\leq x)\qquad\forall x=\big(x_1,\ldots,x_k\big)\in\R^k
		\end{align*}
		wobei $x_i$ Stetigkeitsstelle der Verteilungsfunktion von $X^{(i)}$ ist für alle $i\in\lbrace1,\ldots,k\rbrace$. 
		Beweis ist analog zu \ref{korollar4.5}.
	\end{enumerate}
\end{bemerkung}

Der schwache Limes einer Folge $(P_n)_{n\in\N}$ ist eindeutig, denn es gilt:

\begin{lemma}\label{lemma4.6Einhalb}
	\begin{align*}
		P_n\stackrelnew{w}{}{\longrightarrow} P,~P_n\stackrelnew{w}{}{\longrightarrow} Q\implies P=Q
	\end{align*}
\end{lemma}

\begin{proof}
	Gemäß Definition gilt:
	\begin{align*}
		\int\limits f\d P_n&\stackrel{}{\longrightarrow}\int\limits f\d P\qquad\forall f\in C^b(\S)\\
		\int\limits f\d P_n&\stackrel{}{\longrightarrow}\int\limits f\d Q\qquad\forall f\in C^b(\S)
	\end{align*}
	Der Grenzwert von reellen Zahlenfolgen eindeutig ist, folgt
	\begin{align*}
		\int\limits f\d P=\int\limits f\d Q\qquad\forall f\in C^b(\S)\\
		\stackrel{\ref{satz3.17}}{\implies}
		P=Q
	\end{align*}
\end{proof}

Im Folgenden ist das Ziel die Übertragung unserer Resultate auf Verteilungskonvergenz.

\begin{satz}[Portmanteau-Theorem]\label{satz4.7}\enter
	Folgende Aussagen sind äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)
		\end{aligned}$
		\item $\begin{aligned}
			\E\big[f(X_n)\big]\stackrel{n\to\infty}{\longrightarrow}\E\big[f(X)\big]\qquad\forall f\in C^b(\S)
		\end{aligned}$ gleichmäßig stetig
		\item $\begin{aligned}
			\limsup\limits_{n\to\infty}\P(X_n\in F)\leq\P(X\in F)\qquad\forall F\in\F
		\end{aligned}$
		\item $\begin{aligned}
			\liminf\limits_{n\to\infty}\P(X_n\in G)\geq\P(X\in G)\qquad\forall G\in\G
		\end{aligned}$
		\item $\begin{aligned}
			\P(X_n\in B)\stackrel{n\to\infty}{\longrightarrow}\P(X\in B)\qquad\forall B\in\B(\S)\mit\P(X\in\partial B)=0
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	Wende Satz \ref{satz4.2} an auf $P_n:=\P\circ X^{-1}_n,~P:=\P\circ X^{-1}$ (wegen Def $\stackrel{\L}{\longrightarrow}$). 
	Beachte z. B.
	\begin{align*}
		P_n(F)&=\P\circ X^{-1}_n(F)
		\stackeq{\text{Def}}
		\P\left(X_n^{-1}(F)\right)
		=\P\big(\lbrace\omega\in\Omega:X_n(\omega)\in F\rbrace\big)
		=\P(X_n\in F)
	\end{align*}
	und 
	\begin{align*}
		\int\limits f\d P_n
		=
		\int\limits_{\S} f\d(\P\circ X_n^{-1})
		\stackeq{\text{Trafo}}
		\int\limits_\Omega f(X_n)\d\P
		=\E\big[f(X_n)\big]
	\end{align*}
\end{proof}

Ferner erhält man unter den jeweiligen Voraussetzungen in Theorem \ref{theorem4.3} bzw. Korollar \ref{korollar4.4}:
\begin{align*}
	\P(X_n\in A)\stackrel{n\to\infty}{\longrightarrow}\P(X\in A)\qquad\forall A\in\U\\
	\implies X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)
\end{align*}
Und aus Lemma \ref{lemma4.6Einhalb}:
\begin{align*}
	X_n\stackrel{\L}{\longrightarrow} X,X_n\stackrel{\L}{\longrightarrow} X'\implies X\stackeq{\L}X'
\end{align*}

\subsection*{Das Continuous Mapping Theorem (CMT)}
Sei $h:(\S,d)\to(\S',d')$ messbar.\\
\underline{Ziel:} Finde Bedingungen an $h$, so dass gilt:
\begin{enumerate}[label=(\arabic*)]
	\item $\begin{aligned}
		P_n\stackrelnew{w}{}{\longrightarrow}P\implies P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
	\end{aligned}$
	\item $\begin{aligned}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }(\S,d)\implies h(X_n)\stackrel{\L}{\longrightarrow} h(X)\text{ in }(\S',d')
	\end{aligned}$
\end{enumerate}
Beachte: 
\begin{align*}
	\P\circ\big(h(X_n)\big)^{-1}&=\P\circ(h\circ X_n)^{-1}=\big(\P\circ X_n^{-1}\big)\circ h^{-1}\\
	\P\big(h(X)\big)^{-1}&=\big(\P\circ X^{-1}\big)\circ h^{-1}
\end{align*}
Also folgt (2) aus (1). Zunächst gilt (1), wenn $h$ stetig auf $\S$  ist, denn: Sei $f\in C^b(\S')$ beliebig. Dann gilt:
\begin{align*}
	\int\limits_{\S'} f\d \big(P_n\circ h^{-1}\big)
	\stackeq{\text{Trafo}}
	\int\limits_{\S} \underbrace{ f\circ h}_{\in C^b(\S)}\d P_n
	\stackrel{\ref{def4.1}}{\longrightarrow}\int\limits f\circ h\d P
	\stackeq{\text{Trafo}}
	\int\limits f\d(P\circ h^{-1})\\
	\stackrel{\ref{def4.1}}{\implies}
	P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
\end{align*}
Auf Stetigkeit von $h$ kann i. A. nicht verzichtet werden, denn es gilt:

\begin{beispiel} %4.8
	Sei $\S=\S'=[0,1],~h:[0,1]\to[0,1]$ mit
	\begin{align*}
		h(x):=\left\lbrace\begin{array}{cl}
			1, &\falls x\in\lbrace 0\rbrace\cup\left\lbrace\frac{1}{2\cdot n}:n\in\N\right\rbrace\\
			0, & \sonst
		\end{array}\right.
	\end{align*}
	Sei $P_n:=\delta_{\frac{1}{n}},n\in\N,P:=\delta_{0}$ wobei $\delta_x$ das Dirac-Maß bezeichnet.\\
	Dann gilt $P_n\stackrelnew{w}{}{\longrightarrow} P$, denn
	\begin{align*}
		\int\limits f\d P_n&=f\left(\frac{1}{n}\right)\stackrel{n\to\infty}{\longrightarrow} f(0)=\int\limits f\d P\qquad\forall f\in C^b\big([0,1]\big)
	\end{align*}
	Aber wegen 
	\begin{align*}
		\delta_x\circ h^{-1}=\delta_{h(x)}
	\end{align*}
	gilt für
	\begin{align*}
		Q_n:=P_n\circ h^{-1}=\delta_{\frac{1}{n}}\circ h^{-1}=\delta_{h\left(\frac{1}{n}\right)},\qquad Q:=\P\circ h^{-1}=\delta_1
	\end{align*}
	das Folgende:
	\begin{align*}
		\int\limits f\d Q_n=f\left(h\left(\frac{1}{n}\right)\right)=\left\lbrace\begin{array}{cl}
			f(1), & \falls n\text{ gerade }\\
			0, & \falls n\text{ ungerade }
		\end{array}\right.
	\end{align*}
	Sei $f\in C^b\big([0,1]\big)$ mit $f(0)\neq f(1)$ (z. B. $f=\id$). 
	Folglich ist die Folge $\left(\int f\d Q_n\right)_{n\in\N}$ divergent. 
	Somit:
	\begin{align*}
		\implies\int\limits f\d Q_n\not\longrightarrow\int\limits f\d Q\implies Q_n\stackrelnew{w}{}{\not\to} Q
	\end{align*}
\end{beispiel}

Die Forderung der Stetigkeit lässt sich aber abschwächen so, dass (1) noch gilt. 
Dazu definiere die Menge der Unstetigkeitsstellen
\begin{align*}
	D_h:=\big\lbrace x\in\S: h\text{ \underline{nicht} stetig in }x\big\rbrace.
\end{align*}

\begin{lemma}\label{lemma4.9}
	\begin{align*}
		D_h\in\B(\S)\qquad\forall h:\S\to\S'\text{ beliebig (nicht einmal messbar)}
	\end{align*}
\end{lemma}

\begin{proof}
	Mit der Dreiecksungleichung überlegt man sich leicht:
	\begin{align*}
		h\text{ stetig in }x
		\Longleftrightarrow
		&\forall 0<\varepsilon\in\Q:\exists 0<\delta\in\Q:\exists y,z\in\S:\\
		&d(x,y)<\delta\wedge d(x,z)<\delta
		\implies d'\big(h(y),h(z)\big)<\varepsilon
	\end{align*}
	Damit folgt:
	\begin{align}\label{eqProof4.9Plus}
		\bigcup\limits_{0<\varepsilon\in\Q}\bigcap\limits_{0<\delta\in\Q}\underbrace{\big\lbrace x\in\S:
		\exists y,z\in\S\mit d(x,y)<\delta\wedge d(x,y)<\delta\wedge d'\big(h(y),h(z)\big)\geq\varepsilon\big\rbrace}_{=:A_{\varepsilon,\delta}}
	\end{align}
	Zeige 
	\begin{align}\label{eqProof4.9Stern}
		A_{\varepsilon,\delta}\in\G(\S)\qquad\forall\varepsilon,\delta>0
	\end{align}
	Dazu sei $x_0\in A_{\varepsilon,\delta}$. 
	Dann existieren $y,z\in\S$ mit $d(x_0,y)<\delta$ und $d(z_0,z)<\delta$, aber $d'\big(h(y),h(z)\big)\geq\varepsilon$. 
	Wähle
	\begin{align*}
		\delta_0:=\min\big\lbrace\delta- d(x_0,y),\delta-d(x_0,z)\big\rbrace>0
	\end{align*}
	Ferner gilt:
	\begin{align}\label{eqProof4.9i}
		B(x_0,\delta_0)\subseteq A_{\varepsilon,\delta},
	\end{align}
	denn: Sei $x\in B(x_0,\delta_0)$. 
	Dann gilt
	\begin{align*}
		d(x,y)\leq d(x,x_0)+d(x_0,y)<\delta_0+d(x_0,y)<\delta
	\end{align*}
	und analog
	\begin{align*}
		d(x,z)<\delta.
	\end{align*}
	Also folgt $x\in A_{\varepsilon,\delta}$, denn $d'\big((h(y),h(z)\big)\geq\varepsilon$. 
	Wegen \eqref{eqProof4.9i} ist $x_0$ innerer Punkt von $A_{\varepsilon,\delta}$. 
	Also folgt \eqref{eqProof4.9Stern} und mit \eqref{eqProof4.9Plus} dann die Behauptung.
\end{proof}

\begin{satz}[Continuous Mapping Theorem (CMT)]\enter\label{satz4.10ContinuousMappingTheorem}
	Sei $h:(\S,d)\to(\S',d')$ $\B(\S)$-$\B(\S)$-messbar.
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			P_n\stackrelnew{w}{}{\longrightarrow} P\wedge P(D_h)=0
			\implies P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
		\end{aligned}$
		\item $\begin{aligned}
			X_n\stackrel{\L}{\longrightarrow}\text{ in }(\S,d)\wedge\P(X\in D_h)=0
			\implies h(X_n)\stackrel{\L}{\longrightarrow} h(X)\text{ in }(\S',d') 
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zeige (1):}\\
	Sei $F\in\F(\S')$ (d. h. $F\subseteq\S'$ abgeschlossen) beliebig. 
	Dann gilt:
	\begin{align}\label{eqProof1.4.10(i)}
		\limsup\limits_{n\to\infty} P_n\circ h^{-1}(F)
		&=\limsup\limits_{n\to\infty} P_n\big(\underbrace{h^{-1}(F)}_{\subseteq \overline{h^{-1}(F)}}\big)\\\nonumber
		&\leq \limsup\limits_{n\to\infty} P_n\big(\underbrace{\overline{h^{-1}(F)}}_{\in\F(\S)}\big)\\\nonumber
		\overset{\ref{satz4.2}}&{\leq}
		P\big(\overline{h^{-1}(F)}\big)
	\end{align}
	Es gilt
	\begin{align}\label{eqProof1.4.10(ii)}
		\overline{h^{-1}(F)}\subseteq h^{-1}(F)\cup D_h,
	\end{align}
	denn: Sei $x\in\overline{h^{-1}(F)}$.\nl
	\underline{Fall 1: $x\in D_h$}
	\begin{align*}
		x\in D_h\implies x\in h^{-1}(F)\cup D_h
	\end{align*}
	\underline{Fall 2: $x\not\in D_h$}\\
	Also ist $x\in C_h$, d. h. $h$ ist stetig in $x$. 
	Ferner existiert eine Folge $(x_n)_{n\in\N}\subseteq h^{-1}(F)$ mit $x_n\stackrel{n\to\infty}{\longrightarrow} x$. 
	Wegen der Stetigkeit gilt
	\begin{align*}
		\underbrace{h(x_n)}_{\in F~\forall n}\stackrel{n\to\infty}{\longrightarrow} h(x)
		\implies h(x)\in\overline{F}=F\implies x\in h^{-1}(F)
		\implies x\in h^{-1}(F)\cup D_h
	\end{align*}
	Mit \eqref{eqProof1.4.10(ii)} folgt:
	\begin{align*}
		\P\Big(\overline{h^{-1}(F)}\Big)
		\stackrel{\eqref{eqProof1.4.10(ii)}}{\leq}
		P\Big(h^{-1}(F)\cup D_h\Big)
		\leq P\Big(h^{-1}(F)\Big)+\underbrace{P(D_h)}_{=0}
		=P\circ h^{-1}(F)\\
		\stackrel{\eqref{eqProof1.4.10(i)},\ref{satz4.2}(3)}{\implies} 
		P_n\circ h^{-1}\stackrelnew{w}{}{\longrightarrow} P\circ h^{-1}
	\end{align*}
	\underline{Zeige (2):} folgt aus (1).
\end{proof}

\subsection*{Die Cramér'schen Sätze}
Zusammenstellung einiger Eigenschaften der Verteilungskonvergenz.

\begin{satz}[Teilfolgenprinzip für schwache Konvergenz]\label{satz4.11}\
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			Q_n\stackrelnew{w}{}{\longrightarrow} Q
			\Longleftrightarrow
			\text{Jede TF }(Q_{n'})\subseteq(Q_n)_{n\in\N}\text{ enthält TF }(Q_{n''})\subseteq(Q_{n'}):Q_{n''}\stackrelnew{w}{}{\longrightarrow} Q
		\end{aligned}$
		\item $\begin{aligned}
			X_n\stackrel{\L}{\longrightarrow} X\Longleftrightarrow\text{Jede TF }(X_{n'})\subseteq(X_n)_{n\in\N}\text{ enthält TF }(X_{n''})\subseteq(X_{n'}):X_{n''}\stackrel{\L}{\longrightarrow} X
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	Wir zeigen hier nur (1):\nl
	\underline{Zeige ``$\implies$'':}\\
	Folgt aus der Definition \ref{def4.1} und dem Teilfolgenprinzip für $(\int f\d Q)_{n\in\N}$ für alle $f\in C^b(\S)$.\nl
	\underline{Zeige ``$\Longleftarrow$'':}\\
	Angenommen $Q_n\stackrelnew{w}{}{\not\longrightarrow} Q$. 
	Dann gilt:
	\begin{align*}
		\exists f\in C^b(\S):\int\limits f\d Q_n\not\longrightarrow\int\limits f\d Q
	\end{align*}
	Also existiert ein $\varepsilon>0$ und eine TF $(X_{n'})\subseteq(X_n)_{n\in\N}$ so, dass
	\begin{align}\label{eqProof4.11Stern}
		\left|\int\limits f\d Q_{n'}-\int\limits f\d Q\right|\geq\varepsilon\qquad\forall n'
	\end{align}
	im Widerspruch zur Voraussetzung, da \eqref{eqProof4.11Stern} insbesondere für alle $n''$ gilt.
\end{proof}

\begin{satz}\label{satz4.12}
	\begin{align*}
		X_n\stackrel{\P}{\longrightarrow} X\implies X_n\stackrel{\L}{\longrightarrow} X
	\end{align*}
\end{satz}

\begin{proof}
	Sei $(X_{n'})$ eine Teilfolge von $(X_n)_{n\in\N}$. 
	Dann existiert gemäß \ref{satz3.13} eine TF $(X_{n''})$ von $(X_{n'})$ mit $X_{n''}\stackrel{n''\to\infty}{\longrightarrow} X$ $\P$-fast sicher.
	Damit folgt aus Satz \ref{Satz3.8}:
	\begin{align*}
		&f\big(X_{n''}\big)\stackrel{n''\to\infty}{\longrightarrow}f(X)~\P\text{-fast sicher}&\forall f\in C^b(\S)\\
		\overset{\text{dom Konv}}&{\implies}
		\E\Big[f\big(X_{n''}\big)\Big]\stackrel{n''\to\infty}{\longrightarrow}\E\Big[f\big(X\big)\Big]&\forall f\in C^b(\S)\\
		\overset{\ref{satz4.7}}&{\implies}
		X_{n''}\stackrel{\L}{\longrightarrow} X\\
		\overset{\ref{satz4.11}(2)}&{\implies}
		X_n\stackrel{\L}{\longrightarrow} X
	\end{align*}
\end{proof}
Einfache Beispiele zeigen, dass in \ref{satz4.12} die Umkehrung im Allgemeinen \underline{nicht} gilt. Aber:

\begin{satz}\label{satz4.13}
	Sei $X_n\stackrel{\L}{\longrightarrow}X$ mit $X$ fast sicher konstant.\\
	Dann gilt $X_n\stackrel{\P}{\longrightarrow} X$.
\end{satz}

\begin{proof}
	Nach Voraussetzung existiert eine Konstante $x\in\S$ mit $\P(X=c)=1$. Wir verwenden das folgende Lemma:

	\begin{lem}[Lemma von Uryson]\enter\label{lemmaVonUryson}
		Zu $A,B\in\F(\S)$ mit $A\cap B=\emptyset$ existiert stetige 
		\begin{align*}
			f:\S\to[0,1]\qquad\mit\qquad f(x)=\left\lbrace\begin{array}{cl}
				0, &\falls x\in A\\
				1, &\falls x\in B
			\end{array}\right.
		\end{align*}
	\end{lem}
	
	Sei $\varepsilon>0$. 
	Dann sind $A:=\lbrace c\rbrace, B=\big(B(c,\varepsilon)\big)^C\in\F(\S)\mit A\cap B=\emptyset$. 
	Nun wenden wir das Lemma von Uryson an und erhalten die Existenz einer Abbildung $f:\S\to[0,1]$ stetig (also $f\in C^b(\S)$) mit der Eigenschaft
	\begin{align*}
		f(x)&=\left\lbrace\begin{array}{cl}
			0, &\falls x=c\\
			1, &\falls d(x,c)\geq\varepsilon
		\end{array}\right.\\
		\implies 0 \leq \P\big(d(X_n,X)>\varepsilon\big)
		\overset{\text{Vor}}&=
		\P\big(d(X_n,c)>\varepsilon\big)\\
		&=\E\Big[\underbrace{\indi_{\lbrace d(X_n,c)\geq\varepsilon\rbrace}}_{\leq f(X_n)}\Big]\\
		&\leq\E\big[f(X_n)\big]\stackrelnew{n\to\infty}{\text{Vor}}{\longrightarrow}\E\big[\underbrace{f(X)}_{=\underbrace{f(c)}_{=0}\text{ f.s.}}\big]\\
		&=0
	\end{align*}
	Das Einschließ-Kriterium liefert
	\begin{align*}
		\P\big(d(X_n,X)\geq\varepsilon\big)\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
	\end{align*}
\end{proof}

\begin{satz}[Cramér]\enter\label{satz4.14Cramer}
	Seien $(X_n)_{n\in\N},(Y_n)_{n\in\N}$ zwei Folgen in separablen metrischen Raum $(\S,d)$, die \textbf{stochastisch äquivalent sind}, d. h.
	\begin{align}\label{eq4.14StochastischAquivalent}\tag{Ä}
		d(X_n,Y_n)\stackrel{\P}{\longrightarrow}0
	\end{align}
	Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\Longleftrightarrow Y_n\stackrel{\L}{\longrightarrow} X
	\end{align*}
\end{satz}

\begin{proof}
	Sei $f\in C^b(\S)$ gleichmäßig stetig, d. h.
	\begin{align*}
   		\forall\varepsilon>0\colon
        \exists\delta_\varepsilon>0\colon
        \forall x,y\in\S:\quad
        d(x,y) \leq \delta_\varepsilon \implies \left|f(x)-f(y)\right| \leq \varepsilon
	\end{align*}
	Damit folgt
	\begin{align*}
		&\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
		\overset{\text{Lin}}&=
		\left|\int\limits f(X_n)-f(Y_n)\d\P\right|\\
		\overset{\text{DU}}&{\leq}
		\int\limits\big|f(X_n)-f(Y_n)\big|\d\P\\
		\overset{\text{Lin}}&=
		\int\limits\underbrace{\indi_{\big\lbrace d(X_n,Y_n)\leq\delta\big\rbrace}\cdot\big|f(X_n)-f(Y_n)\big|}_{\leq\varepsilon\text{ wegen glm. Stetigkeit}}\d\P\\
		&\qquad+
		\int\limits\indi_{\big\lbrace d(X_n,Y_n)>\delta\big\rbrace}\cdot\underbrace{\big|f(X_n)-f(Y_n)\big|}_{\leq\big|f(X_n)\big|+\big|f(Y_n)\big|\leq2\cdot\Vert f\Vert_\infty}\d\P\\
		&\leq\varepsilon+2\cdot\Vert f\Vert_\infty\cdot\underbrace{\P\big(d(X_n,Y_n)>\delta_\varepsilon\big)}_{\stackrel{n\to\infty}{\longrightarrow}0}\\
		\implies
		0&\leq\liminf\limits_{n\to\infty}\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
		&\leq\limsup\limits_{n\to\infty}\Big|\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\Big|\\
		&\leq\varepsilon+2\cdot\Vert f\Vert_\infty\cdot 0\\
		&=\varepsilon\qquad\forall\varepsilon>0\\
		&\stackrel{\varepsilon\to0}{\implies}
		\E\big[f(X_n)\big]-\E\big[f(Y_n)\big]\stackrel{n\to\infty}{\longrightarrow} 0\qquad\forall f\in C^b(\S)\text{ glm. stetig}
	\end{align*}
	Es folgt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X
		&\stackrel{\ref{satz4.7}}{\Longleftrightarrow}
		\E\big[f(X_n)\big]\longrightarrow\E\big[f(X)\big]\qquad\forall f\in C^b(\S)\text{ glm. stetig}\\
		&\Longleftrightarrow
		\underbrace{\E\big[f(X_n)\big]}_{=\E\big[f(X_n)\big]+\Big(\E\big[f(Y_n)\big]-\E\big[ f(X_n)\big]\Big)\stackrel{\text{oben}}{\longrightarrow}0}\stackrel{}{\longrightarrow}\E\big[f(X)\big]~\forall f\in C^b(\S)\text{ glm. stetig}\\
		&\stackrel{\ref{satz4.7}}{\Longleftrightarrow}
		Y_n\stackrel{\L}{\longrightarrow} X
	\end{align*}
\end{proof}

\pagebreak[4]
\begin{satz}[Cramér-Slutsky]\label{satz4.15CramerSlutsky}\enter
	Seien $(\S,d)$, $(\S',d')$ separable metrische Räume. 
	Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\wedge Y_n\stackrel{\L}{\longrightarrow} Y\wedge Y\text{ f.s. konstant}
		\implies (X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }(\S\times\S',d\times d')
	\end{align*}
\end{satz}

\begin{proof}
	Nach Voraussetzung existiert ein $c\in\S'$ mit $\P(Y=c)=1$. 
	Mit Satz \ref{satz4.13} gilt:
	\begin{align}\label{eqProof4.15Stern}
		Y_n\stackrel{\L}{\longrightarrow} c
	\end{align}
	Ferner:
	\begin{align*}
		&d\times d'\Big((X_n,Y_n),(X_n,c)\Big)
		=\underbrace{d(X_n,X_n)}_{=0}+d'(Y_n,c)
		=d'(Y_n,c)\\
		&\implies\P\Big(d\times d'\big((X_n,Y_n),(X_n,c)\big)>\varepsilon\Big)
		=\P\big(d'(Y_n,c)>\varepsilon\big)
		\stackrel{n\to\infty}{\longrightarrow} 0\quad\forall\varepsilon>0\text{ wg. } \eqref{eqProof4.15Stern}\\
		&\implies
		\big((X_n,Y_n)\big)_{n\in\N}\text{ und }\big((X_n,c)\big)_{n\in\N}\text{ sind stochastisch äquivalent}
	\end{align*}
	Gemäß Satz \ref{satz4.14Cramer} reicht es zu zeigen, dass
	\begin{align}\label{eqProof4.15Plus}
		(X_n,c)\stackrel{\L}{\longrightarrow}(X,c)\stackrelnew{\L}{\text{f.s.}}{=}(X,Y)\text{ in }(\S\times\S',d\times d')
	\end{align}
	gilt. 
	Dazu sei $f\in C^b(\S\times\S')$ beliebig und $f_c:\S\to\R,~f_c(x):=f(x,c)$. 
	Damit folgt $f_c\in C^b(\S)$. 
	Somit:
	\begin{align*}
		\E\big[f(X_n,c)\big]
		&\stackeq{\text{Def}}
		\E\big[f_c(X_n)\big]
		\stackrel{n\to\infty}{\longrightarrow}\E\big[f_c(X)\big]\stackeq{\text{Def}}
		\E\big[f(X,c)\big]\\
		\stackrel{\text{Def}}{\implies}
		\eqref{eqProof4.15Plus}
	\end{align*}
\end{proof}

\begin{bemerkungnr}\label{bemerkung4.16} %4.16
	Auf die Forderung, dass $Y$ f.s. konstant ist, kann \underline{nicht} verzichtet werden. 
	Also ist die Schlussfolgerung
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S\wedge Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S'
		\implies(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }\S\times\S'
	\end{align*}
	ist im Allgemeinen \underline{nicht} richtig!
\end{bemerkungnr}

\begin{korollar}\label{korollar4.17}
	Seien $\S,\S'$ separable metrische Räume und sei $T$ beliebiger metrischer Raum. 
	Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\wedge Y_n\stackrel{\P}{\longrightarrow} c\mit c\in\S'\text{ Konstante }
		\implies h(X_n,Y_n)\stackrel{\L}{\longrightarrow} h(X,c)\text{ in }T
	\end{align*}
	wobei $h:\S\times\S'\to T$ messbar mit $\P\big((X,c)\in D_h\big)=0$
\end{korollar}

\begin{proof}
	Folgt aus \ref{satz4.15CramerSlutsky} und Satz \ref{satz4.10ContinuousMappingTheorem}.
\end{proof}

\begin{beispiel}\label{beisp4.18}
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^k\wedge Y_n\stackrel{\P}{\longrightarrow} c\text{ in }\R^k
	\end{align*}
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n+Y_n\stackrel{\L}{\longrightarrow} X+c\text{ in }\R^k
		\end{aligned}$
		\item $\begin{aligned}
			\langle X_n,Y_n\rangle\stackrel{\L}{\longrightarrow}\langle c,X\rangle\text{ in }\R
		\end{aligned}$
		\item Für $k=1$ speziell:
		$\begin{aligned}
			Y_n\cdot X_n\stackrel{\L}{\longrightarrow}c\cdot X\text{ in }\R
		\end{aligned}$ und für $c=0$:
		$\begin{aligned}
			Y_n\cdot X_n\stackrel{\L}{\longrightarrow}0\text{ in }\R
		\end{aligned}$
		\item $\begin{aligned}
			\frac{X_n}{Y_n}\stackrel{\L}{\longrightarrow} \frac{X}{c},&\falls c\neq0\wedge\forall n\in\N:Y_n\neq0
		\end{aligned}$
		\item Konstante (= nicht zufällige) $Y_n$, d. h.
		\begin{align*}
			Y_n(\omega)=c_n\qquad\forall\omega\in\Omega,\forall n\in\N
		\end{align*}
		sind natürlich zugelassen:
		\begin{align*}
			(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,c)\text{ in }\R^k\times\R^k
		\end{align*}
	\end{enumerate}
\end{beispiel}

Es gilt:
\begin{align}\label{eqUnder4.18}
	(X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text { in }\S\times S'
	\implies X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S\wedge
	Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S'
\end{align}
denn aus dem CMT folgt
\begin{align*}
	X_n=\pi_1(X_n,Y_n)\stackrelnew{n\to\infty}{\L}{\longrightarrow}\pi_1(X,Y)=X,
\end{align*}
da die \textbf{Projektion}
\begin{align*}
	\pi_1:\S\times\S'\to\S,~\pi_1(x,y):=x\qquad\forall (x,y)\in\S\times\S'
\end{align*}
stetig ist. Analog: $Y_n\stackrel{\L}{\longrightarrow} Y$.\\
Die Umkehrung in \eqref{eqUnder4.18} gilt im Allgemeinen \underline{nicht}! 
Vergleiche \ref{bemerkung4.16}. 
Sie gilt tatsächlich, falls $Y$ fast sicher konstant ist, vergleiche Satz \ref{satz4.15CramerSlutsky}.\\
Ziel: Finde eine andere zusätzliche Forderungen neben der linken Seite von \eqref{eqUnder4.18}, die die umgekehrte Implikation gestattet. 
Dazu:\nl
Seien $(\S_1,d_1)$ und $(\S_2,d_2)$ zwei separable metrische Räume und $\S:=S_1\times\S_2$ mit $d:=d_1\times d_2$ der zugehörige Produktraum. 
(Es folgt, dass $(\S,d)$ separabel ist.) 
Eine geringfügige Modifikation des Beweises von \ref{satz3.3} zeigt:
\begin{align*}
	\B_d(\S_1\times\S_2)=\B_{d_1}(\S_1)\otimes\B_{d_2}(\S_2)
\end{align*}
Für ein Wahrscheinlichkeitsmaß $\P$ auf $\B(\S_1\times\S_2)$ definiere 
\begin{align*}
	\P_1(A_1)&:P(A_1\times\S_2) &\forall A_1\in\B(\S_1)\\
	\P_2(A_2)&:=P(\S_1\times A_2) &\forall A_2\in\B(\S_2)
\end{align*}
$\P_1$ und $\P_2$ heißen \textbf{Randverteilungen von $\P$}.

\begin{theorem}\label{theorem4.19}
	Sei $(\S,d)$ separabel. 
	Dann sind folgende Aussagen äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\P_n\stackrelnew{\omega}{}{\longrightarrow} \P 
		\end{aligned}$
		\item $\begin{aligned}
			\P_n(A_1\times A_2)\stackrel{n\to\infty}{\longrightarrow} \P(A_1\times A_2)\qquad\forall A_i\in\B(\S_i)~\P_i\text{-randlos mit } i=1,2
		\end{aligned}$
	\end{enumerate}
\end{theorem}

\begin{proof}
	Seien $\partial,\partial_1,\partial_2$ die Randoperatoren in $(\S,d),(\S_1,d_1),(\S_2,d_2)$, respektive.\nl
	\underline{Zeige (1) $\implies$ (2):}\\
	Sei $A:=A_1\times A_2$ mit $A_i\in(2)$. 
	Es gilt
	\begin{align}\label{eqProof4.19Stern}\tag{$\ast$}
		\partial A\subseteq((\partial_1 A_1)\times\S_2)\cup(\S_1\times(\partial_2 A_2))
	\end{align}
	%TODO Hier Skizze einfügen
	Daraus folgt
	\begin{align*}
		&\P(\partial A)\leq \underbrace{\P((\partial A_1)\times\S_2)}{=\P_1(\partial A_1)=0}+\underbrace{\P(\S_1\times(\partial_2 A_2))}_ {=\P_2(\partial_2 A_2)=0}=0\\
		&\implies
		A\in\B(\S_1\times\S_2)\text{ ist $\P$-randlos}\\
		&\stackrel{\ref{satz4.2}(5)}{\implies}(2)
	\end{align*}

	\underline{Zeige (2) $\implies$ (1):}\\
	Wir wollen Korollar \ref{korollar4.4} anwenden. Dazu:\\
	Wähle $d$ wie folgt aus:
	\begin{align}\label{eqProof4.19Plus}\tag{+}
		d\big((x_1,x_2),(y_1,y_2)\big)&:=\max\big\lbrace d_1(x_1,y_1),d_2(x_2,y_2)\big\rbrace
	\end{align}
	Sei
	\begin{align*}
		\U:=\big\lbrace A_1\times A_2:A_i\in\B(\S_i)~\P_i\text{-randlos}, i\in\lbrace1,2\rbrace\big\rbrace 
	\end{align*}
	$\U$ ist durchschnittsstabil, denn: 
	Seien $A:=A_1\times A_2,B=B_1\times B_2\in\U$. 
	Dann gilt
	\begin{align}\label{eqProof4.19.1}\tag{1}
		A\cap B=(A_1\cap B_1)\times(A_2\times B_2)
	\end{align}
	Ferner gilt
	\begin{align}\label{eqProof4.19.2}\tag{2}
		\partial_i(A_i\cap B_i)\subseteq(\partial_i A_i)\cup(\partial_i B_i)\qquad\forall i\in\lbrace1,2\rbrace
	\end{align}
	Aus \eqref{eqProof4.19.1} und \eqref{eqProof4.19.2} folgt: 
	$\U$ ist durchschnittsstabil. Sei nun $x=(x_1,x_2)\in\S=\S_1\times\S_2$ und $\varepsilon>0$ beliebig. 
	Aus \eqref{eqProof4.19Plus} folgt:
	\begin{align*}
		B_d(x,\varepsilon)=B_{d_1}(x_1,\varepsilon)\times B_{d_2}(x_2,\varepsilon)
	\end{align*}
	Sei 
	\begin{align*}
		A_\delta:=B_{d_1}(x_1,\delta)\times B_{d_2}(x_2,\delta)
		\stackeq{\eqref{eqProof4.19Plus}} B_d(x,\delta)\qquad\forall\delta>0
	\end{align*}
	Somit gilt (vgl. Beweis von \ref{satz4.2})
	\begin{align*}
		\partial_i B_{d_i}(x_i,\delta)\subseteq\big\lbrace y\in\S_i:d_i(y,x_i)=\delta\big\rbrace\qquad\forall i\in\lbrace1,2\rbrace
	\end{align*}
	Folglich sind die Mengen $\partial_i B_{d_i}(x_i,\delta),\delta>0$ sind paarweise disjunkt. 
	Die Mengen
	\begin{align*}
		E_i:=\Big\lbrace\delta>0:\P_i\big(\partial_i B_{d_i}(x_i,\delta)\big)>0\Big\rbrace\qquad\forall i\in\lbrace1,2\rbrace
	\end{align*}
	sind höchstens abzählbar (vgl. Beweis von \ref{satz4.2}). 
	Folglich ist die Vereinigung $E_1\cup E_2$ höchstens abzählbar und somit liegt $(E_1\cup E_2)^C$ dicht in $[0,\infty)$. 
	Mit
	\begin{align*}
		(E_1\cup E_2)^C&=E_1^C\cap E_2^C=\big\lbrace\delta>0:B_{d_i}(x_i,\delta)\text{ ist $\P_i$-randlos},i\in\lbrace1,2\rbrace\big\rbrace\\
		\implies\exists\delta&\in(0,\varepsilon):B_{d_1}(x_1,\delta)~\P_1\text{-randlos und }B_{d_2}(x_2,\delta)~\P_2\text{-randlos}\\
		\implies A_\delta&=B_{d_1}(x_1,\delta)\times B_{d_2}(x_2,\delta)\in\U\\
		\overset{\eqref{eqProof4.19Plus}}&=
		B_d(x,\delta)\subseteq B_d(x,\varepsilon)
	\end{align*}
	Also offene Kugel ist $A_\delta$ offen, also $\stackrel{\circ}{A_\delta}=A_\delta$. 
	Schließlich folgt
	\begin{align*}
		x\in\stackrel{\circ}{A_\delta}=A_\delta\subseteq B_d(x,\varepsilon)
	\end{align*}
	Also erfüllt $\U$ die Voraussetzungen von Korollar \ref{korollar4.4}. 
	Daraus folgt nun die Behauptung.
\end{proof}

Theorem \ref{theorem4.19} liefert ein nützliches Resultat für Produktmaße. 
Seien $\P^{(i)},\P_n^{(i)},n\in\N$ Wahrscheinlichkeitsmaße auf $\B(\S_i)$ mit $i\in\lbrace1,2\rbrace$. 
Dann sind
\begin{align*}
	\P:=\P^{(1)}\otimes\P^{(2)},\qquad\P_n:=\P_n^{(1)}\otimes\P_n^{(2)}\qquad\forall n\in\N
\end{align*}
Produktmaße auf $\B(\S_1)\otimes\B(\S_2)=\B(\S_1\times\S_2)$ bei Separabilität.

\begin{theorem}\label{thoerem4.20}
	Sei $\S=\S_1\times\S_2$ separabler Produktraum. Dann sind äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\P_n\stackrelnew{w}{}{\longrightarrow}\P
		\end{aligned}$
		\item $\begin{aligned}
			\P_n^{(1)}\stackrelnew{w}{}{\longrightarrow}\P^{(1)}\wedge
			\P_n^{(2)}\stackrelnew{w}{}{\longrightarrow}\P^{(2)}
		\end{aligned}$
	\end{enumerate}
\end{theorem}

\begin{proof}
	Gemäß Definition gilt:
	\begin{align*}
		\P(A_1\times A_2)&\stackeq{\text{Def}}\P^{(1)}(A_1)\cdot\P^{(2)}(A_2)\\
		\P_n(A_1\times A_2)&\stackeq{\text{Def}}\P_n^{(1)}(A_1)\cdot\P_n^{(2)}(A_2)\\
	\end{align*}
	\underline{Zeige (1) $\implies$ (2):}
	\begin{align*}
		\P_n^{(1)}(A_1)&=\P_n(A_1\times\S_2)\stackrel{}{\longrightarrow}\P(A_1\times\S_2)=\P^{(1)}(A_1)
		\qquad\forall A_1\in\B(\S_1)~\P^{(1)}\text{-randlos}
	\end{align*}
	gemäßig \ref{theorem4.19}, denn $\partial\S_2=\emptyset$. 
	Also ist $\S_2\in\B(\S_2)~\P^{(2)}$-randlos. 
	Also folgt aus \ref{satz4.2}(5):
	\begin{align*}
		\P_n^{(1)}\stackrelnew{w}{}{\longrightarrow}\P_n^{(2)}
	\end{align*}
	Analog zeigt man: 
	$\P_n^{(2)}\stackrelnew{w}{}{\longrightarrow}\P^{(2)}$.\nl
	\underline{Zeige (2) $\implies$ (1):}
	\begin{align*}
		&\P_n(A_1\times A_2)=\underbrace{\P_n^{(1)}(A_1)}_{\stackrel{n\to\infty}{\longrightarrow}\P^{(1)}(A_1)}\cdot\underbrace{\P_n^{(2)}(A_2)}_{\stackrel{n\to\infty}{\longrightarrow}\P^{(2)}(A_2)}\qquad\forall A_i~\P_i\text{-randlos},i=1,2\\
		&\implies
		\P^{(1)}(A_1)\cdot\P^{(2)}(A_2)=\P(A_1\times A_2)\qquad\forall A_i~\P_i\text{-randlos},i\in\lbrace1,2\rbrace\\
		&\stackrel{\ref{theorem4.19}}{\implies}\P_n\stackrel{n\to\infty}{\longrightarrow}\P
	\end{align*}
\end{proof}

Mit Theorem \ref{thoerem4.20} erhalten wir neben Satz \ref{satz4.15CramerSlutsky} ein weiteres Resultat über die\\ ``\ul{koordinatenweise Verteilungskonvergenz}'' (die ja im Allgemeinen nicht gilt).

\begin{satz}\label{satz4.21}
	Sei $\S=\S_1\times\S_2$ separabel, $X,X_n,n\in\N$ Zufallsvariablen in $\S_1$, $Y,Y_n,n\in\N$ Zufallsvariablen in $\S_2$ über $(\Omega,\A,\P)$ und gelte
	\begin{enumerate}
		\item $X_n$ und $Y_n$ sind unabhängig für alle $n\in\N$
		\item $X$ und $Y$ sind unabhängig
	\end{enumerate}
	Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }\S_1\wedge Y_n\stackrel{\L}{\longrightarrow} Y\text{ in }\S_2
		\Longleftrightarrow (X_n,Y_n)\stackrel{\L}{\longrightarrow}(X,Y)\text{ in }\S_1\times\S_2
	\end{align*}
\end{satz}

\begin{proof}
	Da
	\begin{align*}
		\P\circ (X_n,Y_n)^{-1}&\stackeq{\text{unab}}\P\circ X_n^{-1}\otimes\P\circ Y_n^{-1}\\
		\P\circ (X,Y)^{-1}&\stackeq{\text{unab}}\P\circ X^{-1}\otimes\P\circ Y^{-1}
	\end{align*}
	folgt die Behauptung aus \ref{thoerem4.20} und Definition \ref{def4.1}.
\end{proof}

\begin{bemerkungnr}\label{bemerkung4.22}
	Die Aussage in \ref{satz4.21} lässt sich problemlos auf $\S_1\times\ldots\times\S_d$ mit $d\geq2$ übertragen.
\end{bemerkungnr}

\section*{Anwendung in der Statistik}
Seien $X_i,i\in\N$ i.i.d. über $(\Omega,\A,\P)$, quadrat-integrierbar, $\mu:=\E[X_i]\in\R$,\\ $\sigma^2:=\Var(X_i)\in(0,\infty)$.\\
Das arithmetische Mittel konvergiert fast sicher gegen den Erwartungswert gemäß dem starken Gesetz der großen Zahlen (SGGZ / SLLN, Kolmogorov), also
\begin{align*}
	\overline{X}_n=\frac{1}{n}\cdot\sum\limits_{i=1}^n X_i\stackrel{}{\longrightarrow}\mu~\P\text{-fast sicher}
\end{align*}
%fast sichere Konvergenz => stochastische Konvergenz => Verteilungskonvergenz?
Folglich gilt
\begin{equation}
	\label{eqAnwendungInDerStatistik}
	\begin{aligned}
		\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)
		&=\sqrt{n}\cdot\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)\\
		&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n(X_i-\mu)\\
		&=\sigma\cdot\underbrace{\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n\left(\frac{X_i-\mu)}{\sigma}\right)}_{\stackrel{\L}{\longrightarrow}\mathcal{N}(0,1)}\stackrel{\L}{\longrightarrow}\sigma\cdot\mathcal{N}(0,1)\stackeq{\L}\mathcal{N}(0,\sigma^2)
	\end{aligned}
\end{equation}
wobei die linke Konvergenz aus dem zentralen Grenzwertsatz (TGWS / CLT) und die rechte Konvergenz auf dem CMT folgt (da $x\mapsto\sigma\cdot x$ stetig). Also folgt:
\begin{align}\label{eqAnwendungInDerStatistikStern}\tag{$\ast$}
	\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)
\end{align}
Die \textbf{empirische Varianz} ist 
\begin{align*}
	S_n^2&:=\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\overline{X}_n)^2\\
	&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big((X_i-\mu)-(\overline{X}_n-\mu)\big)^2\\
	&=\frac{1}{n}\cdot\sum\limits_{i=1}^n\Big((X_i-\mu)^2-2\cdot(X_i-\mu)\cdot(\overline{X}_n-\mu)+(\overline{X}_n-\mu)^2\Big)\\
	&=\frac{1}{n}\cdot\sum\limits(X_i-\mu)^2-2\cdot\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)}_{=(\overline{X}_n-\mu)}\cdot(\overline{X}_n-\mu)+\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(\overline{X}_n-\mu)^2}_{=(\overline{X}-\mu)^2}\\
\end{align*}
Man erhält schließlich:
\begin{align}\label{eqEmpVarAlternativePlus}\tag{+}
	S_n^2&=\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n(X_i-\mu)^2}_{\stackrel{\text{SGGZ}}{\longrightarrow}\E\big[(X_1-\mu)^2\big]=\sigma^2~\P\text{-f.s.}}-\underbrace{(\overline{X}_n-\mu)^2}_{\stackrel{\text{SGGZ+\ref{Satz3.8}}}{\longrightarrow}0\text{ f.s.}}\\\nonumber
	&\implies
	S_n^2\stackrel{\ref{satz3.15}+\ref{Satz3.8}}{\longrightarrow}\sigma^2
\end{align}
(Hierbei wird bei der Anwendung der Sätze \ref{satz3.15} und \ref{Satz3.8} benutzt, dass $(x,y)\mapsto x+y$ stetig ist)

%Skorokhod (russischer Mathematiker), cadlag, rcll (right continues with left limit), in einem Skorokhod-Raum ist die Addtion NICHT stetig. Also ist obiger Schluss i.A. nicht richtig.
Und ähnlich: ($\sqrt{n}$ ist die \textbf{normalisierende Folge})
\begin{align*}
	T_n&:=\sqrt{n}\cdot\left(S_n^2-\sigma^2\right)
	\stackeq{\eqref{eqEmpVarAlternativePlus}}
	\underbrace{\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n\Big((X_i-\mu)^2-\sigma^2\Big)}_{=:V_n}\underbrace{-\sqrt{n}\cdot\big(\overline{X}-\mu\big)^2}_{=:R_n}
	=V_n+R_n
\end{align*}
Mit CLT folgt (analog zur Herleitung von \eqref{eqAnwendungInDerStatistik})
\begin{align*}
	V_n\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2),\qquad\tau^2:=\Var\Big((X_1-\mu)^2\Big)
\end{align*}
falls $\E\big[|X-1|\big]^4<\infty$.
\begin{align*}
	-R_n&=\underbrace{\Big(\sqrt{n}\cdot(\overline{X}_n-\mu)\Big)}_{\stackrelnew{\eqref{eqAnwendungInDerStatistikStern}}{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)}\cdot\underbrace{(\overline{X}_n-\mu)}_{\stackrelnew{\ref{Satz3.12}}{\P}{\longrightarrow}0}\stackrelnew{\ref{beisp4.18}(3)}{\P}{\longrightarrow}0\\
	&\implies \underbrace{R_n}_{=|T_n-V_n|}\stackrel{\P}{\longrightarrow}0\\
	&\implies(T_n)_{n\in\N},(V_n)_{n\in\N}\text{ sind stochastisch äquivalent}\\
	&\stackrel{\ref{satz4.14Cramer}}{\implies}
	T_n\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2)
\end{align*}
\textbf{Zusammenfassung:}
\begin{align*}
	\big(\overline{X}_n,S_n^2\big)\stackrel{n\to\infty}{\longrightarrow}\big(\mu,\sigma^2\big)\text{ in }\R^2\text{ fast sicher}
\end{align*}
d. h. $\big(\overline{X}_n,S_n^2\big)_{n\in\N}$ ist \textbf{stark konsistente Schätzerfolge} für den Parameter $(\mu,\sigma^2)$.
Fernen sind $(\overline{X}_n)_{n\in\N}$ und $(S_n^2)_{n\in\N}$ \textbf{asymptotisch normal}, d.h.
%FUN: Der persönliche Held von Ferger ist Skorokhot. Er hat ihn ca. 2000 mal auf einer Tagung und es gab sogar ein Foto von den beiden, dass aber durch Datenverlust verloren ging
\begin{align*}
	\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2),\qquad
	\sqrt{n}\cdot\big(S_n^2-\sigma^2\big)\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\tau^2)
\end{align*}
Wie sieht es aus mit dem Vektor 
\begin{align*}
	\begin{pmatrix}
		\sqrt{n}\cdot\big(\overline{X}_n-\mu\big)\\
		\sqrt{n}\cdot\big(S_n^2-\sigma^2\big)
	\end{pmatrix}
	=\sqrt{n}\cdot\begin{pmatrix}
		\overline{X}_n-\mu\\
		S_n^2-\sigma^2
	\end{pmatrix}
	\stackrel{\L}{\longrightarrow}?
\end{align*}

\section{Verteilungskonvergenz in \texorpdfstring{$\R^d$}{R\textasciicircum d}}
Korollar \ref{korollar4.5} bzw. dessen Erweiterungen \ref{lemma4.6Einhalb} (2) liefern eine \textbf{analytische} Methode zum Nachweis von $X_n\stackrel{\L}{\longrightarrow}X\text{ in }\R^d$. 
Eine weitere Methode fußt auf

\begin{definition}\label{def5.1}
	Sei $X$ Zufallsvariable in $\R^d$ über $(\Omega,\A,\P)$ und
	\begin{align*}
		\langle x,y\rangle:=:x'y:=\sum\limits_{i=1}^d x_i\cdot y_i\qquad x=(x_1,\ldots,x_d),y=(y_1,\ldots,y_d)\in\R^d
	\end{align*}
	das Standard-Skalarprodukt in $\R^d$. Dann heißt
	\begin{align*}
		\varphi_X(t):=\E\Big[\exp\big(i\cdot\langle t,X\rangle\big)\Big]\qquad\forall t\in\R^d
	\end{align*}
\end{definition}
heißt \textbf{charakteristische Funktion} von $X$.

\begin{satz}[Eindeutigkeitssatz]\label{satz5.2Eindeutigkeitssatz}
	\begin{align*}
		X\stackeq{\L} Y\Longleftrightarrow\varphi_X\equiv\varphi_Y
	\end{align*}
\end{satz}

\begin{proof}
	Siehe Buch \textit{Essentials in Probability} von Jacod und Protter (2000), Seite 107-108.
\end{proof}

\begin{satz}[Stetigkeitssatz]\label{satz5.3Stetigkeitssatz}
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^d\Longleftrightarrow\forall t\in\R^d: \varphi_{X_n}(t)\stackrel{n\to\infty}{\longrightarrow}\varphi_X(t)
	\end{align*}
\end{satz}

\begin{proof}
	Siehe Vorlesung Wahrscheinlichkeitstheorie (Bachelor) oder Jacod und Protter (2000), Seite 163 ff.
\end{proof}

Sehr nützlich ist:

\begin{satz}[Cramér-Wold-Device]\label{satz5.4CramerWoldDevice}\enter
	%Device bedeutet u. A. Trick. Das ist kein Name.
	Folgende Aussagen sind äquivalent:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n\stackrel{\L}{\longrightarrow} X\text{ in }\R^d
		\end{aligned}$
		\item $\begin{aligned}
			\langle t, X_n\rangle\stackrel{\L}{\longrightarrow}\langle t, X\rangle\text{ in }\R\qquad\forall t\in\R^d
		\end{aligned}$
	\end{enumerate}
\end{satz}

\begin{proof}
	\begin{align*}
		\varphi_X(t)
		&\stackeq{\text{Def}}\E\Big[\exp\big(i\cdot\langle t,X\rangle\big)\Big]\qquad\forall t\in\R^d
		\stackeq{d=1}\E\big[\exp(i\cdot X\cdot t)\big]
	\end{align*}
	\underline{Zeige (1) $\implies$ (2):}\\
	Sei $t\in\R^d$. Dann ist $x\mapsto\langle t,x\rangle$ stetig auf $\R^d$. 
	Und aus Satz \ref{satz4.10ContinuousMappingTheorem} (CMT) folgt dann (2).\nl
	\underline{Zeige (2) $\implies$ (1):}
	\begin{align*}
		&\varphi_{X_n}(t)
		\stackeq{\text{Def}}\E\Big[\exp\big(i\cdot\langle t,X_n\rangle\cdot 1\big)\Big]
		\stackeq{\text{Def}}\varphi_{\langle t,X_n\rangle}(1)
		\stackrelnew{\ref{satz5.3Stetigkeitssatz}}{n\to\infty}{\longrightarrow}\underbrace{\varphi_{\langle t,X\rangle}(1)}_{=\varphi_X(t)}\\
		&\implies\varphi_{X_n}\stackrel{n\to\infty}{\longrightarrow}\varphi_X\text{ auf }\R^d
		\stackrel{\ref{satz5.3Stetigkeitssatz}}{\implies}(1)
	\end{align*}
\end{proof}

\section{Der multivariate zentrale Grenzwertsatz (ZGWS) für Dreiecksschemata} %6
Wir betrachen zunächst den \underline{univariaten} Fall. 
Es gelte
\begin{align}\label{eq6.1}\tag{6.1}
	X_{n,1},X_{n,2},\ldots,X_{n,n}\text{ sind unabhängige \ul{reelle} ZV }\forall n\in\N
\end{align}
Die Kollektion
\begin{align*}
	\big\lbrace X_{n,k}:1\leq k\leq n,n\in\N\rbrace
\end{align*}
heißt \textbf{Dreiecksschema / $\Delta$-Schema}.
\begin{align*}
	\begin{matrix}
		X_{1,1}\\
		X_{2,1} & X_{2,2}\\
		X_{3,1} & X_{3,2} & X_{3,3}\\
		\vdots & \vdots & \vdots & \ddots\\
		X_{n,1} & X_{n,2} & \hdots & \hdots & X_{n,n}\\
		\vdots &&&&\vdots & \ddots
	\end{matrix}
\end{align*}

Sei 
\begin{align}\label{eq6.2}\tag{6.2}
	\E[X_{n,k}]=0,~\sigma_{n,k}^2:=\E\left[X_{n,k}^2\right]<\infty~\forall n,k\in\N\text{ und }
	s_n^2:=\sum\limits_{k=1}^n\sigma_{n,k}^2~\forall n\in\N
\end{align}

\begin{satz}[Lindeberg, 1922]\label{satz6.1Lindeberg1922}\enter
	Es gelten \eqref{eq6.1} und \eqref{eq6.2} sowie
	\begin{align}\label{eqSatz6.1LindebergLB}\tag{LB}
		\sum\limits_{k=1}^n\E\left[X_{n,k}^2\cdot\indi_{\big\lbrace|X_{n,k}|>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0.
	\end{align}
	Falls zusätzlich
	\begin{align*}
		s_n^2\stackrel{n\to\infty}{\longrightarrow}\sigma^2\in(0,\infty)
	\end{align*}
	gilt, so gilt:
	\begin{align*}
		\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,\sigma^2)
	\end{align*}
\end{satz}

\begin{bemerkung}
	\begin{align*}
		s_n^2=\Var\left(\sum\limits_{k=1}^n X_{n,k}\right)
	\end{align*}
\end{bemerkung}

\begin{proof}
	So ähnlich wie Beweis von klassischem zentralen Grenzwertsatz in der Vorlesung Wahrscheinlichkeitstheorie (Bachelor), nur technisch etwas komplizierter. 
	Vergleiche auch Billingsley (1995), \textit{Probability and Measure}, Seite 359 ff.
\end{proof}

Im Folgenden betrachten wir die Verallgemeinerung auf den \underline{multivariaten} Fall.\\
Sei $\lbrace X_{n,k}:k\leq n,n\in\N\big\rbrace$ ein $\Delta$-Schema von Zufallsvariablen.
\begin{align*}
	X_{n,k}=\left(X_{n,k}^{(1)},\ldots,X_{n,k}^{(d)}\right)\text{ in }\R^d
\end{align*}
Es gelte die \textbf{zeilenweise Unabhängigkeit}:
\begin{align}\label{eq6.3}\tag{6.3}
	X_{n,1},\ldots,X_{n,n}\text{ sind unabhängig}\qquad\forall n\in\N
\end{align}
%Ferger arbeitet auch am Buß- und Bettag in der Uni. Er ist evangelisch getauft worden und sogar konfirmiert. 
Also die Vektoren seien unabhängig. Daraus folgt nicht, dass deren Komponenten unabhängig sind.
\begin{align}\label{eq6.4}\tag{6.4}
	\E\big[X_{n,k}\big]:=\left(\E\left[X_{n,k}^{(j)}\right]\right)_{1\leq j\leq d}=0:=(0,\ldots,0)\qquad\forall k,n\in\N\\
	\label{eq6.5}\tag{6.5}
	\E\left[\left(X_{n,k}^{(j)}\right)\right]<\infty\qquad\forall 1\leq j\leq d,\forall n,k\in\N
\end{align}
Wegen \eqref{eq6.4} und \eqref{eq6.5} ist die so genannte \textbf{Kovarianzmatrix}
\begin{align*}
	\Cov\left(X_{n,k}\right):=\Big(\underbrace{\Cov\left(X_{n,k}^{(i)},X_{n,k}^{(j)}\right)}_{=\E\left(X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right)}\Big)_{1\leq i,j\leq d}\in\R^{d\times d}
\end{align*}

\begin{satz}[Multivariater Zentraler Grenzwertsatz (ZGWS)]\label{satz6.2MultivariaterZGWS}\enter
	Es gelten \eqref{eq6.3}, \eqref{eq6.4}, \eqref{eq6.5} sowie
	\begin{align}\label{eqSatz6.2LB}\tag{LB}
		\sum\limits_{k=1}^n\E\left[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\right]\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
	\end{align}
	(Hierbei ist $\Vert\cdot\Vert$ die euklidische Norm auf $\R^d$.)\\
	Falls die Normierungsbedingung
	\begin{align}\label{eqSatz6.2NB}\tag{NB}
		\sum\limits_{k=1}^n\Cov\left(X_{n,k}\right)\stackrelnew{\text{komponentenweise}}{n\to\infty}{\longrightarrow}\Gamma\qquad\mit\Gamma\in\R^{d\times d}\text{ positiv definit}
	\end{align}
	erfüllt ist, so gilt:
	\begin{align*}
		\sum\limits_{k=1}^n X_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)\text{ in }\R^d
	\end{align*}
\end{satz}
%Ferger kennt die Dichte die Normalverteilung im Mehrdimensionalen nicht und versucht sie trotzdem aus dem Gedächtnis an die Tafel zu schreiben. Wieder 5 Minuten weg :D
%"Mir persönlich reicht es, das Ding hat eine Dichte!" :D
%"Manchmal ist weniger mehr!"
\begin{proof}
	Sei $N\sim\mathcal{N}_d(0,\Gamma)$. Mit \ref{satz5.4CramerWoldDevice} bleibt zu zeigen:
	\begin{align}
		\label{eqProof6.2Stern}\tag{$\ast$}
		\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle\stackrel{\L}{\longrightarrow}\langle t,N\rangle\text{ in }\R\qquad\forall t\in\R^d
	\end{align}
	Sei $t\in\R^d\setminus\lbrace0\rbrace$ (für $t=0$ ist \eqref{eqProof6.2Stern} trivialerweise erfüllt). Es folgt
	\begin{align*}
		\left\langle t,\sum\limits_{k=1}^n X_{n,k}\right\rangle
		\overset{\text{Lin}}&=
		\sum\limits_{k=1}^n\underbrace{\left\langle t,X_{n,k}\right\rangle}_{=:Y_{n,k}=:Y_{n,k}(t)}
		=\sum\limits_{k=1}^n Y_{n,k}\\
		Y_{n,k}
		&=\sum\limits_{k=1}^d t_j\cdot X_{n,k}^{(j)}
	\end{align*}
	Folglich erfüllt $\big\lbrace Y_{n,k}:1\leq k\leq n,n\in\N\big\rbrace$ die Bedingung \eqref{eq6.1} wegen Blockungslemma und \eqref{eq6.2} wegen \eqref{eq6.4} und Linearität sowie Cauchy-Schwarz-Ungleichung.
	%Hauptsatz des Mannschaftssport: "Jede Kombination von Nullen ist immer Null."
	\begin{align*}
		\big|Y_{n,k}^2\big|&=\big|\langle t,X_{n,k}\rangle\big|^2
		\leq\Vert t\Vert^2\cdot\Vert X_{n,k}\Vert^2
		=\Vert t\Vert^2\cdot\sum\limits_{j=1}^d\left(X_{n,k}^{(j)}\right)^2
	\end{align*}
	
	Da $\langle t,N\rangle=t\cdot N\sim\mathcal{N}(0,t\cdot\Gamma)$ (vgl. Klaus Schmidt \textit{Wahrscheinlichkeit und Maß}, Bsp. 13.2.2), folgt \eqref{eqProof6.2Stern}. 
	Der Nachweis von 
	\begin{align}\label{eqProof6.2SternStern}\tag{$\ast\ast$}
		\sum\limits_{k=1}^n Y_{n,k}\stackrel{\L}{\longrightarrow}\mathcal{N}(0,t'\cdot\Gamma\cdot t)
	\end{align}
	erfolgt mit Satz \ref{satz6.1Lindeberg1922}. 
	Es gilt:
	\begin{align*}
		s_n^2
		&=\sum\limits_{k=1}^n\E\left[Y_{n,k}^2\right]\\
		&=\sum\limits_{k=1}^n\E\left[\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
		&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot t_j\cdot\E\left[X_{n,k}^{(i)}\cdot X_{n,k}^{(j)}\right]\\
		&=\sum\limits_{k=1}^n\sum\limits_{i,j=1}^d t_i\cdot\Big(\Cov(X_{n,k}\Big)_{i,j}\cdot t_j\\
		&=\sum\limits_{k=1}^n t'\cdot\Cov(X_{n,k})\cdot t\\
		\overset{\text{Distri}}&=
		t'\cdot\underbrace{\sum\limits_{k=1}^n\Cov(X_{n,k})\cdot t}_{\stackrel{n\to\infty}{\longrightarrow}\Gamma\text{ wg. \eqref{eqSatz6.2NB}}}\stackrel{n\to\infty}{\longrightarrow} t'\cdot \Gamma\cdot t=:\sigma^2\stackrel{\Gamma\text{ p.d.}}{>}0
	\end{align*}
	Gemäß Cauchy-Schwarz-Ungleichung (CSU) gilt:
	\begin{align}\label{eqProof6.2t}\tag{t}
		&|Y_{n,k}|=\big|\langle t,X_{n,k}\rangle\big|\stackeq{\text{CSU}}{\leq}\Vert t\Vert\cdot\Vert X_{n,k}\Vert\\\nonumber
		&\implies\sum\limits_{k=1}^n\E\Big[\underbrace{|Y_{n,k}|^2}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}\cdot\indi_{\big\lbrace\underbrace{|Y_{n,k}|}_{\leq\Vert t\Vert\cdot\Vert X_{n,k}\Vert}>\varepsilon\big\rbrace}\Big]
		\stackrel{\eqref{eqProof6.2t}}{\leq}
		\Vert t\Vert^2\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\left\lbrace\Vert X_{n,k}>\frac{\varepsilon}{\Vert t\Vert}\right\rbrace}\Big]\stackrel{n\to\infty}{\longrightarrow}0
	\end{align}
	Dies gilt gemäß \ref{satz6.1Lindeberg1922} für alle $\varepsilon>0$. 
	Somit folgt aus Satz \ref{satz6.1Lindeberg1922} dann \eqref{eqProof6.2SternStern} und dadurch \eqref{eqProof6.2Stern} und somit die Behauptung.
\end{proof}

\begin{korollar}\label{korollar6.3}
	Sei $(X_i)_{i\in\N}$ mit $X_i$ iid Zufallsvariable in $\R^d$ mit
	\begin{align*}
		&\E\left[\left(X_1^{(j)}\right)^2\right]<\infty\qquad\forall 1\leq j\leq d\\
		\mu&:=\E\big[X_1\big]=\left(\E\left(X_1^{(1)}\right),\ldots,\E\left(X_1^{(d)}\right)\right)\in\R^d,\\
		\Gamma&:=\Cov(X_1)
		=\left(\Cov\left(X_1^{(i)},X_1^{(j)}\right)\right)_{i,j=1}^d
		=\left(\E\left[\left(X_1^{(i)}-\mu_i\right)\cdot\left(X_1^{(j)}-\mu_j\right)\right]\right)_{i,j=1}^d\\&\text{ positiv definit}
	\end{align*}
	Dann gilt:
	\begin{align*}
		\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^n(X_i-\mu)\stackrel{\L}{\longrightarrow}\mathcal{N}_d(0,\Gamma)
	\end{align*}
\end{korollar}

\begin{proof}
	\begin{align}\label{eqProof6.3Stern}\tag{$\ast$}
		\frac{1}{\sqrt{n}}\cdot\sum\limits_{k=1}^n(X_k-\mu)
		&=\sum\limits_{k=1}^n\underbrace{\frac{1}{\sqrt{n}}\cdot(X_k-\mu)}_{=:X_{n,k}}\qquad\forall 1\leq k\leq n,\forall n\in\N
	\end{align}
	Dann sind \eqref{eq6.3}, \eqref{eq6.4} und \eqref{eq6.5} erfüllt und es gilt
	\begin{align*}
		\Cov(X_{n,k})
		&=\frac{1}{n}\cdot\Gamma\implies\sum\limits_{k=1}^n\Cov(X_{n,k})=\Gamma
	\end{align*}
	Somit ist \eqref{eqSatz6.2NB} erfüllt. Zu \eqref{eqSatz6.2LB}:
	\begin{align*}
		\sum\limits\E\Big[\Vert X_{n,k}\Vert^2\cdot\indi_{\big\lbrace\Vert X_{n,k}\Vert>\varepsilon\big\rbrace}\Big]
		&\stackeq{\eqref{eqProof6.3Stern}}
		\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_k-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_k-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
		&\stackeq{\text{iid}}
		\frac{1}{n}\cdot\sum\limits_{k=1}^n\E\Big[\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace\Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}\Big]\\
		&=\E\Big[\underbrace{\Vert X_1-\mu\Vert^2\cdot\indi_{\big\lbrace \Vert X_1-\mu\Vert>\varepsilon\cdot\sqrt{n}\big\rbrace}}_{\stackrel{n\to\infty}{\longrightarrow}0~\forall\varepsilon>0}\Big]
		\stackrelnew{\text{domKonv}}{n\to\infty}{\longrightarrow}0
	\end{align*}
	Hierbei geht der Satz der dominierten Konvergenz ein, denn $\Vert X_1-\mu\Vert^2$ ist Dominante und integrierbar, da 
	\begin{align*}
		\Vert X_1-\mu\Vert^2\leq\big(\Vert X_1\Vert+\Vert\mu\Vert\big)^2\leq 2\cdot\left(\Vert X_1\Vert^2+\Vert\mu\Vert^2\right)
	\end{align*}
	und 
	\begin{align*}
		\E\left[\Vert X_1\Vert^2\right]
		&=\E\left[\sum\limits_{j=1}^d\left(X_1^{(j)}\right)^2\right]
		=\sum\limits_{j=1}^d\underbrace{\E\left[\left(X_1^{(j)}\right)^2\right]}_{<\infty~\forall j}<\infty
	\end{align*}
	Somit ist \eqref{eqSatz6.2LB} erfüllt und es folgt mit \ref{satz6.2MultivariaterZGWS} die Behauptung.
\end{proof}

Für $d=1$ liefert Korollar \ref{korollar6.3} den klassischen ZGWS.

\section{Verteilungskonvergenz im Raum stetiger Funktionen} %7
Sei $I:=[a,b]$ mit $a<b$ und $C:=C(I):=\big\lbrace f:I\to\R\mid f\text{ stetig }\big\rbrace$ versehen mit der Supremumsmetrik
\begin{align*}
	d(f,g)&:=\sup\limits_{t\in I}\big|f(t)-g(t)\big|\qquad\forall f,g\in C.
\end{align*}
Gemäß Beispiel \ref{beisp2.6} ist $(C,d)$ separabel. 
Die durch $d$ induzierte Borel-$\sigma$-Algebra
%\begin{align*}
$\B(C):=\B_d\big(C(I)\big)$
%\end{align*}
gestattet einfache Beschreibung. 
Dazu:

\begin{definition}\ %7.1
	\begin{enumerate}[label=(\arabic*)]
		\item Sei $t\in I$. Die Abbildung 
		\begin{align*}
			\pi_t:C\to\R,\qquad\pi_t(f):=f(t)\qquad\forall f\in C
		\end{align*}
		heißt \textbf{Projektion in $t$}.
		\item Sei $T:=\big\lbrace t_1,\ldots,t_k\big\rbrace\subseteq I,k\in\N$. Die Abbildung
		\begin{align*}
			\pi_T:C\to\R^k,\qquad \pi_T(f):=\Big(f(t_1),\ldots,f(t_k)\Big)=:
			\Big(f(t)\Big)_{t\in T}\qquad\forall f\in C
		\end{align*}
		heißt \textbf{Projektion in $T$}.
	\end{enumerate}
\end{definition}

\begin{satz}\label{satz7.2}
	\begin{align*}
		\B(C)
		=\sigma\Big(\pi_t:t\in I\Big)=\sigma\Big(\pi_T:T\subseteq I,T\text{ endlich}\Big)
		=\sigma\Big(\big\lbrace \pi_t^{-1}(B):t\in I,B\in\B(\R)\big\rbrace\Big)
	\end{align*}
\end{satz}

\begin{proof}
	Da $\pi_t=\pi_{\lbrace t\rbrace}$ und $T=\lbrace t\rbrace$, folgt
	\begin{align}\label{eqProof7.2(1)}\tag{1}
		\sigma\Big(\pi_t:t\in I\Big)
		\subseteq\sigma\Big(\pi_T:T\subseteq I\text{ endlich}\Big)
	\end{align}
	Weiterhin gilt:
	\begin{align*}
		&\big\Vert\pi_T(f)-\pi_T(g)\big\Vert_\infty
		=\max\limits_{1\leq i\leq k}\big|f(t_i)-g(t_i)\big|
		\leq d(f,g)\qquad\forall f,g\in C,\forall T=\lbrace t_1,\ldots,t_k\rbrace\subseteq I\\
		&\implies\pi_T:(C,d)\to\R^d\text{ ist stetig}
	\end{align*}
	Aus Satz \ref{Lemma3.2} (2) folgt, dass $B_T:\B(C)\to\B(\R^k)$ messbar ist für alle endlichen Teilmengen $T\subseteq I$. 
	Da $\sigma\big(\pi_T:T\subseteq I\text{ endlich}\big)$ die \ul{kleinste} $\sigma$-Algebra auf $C$ ist, bzgl. der \ul{alle} $\pi_T,T\subseteq I$ endlich, messbar sind, folgt
	\begin{align}\label{eqProof7.2(2)}\tag{2}
		\sigma\Big(\pi_T:T\subseteq I,T\text{ endlich }\Big)\subseteq\B(C)
	\end{align}
	Wegen \eqref{eqProof7.2(1)} und \eqref{eqProof7.2(2)} bleibt zu zeigen:
	\begin{align}\label{eqProof7.2(3)}\tag{3}
		\B(C)\subseteq\sigma\Big(\pi_t:t\in I\Big)
	\end{align}
	Dazu sei $G\in\G(C)$, d.h. $G\subseteq C$ offen. 
	Gemäß Satz \ref{satz2.9} existieren $f_i\in C$, $\varepsilon_i>0$, $i\in\N$ mit
	\begin{align}\label{eqProof7.2(4)}\tag{4}
		G=\bigcup\limits_{i\in\N}B\big(f_i,\varepsilon_i\big)\text{ (offene Kugel)}.
	\end{align}
	Für jede offene Kugel
	\begin{align}\label{eqProof7.2(5)}\tag{5}
		B(f,\varepsilon)
		&=\big\lbrace g\in C:d(g,f)<\varepsilon\big\rbrace
		=\bigcup\limits_{k\in\N}\underbrace{\left\lbrace g\in C:d(g,f)\leq\varepsilon-\frac{1}{k}\right\rbrace}_{=\text{ abgeschlossene Kugel}}
	\end{align}
	Betrachte daher eine beliebige \ul{abgeschlosssene} Kugel
	\begin{align*}
		\overline{B}(f,\delta)
		&=\Big\lbrace g\in C:\sup\limits_{t\in I}\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
		&=\Big\lbrace g\in C:\sup\limits_{t\in I\cap\Q}\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
		&=\bigcap\limits_{t\in I\cap\Q}\Big\lbrace g\in C:\big|g(t)-f(t)\big|\leq\delta\Big\rbrace\\
		&=\bigcap\limits_{t\in I\cap\Q}\underbrace{\Big\lbrace g\in C:f(t)-\delta\leq \overbrace{g(t)}^{=\pi_t(g)}\leq f(t)+\delta\Big\rbrace}_{\pi_t^{-1}\Big(\big[f(t)-\delta,f(t)+\delta\big]\Big)\in\tilde{\B}}\\
		&\implies
		\overline{B}(f,\delta)\in\tilde{\B}\qquad\forall f\in C,\forall\delta>0\\
		&\stackrel{\eqref{eqProof7.2(5)}}{\implies}
		B(f,\varepsilon)\in\tilde{\B}\qquad\forall f\in C,\forall\varepsilon>0\\
		&\stackrel{\eqref{eqProof7.2(4)}}{\implies}
		G\in\tilde{\B}\\
		&\implies
		\G(C)\subseteq\tilde{\B}\\
		&\implies\sigma\big(\G(C))=\B(C)\subseteq\tilde{\B}
	\end{align*}
\end{proof}
%Ferger hat Borel-Mengen am Anfang des Studiums gehasst, weil er sie nicht verstanden hat. Jetzt sieht er das anders.

Ein stochastischer Prozess $\big\lbrace X(t):t\in I\big\rbrace$ heißt \textbf{indiziert nach $I$ (über $(\Omega,\A)$)}
\begin{align*}
	:\Longleftrightarrow X(t):(\Omega,\A)\to(\R,\B(\R)),
	\qquad\omega\mapsto X(t)(\omega)\quad
	\text{ist messbar}\qquad\forall t\in I
\end{align*}
%Ferger: "Trage ich zu einer Verwirrung bei?"
Schreibweise: $X(t,\omega):=X(t)(\omega)$\\
Falls die \textbf{Pfade / Trajektorien}
\begin{align*}
	X(\cdot,\omega):I\to\R,\qquad t\mapsto X(t,\omega)\qquad\forall \omega\in\Omega
\end{align*}
stetig sind (für alle $\omega\in\Omega$), so liefert die Zuordnung $\omega\mapsto X(\cdot,\omega)$ eine Abbildung $X:\Omega\to C(I)$. 
Diese Abbildung $X$ heißt \textbf{Pfadabbildung} des stochastischen Prozesses und wird mit diesem identifiziert.\nl
%TODO Hier kann man eine Skizze einfügen
Satz \ref{satz7.2} liefert ein sehr handliches Kriterium für die Messbarkeit von allgemeinen Abbildungen $X:\Omega\to X$.

\begin{satz}\label{satz7.3}
	Sei $(\Omega,\A)$ messbarer Raum und $C:=C(I)$ versehen mit der Borel-$\sigma$-Algebra $\B(C)$ sowie $X:\Omega\to C$. Dann gilt:
	\begin{align*}
		X\text{ ist }A\text{-}\B(C)\text{-messbar}\Longleftrightarrow\forall t\in I:\pi_t\circ X\text { ist }\A\text{-}\B(R)\text{-messbar}
	\end{align*}
\end{satz}

\begin{proof}
	Erinnere an das Messbarkeitslemma:\nl
	\textbf{Lemma. (Messbarkeitslemma)}\\
	Sei $(\Omega,\A)$ messbarer Raum, $\Omega'\neq\emptyset$, $(f_t)_{t\in T}$ eine Familie von Abbildungen\\ $f_t:\Omega'\to(\Omega,\A)$ wobei $(\Omega_t,\A_t)$ messbarer Raum für alle $t\in T$, $T\neq\emptyset$ beliebige Indexmenge. 
	Ferner sei
	\begin{align*}
		\A':=\sigma\big(f_t:t\in T\big)\stackeq{\text{Def}}\sigma\left(\bigcup\limits_{t\in T} f_t^{-1}(\A_t)\right)
	\end{align*}
	Dann gilt für eine Abbildung $Z:\Omega\to\Omega'$:
	\begin{align*}
		Z\text{ ist }\A\text{-}\A'\text{-messbar}\Longleftrightarrow\forall t\in T: f_t\circ Z\text{ ist }\A\text{-}\A_t\text{-messbar}
	\end{align*}
	Im Falle von Satz \ref{satz7.3}: $\Omega'=C, f_t=\pi_t, T=I,\A'=\sigma\big(\pi_t:T\in I\big)\stackeq{\ref{satz7.2}}\B(C),Z=X$
\end{proof}

Stochastische Prozesse mit stetigen Pfaden nennt man \textbf{stetige} stochastische Prozesse. 
Satz \ref{satz7.3} besagt nun: 
Die Pfadabbildung eines jeden stetigen stochastischen Prozesses ist $\A\text{-}\B(C)$-messbar. 
Insbesondere folgt:\\
Jeder stetige stochastische Prozess indiziert nach $I$ kann aufgefasst werden als Zufallsvariable im metrischen Raum $\big(C(I),d\big)$.

\begin{beispiel}\label{beispiel7.4}
	Sei $(\xi_i)_{i\in\N}$ eine Folge von reellen Zufallsvariablen über $(\Omega,\A)$ und für $n\in\N$ sei
	\begin{align*}
		S_k:=\sum\limits_{i=1}^k\xi_i\qquad\forall0\leq k\leq n\qquad S_0:=0
	\end{align*}
	Der Polygonzug $X_n$ durch die Punkte $\left(\frac{k}{n},\frac{1}{\sqrt{n}}\cdot S_k\right),0\leq k\leq n$ heißt \textbf{$n$-ter Partialsummenprozess}.
	%TODO Hier Skizze einfügen (ist echt eine schöne Skizze)
	Mit der Zwei-Punkte-Formel der Geradengleichung folgt:
	\begin{align*}
		X_n(t)=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^{\lfloor n\cdot t\rfloor}\xi_i+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lfloor n\cdot t\rfloor\big)\cdot\xi_{\lfloor n\cdot t\rfloor+1}\qquad\forall t\in[0,1]=I
	\end{align*}
	Hierbei ist die Gaußklammer (Abrundefunktion) wie folgt definiert:
	\begin{align*}
		\lfloor u\rfloor:=[u]:=\max\big\lbrace l\in\Z:l\leq n\big\rbrace
	\end{align*}
	$X_n(t)$ ist eine reelle Zufallsvariable für alle $t\in I$. 
	Gemäß Konstruktion (Polygonzug) ist jeder Pfad von $X_n$ stetig auf $[0,1]$. 
	Aus Satz \ref{satz7.3} folgt, dass $X_n~\A\text{-}\B(C)$-messbar ist, also Zufallsvariable in $\Big(C\big([0,1]\big),d\Big)$. 
	Weitere Anwendung von Satz \ref{satz7.2}:
\end{beispiel}

\begin{satz}\label{satz7.5}\
	\begin{enumerate}[label=(\arabic*)]
		\item $P,Q$ seien Wahrscheinlichkeitsmaße auf $\B(C)$. Dann gilt:
		\begin{align*}
			P=Q\Longleftrightarrow\forall T\subseteq I\text{ endlich }: P\circ\pi_T^{-1}=Q\circ\pi_T^{-1}
		\end{align*}
		\item $X,Y$ seien Zufallsvariablen in $\big(C(I),d\big)$. Dann gilt:
		\begin{align*}
			X\stackeq{\L}Y\Longleftrightarrow \pi_T(X)\stackeq{\L}\pi_T(Y)
		\end{align*}
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zu (1), zeige ``$\implies$'':} Trivial.\nl
	\underline{Zu (1), zeige ``$\Longleftarrow$'':}\\
	Gemäß Satz \ref{satz7.2} ist
	\begin{align*}
		\mathcal{E}:=\left\lbrace\pi_T^{-1}(A):A\in\B(\R^{|T|},T\subseteq I\text{ endlich}\right\rbrace
	\end{align*}
	ein Erzeuger von $\B(C)$, d.h. $\B(C)=\sigma(\mathcal{E})$. Erinnerung:
	\begin{align*}
		g^{-1}(\mathcal{C})&:=\big\lbrace g^{-1}(C):C\in\mathcal{C}\big\rbrace\text{ für Mengenfamilie }\mathcal{C}\\
		\B(C)&\stackeq{\ref{satz7.2}}\sigma\big(\pi_T:T\subseteq I\text{ endlich}\big)
		=\sigma\Bigg(\underbrace{\bigcup\limits_{\begin{subarray}{c}T\subseteq I\\T\text{ endlich}\end{subarray}}\pi_T^{-1}\left(\B\left(\R^{|T|}\right)\right)}_{=\mathcal{E}}\Bigg)
	\end{align*}
	Nach Voraussetzung gilt:
	\begin{align*}
		\underbrace{P\circ\pi_T^{-1}(A)}_{\stackeq{\text{Def}}P\left(\pi_T^{-1}(A)\right)}&=\underbrace{Q\circ\pi_T^{-1}(A)}_{=Q\left(\pi_T^{-1}(A)\right)}\qquad\forall A\in\B\left(\R^{|T|}\right)\\
		\implies
		P|_\mathcal{E}&=Q|_\mathcal{E}
	\end{align*}
	$\mathcal{E}$ ist durchschnittsstabil (nachrechnen!). 
	Jetzt liefert der \textit{Maßeindeutigkeitssatz}, dass $P=Q$ auf $\sigma(\mathcal{E})=\B(C)$.
	(Der Maßeindeutigkeitssatz besagt ungefähr: 
	zwei Wahrscheinlichkeitsmaße, die auf einen Schnittstabilen Mengensystem überein stimmen, stimmen auch auf dessen Erzeugnis überein.)\nl
	\underline{Zeige (2):}
	\begin{align*}
		X\stackeq{\L} X
		\overset{\text{Def}}&{\Longleftrightarrow}
		\P\circ X^{-1}=\P\circ Y^{-1}\\
		\overset{(1)}&{\Longleftrightarrow}
		\underbrace{\left(\P\circ X^{-1}\right)\circ \pi_T^{-1}}_{=\P\circ\left(\pi_T\circ X\right)^{-1}}
		=\underbrace{\left(\P\circ Y^{-1}\right)\circ\pi_T^{-1}}_{=\P\circ\left(\pi_T\circ Y\right)^{-1}}
		&\forall T\subseteq T\text{ endlich}\\
		&\Longleftrightarrow
		\underbrace{\pi_T\circ X}_{=\pi_T(X)}\stackeq{\L}\underbrace{\pi_T\circ Y}_{=\pi_T(Y)}
		&\forall T\subseteq I\text{ endlich}
	\end{align*}
\end{proof}

\begin{bemerkungnr} %7.6
	Die Wahrscheinlichkeitsmaße $\P\circ\pi_T^{-1}$ bzw. die Verteilungen
	\begin{align*}
		\pi_T(X)=\big(X(t_1),\ldots,X(t_k)\big),\qquad T=\big\lbrace t_1,\ldots,t_k\rbrace\subseteq I
	\end{align*}
	heißen \textbf{endlich dimensionale Randverteilungen von $\P$} bzw. von $X$.\\
	Insbesondere ist gemäß \ref{satz7.5} (2) die Verteilung eines stetigen stochastischen Prozesses $X$ aufgefasst als Zufallsvariable in $C$ eindeutig durch die Verteilungen der Vektoren
	\begin{align*}
		\big(X(t_1),\ldots,X(t_k)\big),\qquad t_1,\ldots,t_k\in I,k\in\N
	\end{align*}
	festgelegt.
\end{bemerkungnr}

Nächstes Ziel: Handhabbare Kriterien für den Nachweis der Verteilungskonvergenz in $C$. Dazu:

\begin{definition} %7.z
	Für eine Funktion $f\colon I\to\R$ und $\delta>0$ definiere den \textbf{Stetigkeitsmodul / Oszillationsmodul}
	\begin{align*}
		\omega(f,\delta):=\sup\limits\Big\lbrace \big|f(s)-f(t)\big|:s,t\in I\mit |s-t|\leq\delta\Big\rbrace
	\end{align*}
\end{definition}

Aus der Analysis ist bekannt:
\begin{align*}
	f\in C(I)\Longleftrightarrow\omega(f,\delta)\stackrel{\delta\to0}{\longrightarrow}0
\end{align*}

\begin{lemma}\label{lemma7.8}
	$\omega(\cdot,\delta)\colon (C,d)\to\R$ ist stetig für jedes $\delta>0$ und damit gemäß Lemma \ref{Lemma3.2} (2) auch $\B(C)$-$\B(\R)$-messbar.
\end{lemma}

\begin{proof}
	Sei $g\in C$ und $s,t\in I$ mit $|s-t|\leq\delta$. 
	Dann gilt:
	\begin{align*}
		\big| f(s)-f(t)\big|
		&=\big|f(s)-g(s)+g(s)-g(t)+g(t)-f(t)\big|\\
		\overset{\text{DU}}&{\leq}
		\underbrace{\big|f(s)-g(s)\big|}_{\leq d(f,g)}+\underbrace{\big| g(s)-g(t)\big|}_{\leq\omega(g,\delta)}+\underbrace{\big| g(t)-f(t)\big|}_{\leq d(f,g)}\\
		&\leq
		\omega(g,\delta)+2\cdot d(f,g)\\
		\overset{\sup}&{\implies}
		\omega(f,\delta)
		\leq\omega(g,\delta)+2\cdot d(f,g)\\
		&\implies \omega(f,\delta)-\omega(g,\delta)
		\leq 2\cdot d(f,g) &\forall f,g\\
		&\implies \omega(g,\delta)-\omega(f,\delta)
		\leq 2\cdot d(g,f)=2\cdot d(f,g)\\
		&\implies
		\big|\omega(f,\delta)-\omega(g,\delta)\big|\leq 2\cdot d(f,g)
	\end{align*}
	Das heißt, $\omega(\cdot,\delta)$ ist sogar Lipschitz-stetig.
\end{proof}

%Ferger: "Ich war übrigens gestern in der Stadt. Da bin ich eigentlich nie. Ich war in mindestens 7 oder Schuhgeschäften. Es war noch jemand dabei. Ich selber habe kein Schuhe gekauft. Und in jedem Geschäft war die Verweildauer sehr lang. Und jedes Schuhgeshcäft wurde GESCANNT: Jeder Schuh wurde angefasst und begutachtet. [...] Weihnachten ist was Schönes!"

%Ferger: "Wenn man sich rote Schuhe kauft, muss dazu auch noch eine passende rote Tasche kaufen. Wussten Sie das?"

Erstes Kriterium in:

\begin{satz}\label{satz7.9}
	Seien $X,X_n,n\in\N$ Zufallsvariablen in $C$ über $(\Omega,\A,\P)$. Falls
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\big(X_n(t_1),\ldots,X_n(t_k)\big)\stackrel{\L}{\longrightarrow}\big(X(t_1),\ldots,X(t_k)\big)
			\qquad\forall t_1,\ldots,t_k\in I,\forall k\in\N
		\end{aligned}$\\
		so genannte \textbf{Konvergenz der fidis} (finite dimensional distributions).\\
		Notation: $X_n\stackrelnew{\text{fd}}{}{\longrightarrow} X$
		\item $\begin{aligned}
			\lim\limits_{k\to\infty}\limsup\limits_{n\to\infty}\P\Big(\omega\big(X_n,\delta_k\big)>\varepsilon\Big)=0\qquad\forall\varepsilon>0
		\end{aligned}$\\
		für eine Folge $(\delta_k)_{k\in\N}\subseteq(0,\infty)\mit\delta_k\downarrow0$
	\end{enumerate}
	so folgt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow}X\text{ in }(C,d)
	\end{align*}
\end{satz}

\begin{proof}
	Siehe Gänssler und Stute (1977) \textit{Wahrscheinlichkeitstheorie}, Seite 348
\end{proof}

\begin{beispiel}\label{beispiel7.10} Sei 
	\begin{align*}
		X_n(t):=A_n+B_n\cdot t+C_n\cdot t^2\qquad\forall t\in I
		\qquad\mit \big(A_n,B_n,C_n)\stackrel{\L}{\longrightarrow}(A,B,C)\in\R^3
	\end{align*}
	Dann gilt:
	\begin{align*}
		X_n\stackrel{n\to\infty}{\longrightarrow} X\text{ in }(C,d)\qquad\mit\qquad X(t)=A+B\cdot t+C\cdot t^2
	\end{align*}
	
	\begin{proof}
		Seien $t_1,\ldots,t_k\in I$ beliebig. 
		Für Voraussetzung (1) in Satz \ref{satz7.9} reicht es gemäß Cramér-Wold-Device \ref{satz5.4CramerWoldDevice} zu zeigen:
		\begin{align*}
			\sum\limits_{j=1}^k\lambda_j\cdot X_n(t_j)\stackrel{\L}{\longrightarrow}\sum\limits_{j=1}^k\lambda_j\cdot X(t_j)\text{ in }\R\qquad\forall\lambda=\big(\lambda_1,\ldots,\lambda_k\big)\in\R^k
		\end{align*}
%Ich wurde von Prof Ferger bemitleidet, weil ich in Tex nicht alles umsetzen kann, was in der Tafel durch Wisch-Technik erzeugt.
		Dazu: 
		\begin{align*}
			\sum\limits_{j=1}^k\lambda_j\cdot X_n(t_j)
			&=A_n\cdot\sum\limits_{j=1}^n\lambda_j+B_n\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j+C_n\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j^2\\
			\overset{\L,\ref{satz4.10ContinuousMappingTheorem}}&{\longrightarrow}
			A\cdot\sum\limits_{j=1}^k\lambda_j+B\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j+C\cdot\sum\limits_{j=1}^k\lambda_j\cdot t_j^2
			=\sum\limits_{j=1}^k\lambda_j\cdot X(t_j)
		\end{align*}
		Zu Voraussetzung (2) in Satz \ref{satz7.9}:
		\begin{align*}
			\big|X_n(s)-X_n(t)\big|
			&=\Big|B_n\cdot(s-t)+C_n\cdot\underbrace{\big(s^2-t^2\big)}_{(s-t)\cdot(s+t)}\Big|\\
			\overset{\text{DU}}&{\leq}
			|B_n|\cdot\underbrace{|s-t|}_{\leq\delta}+|C_n|\cdot\underbrace{|s-t|}_{\leq\delta}\cdot\underbrace{|s+t|}_{\leq|s|+|t|\leq:K}\\
			&\leq
			|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta\qquad\forall s,t\in I\mit |s-t|\leq\delta\\
			\overset{\sup}{\implies}
			\omega(X_n,\delta)
			&\leq|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta\\
			\implies
			\P\Big(\omega\big(X_n,\delta\big)>\varepsilon\Big)
			&\leq\P\Big(|B_n|\cdot\delta+K\cdot|C_n|\cdot\delta>\varepsilon\Big)\\
			\overset{\eqref{eqProofBeispiel7.10}}&{\leq}
			\P\Big(|B_n|\cdot\delta>\frac{\varepsilon}{2}\Big)+\P\Big(K\cdot|C_n|\cdot\delta>\frac{\varepsilon}{2}\Big)\\
			&=
			\P\Big(|B_n|>\frac{\varepsilon}{2\cdot\delta}\Big)+\P\Big(|C_n|>\frac{\varepsilon}{2\cdot K\cdot\delta}\Big)\\
			&=
			1-F_n\left(\frac{\varepsilon}{2\cdot\delta}\right)+1-G_n\left(\frac{\varepsilon}{2\cdot K\cdot\delta}\right)
		\end{align*}
%Ferger: "Warum habe ich das eigentlich so gemacht!?"
		Hierbei ist $F_n$ die Verteilungsfunktion von $|B_n|\stackrel{\L}{\longrightarrow}|B|$ und $G_n$ die Verteilungsfunktion von $|C_n|$.
		Erinnerung:
		\begin{align}\label{eqProofBeispiel7.10}
			\big\lbrace U+V>\varepsilon\big\rbrace\subseteq\left\lbrace U>\frac{\varepsilon}{2}\right\rbrace\cup\left\lbrace V>\frac{\varepsilon}{2}\right\rbrace
		\end{align}
		Mit Korollar \ref{korollar4.5} folgt, dass es eine Fologe $(\delta_k)_{k\in\N}$  mit $\delta_k\downarrow0$, 
		so dass $\frac{\varepsilon}{2\cdot\delta_k}$ und $\frac{\varepsilon}{2\cdot K\cdot\delta_k}$ Stetigkeitsstellen der jeweiligen Grenz-Verteilungfunktionen sind,
%Ferger: "Na gut, einen Nobelpreis für Literatur kriege ich jetzt nicht."
		\begin{align*}
			\limsup\limits_{n\to\infty}\P\Big(\omega\big(X_n,\delta_k\big)>\varepsilon\Big)\leq\P\Big(|B|>\underbrace{\frac{\varepsilon}{2\cdot\delta_k}}_{\stackrel{k\to\infty}{\longrightarrow}\infty}\Big)
			+\P\Big(|C|>\underbrace{\frac{\varepsilon}{2\cdot K\cdot\delta_k}}_{\stackrel{k\to\infty}{\longrightarrow}\infty}\Big)
			\qquad\forall k\in\N
		\end{align*}
%Ferger: "Hach ich bin so doof. Ich hätte mir das Leben leichter machen können."
		$k\to\infty$ liefert dann (2).\\
		\textbf{Aufgabe:} Zeige dieses Beispiel durch CMT \ref{satz4.10ContinuousMappingTheorem}.
	\end{proof}
\end{beispiel}
 
Zweite Möglichkeit für den Nachweis der Verteilungskonvergenz in $C$ liefert:

\begin{satz}[Momentenkriterium von Kolmogoroff]\label{satz7.11MomentenkriteriumVonKolmogoroff}\enter
	Seien $X,X_n,n\in\N$ Zufallsvariablen in $C$ mit
	\begin{align}\label{eqSatz7.11Vor1}\tag{1}
		X_n\stackrelnew{\text{fd}}{}{\longrightarrow} X\qquad\text{(vgl. (1) in \ref{satz7.9})}
	\end{align}
	Falls es eine Konstante $\gamma>0$ und $\alpha>1$ sowie eine stetige und monoton wachsende Funktion $F:I\to\R$ gibt, derart, dass
	\begin{align}\label{eqSatz7.11VorM}\tag{M}
		\E\Big[|X_n(s)-X_n(t)|^\gamma\Big]\leq\big(F(s)-F(t)\big)^\alpha\qquad\forall s>t,s,t\in I
	\end{align}
	Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} X\text{ in }\big(C(I),d\big)
	\end{align*}
\end{satz}

\begin{proof}
	Siehe Billingsley (1968), \textit{Convergence of probability measures}, Seite 96.
%Ferger: "Das Buch hier war früher meine Fibel."
%Ferger: "Jim Morrison von den Doors. Das war mein Held. Neben Skorokott."
\end{proof}

\ul{Ziel:} Verteilungskonvergenz der Partialsummenprozesse $X_n$ aus dem Beispiel \ref{beispiel7.4}. 
Dazu:

\begin{definition}\label{def7.12} %7.12
	Sei $I=[0,b]\mit b>0$ und $B:=\big\lbrace B(t):=B(t,\omega)\in I\big\rbrace$ ein stetiger stochastischer Prozess über $(\Omega,\A,\P)$ mit 
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			B(0)=B(0,\omega)=0\qquad\forall\omega\in\Omega
		\end{aligned}$
		\item $\begin{aligned}
			\forall 0=:t_0\leq t_1<\ldots<t_r\leq b
		\end{aligned}$ sind die Zuwächse $B(t_i)-B(t_{i-1}),1\leq i\leq r$\\ \ul{unabhängig}
		\item $\begin{aligned}
			0\leq s<t\leq b\implies B(t)-B(s)\sim\mathcal{N}(0,t-s)
		\end{aligned}$
	\end{enumerate}
	Dann heißt $B$ \textbf{Brownsche Bewegung (BB)} auf $I$.\\
	Vollkommen analog definiert man eine BB auf $I=[0,\infty)$.
\end{definition}

%Ferger: "Das ist wie so ein Wunschzettel. Es gelte 1, 2 und 3. Passend zur Weihnachtszeit. Wenn's dumm läuft, kann es noch sein, dass mein Wunsch nicht erfüllt wird, weil er nicht erfüllt werden kann."
%Ferger: "In der Hoffnung das Sie nicht schonmal hier gesessen haben: Weil ich immer dieselben Geschichten erzähle. Ich bin jetzt in so einem Alter wo man alles mehrfach erzählt."
%Ferger: "Da hat mal jemand eine Doktorarbeit geschrieben über eine tolle Fukntionenklasse. Und dann kam jemand daher, dass die Funktionenklasse nur aus der Eins-Funktion besteht. Das war echt ein Griff ins Klo. Wäre noch heftiger gewesen, wenn die Funktionenklasse leer gewesen wäre. Aber: So sind wir ja auch alle gestrickt. So werden wir konditionert. Weil, wenn der Mathematiker irgendwas hört, fragt der Mathematiker ständig "Existiert das?""

\begin{satz}[Lévy]\label{satz7.13Levy}
	Eine BB existiert.
\end{satz}

\begin{proof}
	Siehe Vorlesung \textit{Stochastische Prozesse}.
%Ferger: "Es gibt einen konstruktiven Beweis, der ist auch lehrrreich, aber da haben wir jetzt keine Zeit für.
%Anmerkung des Autors: Geschichten erzhählen ist wohl wichtiger
\end{proof}

\begin{lemma}\label{lemma7.14}
	Sei $B$ eine BB auf $I$ und $t_1<\ldots<t_r$ aus $I$. Dann gilt:
	\begin{align*}
		\Big(B(t_1),\ldots, B(t_r)\Big)^T\sim\mathcal{N}_r(0,\Gamma)\text{ wobei }\\
		\Gamma:=\Big(\Cov\big(B(t_i),B(t_j)\big)\Big)_{1\leq i,j\leq r}=\big(t_i\wedge t_j\big)_{1\leq i,j,\leq r}
	\end{align*}
	% \wedge meint Minimum
\end{lemma}

\begin{proof}
	Sei $t_0:=0$. Dann gilt:
	\begin{align*}
		B(t_j)
		\overset{\text{\ref{def7.12}(1)}}=
		B(t_j)-\underbrace{B(t_0)}_{=0}
		\overset{\text{Teles}}{=}
		\sum\limits_{i=1}^j\big(B(t_i)-B(t_{i-1})\big)
		=\sum\limits_{i=1}^j\sqrt{t_i-t_{i-1}}\cdot\underbrace{\frac{B(t_i)-B(t_{i-1})}{\sqrt{t_i-t_{i-1}}}}_{=:Z_i}
	\end{align*}
	Da $B(t_i)-B(t_{i-1})\sim\mathcal{N}(0,t_i-t_{i-1})$ folgt, dass die $Z_i$ i.i.d. $\sim\mathcal{N}(0,1)$ sind.
	Also ist der Vektor
	\begin{align*}
		\begin{pmatrix}
			B(t_1)\\
			\vdots\\
			B(t_r)
		\end{pmatrix}&=\begin{pmatrix}
			\sqrt{t_1} & 0 & \hdots & \hdots & 0\\
			\sqrt{t_1} & \sqrt{t_2-t_1} & 0 & \hdots & 0\\
			\vdots & \vdots & \ddots & \ddots & \vdots\\
			\sqrt{t_1} & \sqrt{t_2-t_1} & \sqrt{t_3-t_2} & \hdots & 0\\
		\end{pmatrix}\cdot\begin{pmatrix}
			Z_1\\
			\vdots\\
			Z_r
		\end{pmatrix}\implies\begin{pmatrix}
			B(t_1)\\
			\vdots\\
			B(t_r)
		\end{pmatrix}\sim\mathcal{N}_r(\mu,\Gamma)\\
		\mu_i:&=\E\Big[B(T_i)\Big]=\E\Big[\underbrace{B(t_i)-B(t_0)}_{\sim\mathcal{N}(0,t_i-t_0)}\Big]\qquad\forall i\\
		\implies\mu_i :&=\big(\mu_1,\ldots,\mu_r\big)=0
	\end{align*}
	Ferner sei $i\leq j$. Dann gilt:
	\begin{align*}
		\Cov\big(B(t_i),B(t_j)\big)
		&=\E\Big[B(t_i)\cdot B(t_j)\Big]\\
		&=\E\Big[B(t_i)\cdot\big( B(t_j)-B(t_i)+B(t_i)\big)\Big]\\
		&=\E\Big[\underbrace{B(t_i)\cdot\big(B(t_j)-B(t_i)\big)}_{\text{beide Faktoren sind unabh.}}+\big(B(t_i)\big)^2\Big]\\
		&=\E\Big[B(t_i)\cdot\big(B(t_j)-B(t_i)\big)\Big]+\E\Big[\big(B(t_i)\big)^2\Big]\\
		\overset{\text{unab}}&=
		\underbrace{\E\big[B(t_i)\big]}_{=}\cdot\underbrace{\E\big[B(t_j)-B(t_i)\big]}_{=0}+\underbrace{\E\Big[\big(B(t_i)\big)^2\Big]}_{=\Var\big(B(t_i)\big)}\\
		\overset{\text{Vert}}&=
		\Var\big(\mathcal{N}(0,t_i)\big)\\
		&=t_i\\
		\overset{i\leq j}&{=}
		t_i\wedge t_j
	\end{align*}
	Analog behandle den Fall $i\geq j$.
\end{proof}

\begin{korollar}\label{korollar7.15Folgerung}
	Die Verteilung einer BB ist eindeutig bestimmt. 
\end{korollar}

\begin{proof}
	Seien $B$ und $\tilde{B}$ zwei BBs (i.d.R. über unterschiedlichen Wahrscheinlichkeitsräumen definiert). Dann gilt:
	\begin{align*}
		B\stackeq{\L}\tilde{B}
		\overset{\ref{satz7.5}}&{\Longleftrightarrow}
		\big(B(t_1),\ldots,B(t_r)\big)^T\stackeq{\L}\big(\tilde{B}(t_1),\ldots,\tilde{B}(t_r)\big)^T
		&\forall t_1<\ldots<t_r\in I,\forall r\in\N\\
		\overset{\ref{lemma7.14}}&\Longleftrightarrow
		\mathcal{N}_r(0,\Gamma)=\mathcal{N}_r(0,\Gamma)
		&\forall t_1<\ldots<t_r\in I,\forall r\in\N
	\end{align*}
	Die letzte Aussage gilt aber, weil jede mehrdimensionale Normalverteilung $\mathcal{N}_r(\mu,\Gamma)$ eindeutig durch $\mu$ und $\Gamma$ festgelegt ist:
	\begin{align*}
		\mathcal{N}(\mu,\sigma^2)=\mathcal{N}(m,s^2)\Longleftrightarrow m=\mu\text{ und }s^2=\sigma^2
	\end{align*}
\end{proof}

Die Verteilung $\P\circ B^{-1}=:W$ von  einer Brownschen Bewegung $B$ heißt \textbf{Wiener-Maß} (nach Norbert Wiener).
%Ferger: "Absoluter Schlaukopf. Ein Ammi. Ein US-Amerikaner. Der hat 21 promoviert. Da wo wir noch mit den Klötzchen spielen, hat der schon promoviert."
\begin{align*}
	B:(\Omega,\A,\P)\to\Big(C(I),\B_d\big(C(I)\big)\Big)
\end{align*}
%Ferger: "Die Brownsche Bewegung wird genannt Brownsche Bewegeung, weil folgendes passiert ist: es gab einen schottischen Bonatiker. Der hat also eine Flüssigkeit genommen, z. B. Wasser und dann ganz kleine Pollenkörner oder Staubkörnchen, also so was ganz klitzekleines oder eie kleine Minischuppe in sein Töpfchen getan, in seine Flüssigkeit und hat draufgeguckt und gesagt "Boah, da ist ja was los." [...] Ja ich erzähl das so wie bei der Sendung mit der Maus, weil man das so gut versteht. [...] Und er stellt fest, dass da viel los ist. Die Beobachtung hatten andere Leute wohl auch schon gemacht. Die ersten Leute, jeder von denen hat sich natürlich gefragt: Was steckt dahinter? Die ersten Leute haben als erklärungsversuch gegeben, dass es biologische Energie in sich hat. Der Robert Braun hat gesagt: Nein, das ist nicht so. Können Sie ja mal googeln. Entscheidend ist Folgendes: Das war ca. 182?. Dann kam Albert Einstein und hat gesagt: Nenene bzw. jajaja, was hier passiert ist Folgendes: So eine Flüssigkeit besteht ja aus ganz vielen Molekülen, die in Bewegung sind. Und die schießen wie wild hin und her. Die Moleküle sind aber klitzeklitzeklein. Das kleine Teilchen ist aber im mirkoskopischen Bereich, also im Vergleich zu den Molekülen ein Riesen-Oschi. Und die kleinen Teilchen hauen die ganze Zeit dagegen. Der Einstein ist hergegangen und hat das für Physiker-Verhältnisse sehr mathematisch beschrieben. Der Wiener hat das nochmal auf saubere, mathematische Füße gestellt (Funtionenraum, Verteilungen und solche Sachen...) Die haben das ein oder andere wohl handwaving gemacht und Norbert Wiener hat das dann mal mathematisch sauberr gemacht. Deshalb heißt die Verteilung einer Brownschen auch Wiener Maß.
%Ferger: Sind ja immernoch 7 Minuten. Mist. Na da kann ich ja noch eine Geschichte erzählen. 

\begin{satz}[Donsker]\label{satz7.16Donsker}%\enter
	Seien $(\xi)_{i\in\N}$ i.i.d. mit $\E[\xi_1]=0$ und $\Var(\xi_1)=1$.
	\begin{align*}
		S_k&:=\sum\limits_{i=1}^k\xi_i&\forall& k\in\N_0\\
		X_n(t)&:=\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t\rceil}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1} &\forall& t\in I=[0,b]
	\end{align*}
	$b>0$ fest), d.h. $X_n$ ist der Polygonzug durch die Punkte $\left(\frac{k}{n},\frac{1}{\sqrt{n}}\cdot S_k\right)_{0\leq k\leq b\cdot n}$. Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow} B\text{ in }\big(C(I),d\big)
	\end{align*}
	wobei $B$ eine BB auf $I=[0,b]$.
\end{satz}

\begin{proof}
	Anwendung von Satz \ref{satz7.11MomentenkriteriumVonKolmogoroff}. 
	Zeige also dessen Voraussetzungen:\nl
	\underline{\eqref{eqSatz7.11Vor1} Zeige Konvergenz der fidis:}\\
	Seien $0\leq t_1<\ldots<t_r\leq b$. 
	Dann gilt:
	\begin{align*}
		\left\Vert\big(X_n(t_i)\big)_{1\leq i\leq r}-\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}\right\Vert
		&=\Bigg(\sum\limits_{i=1}^r\underbrace{\left|X_n(t_i)-\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right|^2}_{\frac{1}{\sqrt{n}}\cdot\underbrace{\big|n\cdot t_i-\lceil n\cdot t_i\rceil\big|}_{\leq1}\cdot\xi_{\lceil n\cdot t_i\rceil}}\Bigg)^{\frac{1}{2}}\\
		&\leq
		\frac{1}{\sqrt{n}}\cdot\left(\sum\limits_{i=1}^r\xi_{\lceil n\cdot t_i\rceil+1}^2\right)^{\frac{1}{2}}
		\implies\\
		\P\left(\left\Vert\big(X_n(t_i)\big)_{1\leq i\leq r}-\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}\right\Vert\right)
		&\leq\P\left(\sum\limits_{i=1}^r\xi^2_{\lceil n\cdot t_i\rceil+1}>n\cdot\varepsilon^2\right)\\
		\overset{\text{Markov}}&\leq
		\frac{1}{n}\cdot\varepsilon^{-2}\cdot\sum\limits_{i=1}^r\underbrace{\E\left[\xi^2_{\lceil n\cdot t_i\rceil+1}\right]}_{=1~\forall i}\\
		&=
		\frac{1}{n}\cdot\varepsilon^{-2}\cdot r\stackrel{n\to\infty}{\longrightarrow}0\qquad\forall\varepsilon>0
	\end{align*}
	Folglich sind die beiden Folgen
	\begin{align*}
		\big(X_n(t_i)\big)_{1\leq i\leq r}\qquad\text{und}\qquad\left(\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{\lceil n\cdot t_i\rceil}\xi_j\right)_{1\leq i\leq r}
	\end{align*}
	stochastisch äquivalent. 
	Wegen Cramér (Satz \ref{satz4.14Cramer} genügt reicht es
	\begin{align}\label{eqProof7.16fd}\tag{fd}
		\left(\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t_i\rceil}\right)_{1\leq i\leq r}\stackrel{\L}{\longrightarrow}\big(B(t_1),\ldots,B(t_r)\big)
	\end{align}
	zu zeigen. 
	Aber \eqref{eqProof7.16fd} ist äquivalent zu 
	\begin{align}\label{eqProof7.16Stern}\tag{$\ast$}
		\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)_{1\leq i\leq r}\stackrel{\L}{\longrightarrow}\big(B(t_i)-B(t_{i-1})\big)_{1\leq i\leq r}
	\end{align}
	denn:
	\begin{align*}
		h:\R^r\to\R^r,\qquad h(x_1,\ldots,x_r):=\big(x_1,x_2-x_1,x_3-x_2,\ldots,x_r-x_{r-1}\big)
	\end{align*}
	ist stetig auf $\R^r$ und hat stetige Inverse $h^{-1}$ mit 
	\begin{align*}
		h^{-1}(y_1,\ldots,y_r)=\big(y_1,y_1+y_2,\ldots,y_1+\ldots+y_r\big)
	\end{align*}
	Also ist \eqref{eqProof7.16fd} äquivalent zu \eqref{eqProof7.16Stern} gemäß CTM \ref{satz4.10ContinuousMappingTheorem}.\\
	Beachte wegen Blockungslemma sind die Komponenten $\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)_{1\leq i\leq r}$ \underline{unabhängig} und gemäß \ref{def7.12} (2) sind auch $\big(B(t_i)-B(t_{i-1})\big)$, $a\leq i\leq r$ unabhängig. 
	Somit ist gemäß Satz \ref{satz4.21} \eqref{eqProof7.16Stern} äquivalent zu 
	\begin{align}\label{eqProof7.16SternStern}\tag{$\ast\ast$}
		\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)
		\stackrel{\L}{\longrightarrow}
		\big(B(t_i)-B(t_{i-1})\big)\text{ in }\R\qquad\forall 1\leq i\leq r
	\end{align}
	Dazu setze $k_n:=\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil$. Dann gilt:
	\begin{align*}
		&\frac{1}{\sqrt{n}}\cdot\left(S_{\lceil n\cdot t_i\rceil}-S_{\lceil n\cdot t_{i-1}\rceil}\right)\\
		&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=\lceil n\cdot t_{i-1}\rceil+1}^{\lceil n\cdot t_i\rceil}\xi_j
		=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{k_n}\xi_{\lceil n\cdot t_{i-1}\rceil+j}
		\overset{\L}{=}
		\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j\\
		&=\underbrace{\sqrt{\frac{k_n}{n}}}_{=\sqrt{\frac{\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil}{n}}}\cdot\frac{1}{\sqrt{k_n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j\\
		&=\underbrace{\sqrt{\frac{\lceil n\cdot t_i\rceil-\lceil n\cdot t_{i-1}\rceil}{n}}}_{\stackrel{n\to\infty}{\longrightarrow}\sqrt{t_i-t_{i-1}}}\cdot\underbrace{\frac{1}{\sqrt{k_n}}\cdot\sum\limits_{j=1}^{k_n}\xi_j}_{\stackrelnew{\text{ZGWS}}{n\to\infty}{\longrightarrow}\mathcal{N}(0,1)}\stackrelnew{\ref{beisp4.18}(3)}{\L}{\longrightarrow}\sqrt{t_i-t_{i-1}}\cdot\mathcal{N}(0,1)
		\overset{\L}{=}\underbrace{\mathcal{N}(0,t_i-t_{i-1})}_{
		\overset{\ref{def7.12}}{=}B(t_i)-B(t_{i-1})}
	\end{align*}
	Damit ist Voraussetzung \eqref{eqSatz7.11Vor1} aus \ref{lemma7.14} gezeigt.\nl
	Wir zeigen \eqref{eqSatz7.11VorM} für $\gamma=4$ und $\alpha=2$, falls $\mu_4:=\E\big[\xi_1^4\big]<\infty$ (das ist eine stärkere Voraussetzung)\\
	Seien $s>t$ aus $I=[0,b]$. 
	Dann gilt:
	\begin{align}\label{eqProof7.16Plus}\tag{+}
		&X_n(s)-X_n(t)\\
		&=\frac{1}{\sqrt{n}}\cdot\sum\limits_{j=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_j+\frac{1}{\sqrt{n}}\cdot\big(n\cdot s-\lceil n\cdot s\rceil\big)\cdot\xi_{\lceil n\cdot s\rceil+1}
		-\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1}\nonumber
	\end{align}
	Da 
	\begin{align}\label{eqProof7.16PlusPlus}\tag{++}
		\lceil n\cdot t\rceil\leq n\cdot t\leq\lceil n\cdot t\rceil+1\qquad\forall t\geq0
	\end{align}
	folgt
	\begin{align*}
		\frac{k}{n}\leq t<\frac{k+1}{n},\qquad\frac{l}{n}\leq s<\frac{l+1}{n},\qquad k=\lceil n\cdot t\rceil,\qquad l=\lceil n\cdot s\rceil
	\end{align*}
	\underline{Fall 1: $s-t\leq\frac{1}{n}$}
	\begin{enumerate}[label=(\roman*)]
		\item $s$ und $t$ liegen im selben Intervall $\left[\frac{k}{n},\frac{k+1}{n}\right)$
		\item $s$ und $t$ liegen in zwei aufeinanderfolgenden Intervallen $t\in\left[\frac{k}{n},\frac{k\cdot n}{n}\right]$, $s\in\left[\frac{k+1}{n},\frac{k+2}{n}\right)$
	\end{enumerate}
%Ferger: Habe ich schon erwähnt, dass ich in Deutsch richtig schlecht war?
	\ul{Fall (i)}: $\lceil n\cdot s\rceil=\lceil n\cdot t\rceil$ und
	\begin{align*}
		X_n(s)-X_n(t)
		&=\frac{1}{\sqrt{n}}\cdot(n\cdot s-n\cdot t)\cdot\xi_{\lceil n\cdot t\rceil+1}
		=\sqrt{n}\cdot(s-t)\cdot\xi_{\lceil n\cdot t\rceil+1}\\
		\implies
		\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
		&=n^2\cdot(s-t)^4\cdot\mu_4
		=\mu_4\cdot(s-t)^2\cdot\underbrace{(s-t)^2}_{<\frac{1}{n^2}}\cdot n^2\\
		&\leq\mu_4\cdot(s-t)^2
	\end{align*}
%Ferger: Sie haben vielleicht mitbekommen es wird momentan von der Digitalisierung geredet. Und der Bund will 5 mrd € zur Verfügung stellen. Aber die Lehrer sagen: "Wir müssen die Kinder in das Zeitalter der Digitalisierung bringen". Gestern, ich lag gerade so auf meiner Couch mit einem Glas Wein und ein Journalist sagte dann "Man darf nicht zurück in die Kreidezeit (Lehrer an der Tafel)" Ich bin so kurz zusammengezuckt und habe mir gedacht: "Ach du scheiße, was machst du denn in Dresden..." Also es gibt tatsächlich Leite, die sich auch damit beschäftigen und sagen das ist Mühsam mit der Tafel. Aber indem man schreibt, beschäftigt man sich schon intensiver mit Stoff. 
%Ferger: Manchmal ist weniger mehr.
	\ul{Fall (ii):} $k=\lceil n\cdot t\rceil$ und $l=k+1=\lceil n\cdot s\rceil$. Dann folgt aus \eqref{eqProof7.16Plus}:
	\begin{align*}
		X_n(s)-X_n(t)
		&=\frac{1}{\sqrt{n}}\cdot\xi_{k+1}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot s-(k+1)\big)\cdot\xi_{k+2}-\frac{1}{\sqrt{n}}\cdot(n\cdot t-k)\cdot\xi_{k+1}\\
		&=\sqrt{n}\cdot\left(s-\frac{k+1}{n}\right)\cdot\xi_{k+2}+\sqrt{n}\cdot\xi_{k+1}\cdot\left(\frac{k+1}{n}-t\right)
	\end{align*}

	\begin{lem}[$c_r$-Ungleichung]\enter
		Seien $a_1,\ldots,a_m\in\R$ (paarweise verschieden) und $r\geq 1$. Dann gilt:
		\begin{align}\label{eqCrUngleichung}\tag{$C_r$}
			\left|\sum\limits_{i=1}^m a_i\right|^r\leq c_r\cdot\sum\limits_{i=1}^m|a_i|^r\mit c_r:= m^{r-1}
		\end{align}
	\end{lem}

	\begin{proof}
		Sei $Z$ diskrete Zufallsvariable mit $\P(Z=a_i)=\frac{1}{m}$ für $1\leq i\leq m$. Dann gilt:
		\begin{align*}
			\big|\underbrace{\E[Z]}_{=\frac{1}{n}\cdot\sum\limits_{i=1}^m a_i}\big|^r
			\overset{\text{Jensen}}&\leq
			\underbrace{\E\Big[|Z|^r\Big]}_{=\frac{1}{m}\cdot\sum\limits_{i=1}^m|a_i|^r}
			\implies
			m^{-r}\cdot\left|\sum\limits_{i=1}^m a_i\right|^r\leq\frac{1}{m}\cdot\sum\limits_{i=1}^m|a_i|^r
		\end{align*}
	\end{proof}

	Mit $m=2$ und $r=4$ folgt:
	\begin{align*}
		\big|X_n(s)-X_n(t)\big|^4
		&\leq 8\cdot\bigg(n^2\cdot\xi^4_{k+2}\cdot\Big(\underbrace{s-\frac{k+1}{n}}_{\leq s-t}\Big)^4+n^2\cdot\xi^4_{k+1}\cdot\Big(\underbrace{\frac{k+1}{n}-t}_{\leq s-t}\Big)^4\bigg)\\
		\implies
		\E\left[\big|X_n(s)-X_n(t)\big|^4\right]
		&\leq 16\cdot\mu_4\cdot \underbrace{n^2\cdot(s-t)^2}_{\overset{\text{Fall 1}}{\leq} 1}\cdot(s-t)^2
		\leq
		16\cdot\mu_4\cdot(s-t)^2
	\end{align*}

	\underline{Fall 2: $s-t\geq\frac{1}{n}$}\\
	Aus \eqref{eqProof7.16Plus} und \eqref{eqCrUngleichung} mit $r=4$ und $m=3$ folgt wegen $\big|\lceil n\cdot s-\lceil n\cdot s\rceil\big|\leq1$:
	\begin{align*}
		\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
		\overset{}&{\leq}
		27\cdot\left(\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_i\right|^4\right]+\underbrace{\frac{1}{n^2}}_{\leq(s-t)^2}\cdot\mu_4+\underbrace{\frac{1}{n^2}}_{\leq(s-t)^2}\cdot\mu_4\right)
	\end{align*}

	\begin{lem}[Momenten-Ungleichung]\enter
	Seien $\xi_1,\ldots,\xi_n$ i.i.d., zentriert mit $\mu_4:=\E[\xi_1^4]<\infty$ und $\mu_2:=\E[\xi_1^1]$. Dann gilt:
		\begin{align*}
			\E\left[\left|\sum\limits_{i=1}^n\xi_i\right|^4\right]=n\cdot\mu_4+3\cdot n\cdot(n-1)\cdot\mu_2^2
			\leq 4\cdot\mu_4\cdot n^2
		\end{align*}
	\end{lem}

	\begin{proof}
		Zeige zuerst das Gleichheitszeichen:
		\begin{align*}
			\E\left[\left|\sum\limits_{i=1}^n\xi_i\right|^4\right]
			&=\E\left[\left(\sum\limits_{i=1}^n\xi_i\right)^4\right]\\
			&=\E\left[\sum\limits_{1\leq i,j,k,l\leq n}\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_l\right]\\
			&=\sum\limits_{1\leq i,j,k,l\leq n}\underbrace{\E\Big[\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_l\Big]}_{=:\mu_{i,j,k,l}}
		\end{align*}
		Die Tupel $(i,j,k,l)\in\lbrace1,\ldots,n\rbrace^4$ mit mindestens drei verschiedenen Komponenten liefern $\mu_{i,j,k,l}=0$, 
		denn z. B. (verschiedene Buchstaben repräsentieren verschiedene Zahlen):
		\begin{align*}
			\mu_{i,j,k,j}
			&=\E\Big[\xi_i\cdot\xi_j\cdot\xi_k\cdot\xi_j\Big]
			=\E\Big[\xi_i\cdot\xi_j^2\cdot\xi_k\Big]=\underbrace{\E\big[\xi_i\big]}_{=0}\cdot\E\big[\xi_i^2\big]\cdot\E\big[\xi_k\big]
		\end{align*}
		Folglich reduziert sich die obige auf (!)
		\begin{align*}
			&\sum\limits_{i=1}^4\underbrace{\E\Big[\xi_i^4\Big]}_{=\mu_4}
			+6\cdot\sum\limits_{1\leq i\leq j\leq n}
			\underbrace{\E\Big[\xi_i^2\cdot\xi_j^2\Big]}_{=\mu_2^2}
			+\underbrace{\cdot\sum\limits_{1\leq i\leq j\leq n}\underbrace{\E\Big[\xi_i\Big]}_{=0}\cdot\E\Big[\xi_j^3\Big]+4\cdot\sum\limits_{1\leq i\leq j\leq n}\E\Big[\xi_i^3\Big]\cdot\underbrace{\E\Big[\xi_j\Big]}_{=0}}_{=0}\\
			&=n\cdot\mu_4+6\cdot\mu_2^2\cdot\underbrace{\begin{pmatrix}
			n\\ 2
			\end{pmatrix}}_{=\frac{n\cdot(n-1)}{2}}\\
			&=\underbrace{n}_{\leq n^2}\cdot\mu_4+3\dot n\cdot\underbrace{(n-1)}_{\leq n}\cdot\underbrace{\mu_2^2}_{=\big(\E[\xi_1^2]\big)^2\stackrel{\text{Jensen}}{\leq}\mu_4}\\
			&\leq 4\cdot n^2\cdot\mu_4
		\end{align*}
		Siehe auch \textit{Turkish Journal of Mathematics, Moment equalities via integer partitions} (2014) von Dietmar Ferger für mehr Hintergründe.
	\end{proof}

	Mit diesem Lemma folgt:
	\begin{align*}
		\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=\lceil n\cdot t\rceil+1}^{\lceil n\cdot s\rceil}\xi_i\right|^4\right]
		&=
		\frac{1}{n^2}\cdot\E\left[\left|\sum\limits_{i=1}^{\lceil n\cdot s\rceil-\lceil n\cdot t\rceil}\xi_{i+\lceil n\cdot t\rceil}\right|^4\right]\\
		&\leq
		\frac{1}{n^2}\cdot 4\cdot\mu_4\cdot\Big(\underbrace{\lceil n\cdot s\rceil-\overbrace{\lceil n\cdot t\rceil}^{>n\cdot t-1}}_{\leq\underbrace{ n\cdot s-n\cdot t}_{=n\cdot(s-t)}+\underbrace{1}_{\stackrel{\text{2. Fall}}{\leq}n\cdot(s-t)}\leq2\cdot n\cdot(s-t)}\Big)^2\\
		&\leq
		\frac{1}{n^2}\cdot 4\cdot\mu_4\cdot 4\cdot n^2\cdot(s-t)^2\\
		&=16\cdot\mu_4\cdot(s-t)^2\\
		\implies
		\E\Big[\big|X_n(s)-X_n(t)\big|^4\Big]
		&\leq 27\cdot18\cdot\mu_4\cdot(s-t)^2\qquad\forall s>t\in I\\
		&=\Big(F(s)-F(t)\Big)^2
	\end{align*}

	wobei $F(s):=\sqrt{27\cdot18\cdot\mu_4}\cdot s$. 
	Offenbar ist $F$ stetig und streng monoton wachsend auf $I=[0,b]$.
	Folglich ist \eqref{eqSatz7.11VorM} aus Satz \ref{satz7.11MomentenkriteriumVonKolmogoroff} erfüllt mit $\mu=4$ und $\alpha=2>1$.\nl
	Wir haben Donsker \ref{satz7.16Donsker} gezeigt, allerdings unter der \ul{stärkeren} Voraussetzung $\E\big[\xi_1^4]<\infty$. 
	Den allgemeinen Fall $\E[\xi_1^2]<\infty$ zeigt man mit der sogenannten \textit{Methode des Stutzens (truncation)}, 
	vergleiche Achim Klenke (2008) \textit{Wahrscheinlichkeitstheorie}
\end{proof}

%Ferger: Ich bin mit meinem Leben ja sehr zufrieden. [...] Das anstregendste als Professor ist das Tafelwischen. Ansonsten ist der Job leicht verdientes Geld. 

\setcounter{satz}{15}
\begin{bemerkungnr}\label{bemerkung7.16Einhalb} %7.16. Einhalb
%\begin{bemerkung}
	Unser Beweis lässt sich sofort übertragen auf Dreiecksschemata\\ 
	$\big\lbrace\xi_{i,n}:1\leq i\leq n,n\in\N\big\rbrace$ mit $\xi_{1,n},\ldots,\xi_{n,n}$ i.i.d. $\sim H$ (verteilt nach Verteilungsfunktion $H$), 
	wobei die Verteilungsfunktion $H$ nicht von $n$ abhängt und $\E\big[\xi_{i,n}\big]=0$ und $\E\big[\xi_{i,n}^4\big]<\infty$ und $\Var(\xi_{i,n})=1$. \nl
	Prokhorov (1956), \textit{Convergence of random processes and limit theorems in probability theory, Theory of Probability and its applications 1},
	Seite 157-214, zeigt, dass auch hier die Existenz zweiter Momente $\E\big[\xi_{i,n}^2\big]<\infty$ ausreicht.
%\end{bemerkung}
\end{bemerkungnr}

Seien $(\xi_i)_{i\geq1}$ i.i.d. $\sim F$ mit $\E\big[\xi_1\big]=\mu\in\R$ und $\sigma^2:=\Var(\xi_1)\in(0,\infty)$. 
Dann ist Donsker \ref{satz7.16Donsker} anwendbar auf die \textbf{standardisierten Zufallsvariablen}
\begin{align*}
	\tilde{\xi}_i:=\frac{\xi_i-\mu}{\sigma},\qquad\forall i\geq1
\end{align*}
Beachte: Die Grenzverteilung $W$ in Satz \ref{satz7.16Donsker} (also das Wiener-Maß) hängt \ul{nicht} von $F$ ab. 
Die Grenzverteilung ist also invariant unter $F$. Deshalb heißt Satz \ref{satz7.16Donsker} auch \textbf{Invarianzprinzip}. 
(Andere Formulierung: \textbf{Funktionaler Grenzwertsatz})\nl
Sei $h\colon C\big([0,b]\big)\to\R$ messbar und $W$-fast überall stetig. 
Dann gilt wegen Satz \ref{satz7.16Donsker} und \ref{satz4.10ContinuousMappingTheorem}:
\begin{align}\label{eqUnder7.16Eins}\tag{1}
	h(X_n)\stackrel{\L}{\longrightarrow} h(B)
\end{align}
Kennt man die Verteilung von $h(B)$ (=Funktional der Brownschen Bewegung, dazu existiert umfangreiche Literatur, z.B. Borodin und Salminen (2002),
\textit{Handbook of Brownian motion}), so auch die Grenzverteilung von $h(X_n)$. Dies macht man sich in der asymptotischen Statistik zunutze.
(siehe Beispiel später) Umgekehrt lässt sich oft die Grenzverteilung $h(W)$ für besonders einfache Verteilungfunktionen $F$ bestimmen.
\begin{align*}
	h\big(X_n\big)\stackrel{\L}{\longrightarrow}Z
	\overset{\eqref{eqUnder7.16Eins}~\&~\ref{lemma4.6Einhalb}}{\implies}
	h(B)=Z
\end{align*}
Damit ist der Satz \ref{satz7.16Donsker} von Donsker auch nützlich in der Wahrscheinlichkeitstheorie.

\subsection{Anwendung von Donsker in der Statistik}
\begin{notation}
	Sei $X$ eine reelle Zufallsvariable. Dann schreibe
	\begin{align*}
		X\sim(\mu,\sigma^2):\Longleftrightarrow\E[X]=\mu\qquad\text{und}\qquad\Var(X)=\sigma^2
	\end{align*}
\end{notation}

Wir betrachten das \underline{Change-Point-Problem:}
% mit diesem Problem hat Ferger seine Uni-Karriere gestartet
$X_{1,n},\ldots,X_{n,n},n\in\N$ unabhängig mit 
\begin{align*}
	\left\lbrace\begin{array}{cl}
		X_{i,n}\text{ i.i.d.}\sim(\mu,\sigma^2), &\falls 1\leq i\leq\tau_n\\
		X_{i,n}\text{ i.i.d.}\sim(\mu,\tau^2), &\falls \tau_n< i\leq n
	\end{array}\right.
\end{align*}
wobei $\tau_n\in\lbrace1,\ldots,n\rbrace$ \ul{unbekannt} (der sogenannte \textbf{Change-point\\ (moment of change)}) 

\begin{align*}
	\underbrace{X_{1,n},\ldots,X_{\tau_n,n}}_{\text{i.i.d.}\sim(\mu,\sigma^2)}\qquad\underbrace{X_{\tau_n+1,n},\ldots,X_{n,n}}_{\text{i.i.d.}\sim(\nu,\tau^2)}
\end{align*}
\underline{Annahmen:} $\mu,\sigma^2$ bekannt, $\nu,\tau^2$ unbekannt, $\nu>\mu$\nl
\underline{Ziel:} Finde Test für 
$H_0:\tau_n=n$, d.h. es hat kein Wechsel stattgefunden vs. $H_1:1\leq\tau_n<n$. Dazu betrachte
\begin{align*}
	S_k:=S_{k,n}&:=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^k\underbrace{\frac{X_{i,n}-\mu}{\sigma}}_{=:\xi_{i,n}}&\forall& 0\leq k\leq n\\
	\E[S_k]&=0\qquad	&\forall& 0\leq k\leq\tau_n
\end{align*}
da
\begin{align*}
	S_k&=S_{\tau_n}+\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=\tau_n+1}^k\frac{X_{i,n}-\mu}{\sigma}
\end{align*}
folgt
\begin{align*}
	\E[S_k]
	&=0+\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=\tau_n+1}^k\frac{\overbrace{\E[X_{i,n}]}^{=\nu}-\mu}{\sigma}
	=\frac{1}{\sqrt{n}}\cdot\big(k-\tau_n\big)\cdot\frac{\nu-\mu}{\sigma}\qquad\forall\tau_n<k\leq n
\end{align*}
%TODO Hier Plot von $(k,\E[S_k)]$ einfügen 
%für k\leq\tau_n ist es immer 0,
%für k>\tau_n wächst es monoton
%TODO Hier Plot von $(k,S_k)$ einfügen, unter H_1:
%für k\leq\tau_n ist es ein "rumzappeln" um die X-Achse
%für k>\tau_n gibt es dann einen "Drift" nach oben
%TODO Hier Plot von $(k,S_k)$ einfügen, unter H_0:
%hier kein Drift nach oben, nur herumzappeln um die x_Achse

Planversibler Test:
$H_0$ verwerfen $:\Longleftrightarrow T_n:=\max\limits_{0\leq k\leq n} S_k>k_\alpha$\nl
\underline{Ziel:} Bestimme kritischen Wert $k_\alpha$

\begin{lemma}\label{lemma7.17}
	Sei $\underline{y}=\big(y_0,y_1,\ldots,y_n\big)\in\R^{n+1}$ und $h(\ul{y})$ der Polygonzug durch die Punkte $\left(\frac{k}{n},y_k\right)_{0\leq k\leq n}$. 
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\max\limits_{0\leq k\leq n}y_k=\max\limits_{0\leq t\leq 1} h(\ul{y})(t)
		\end{aligned}$
		%TODO Hier  könnte man eine Skizze einfügen
		\item $\begin{aligned}
			\max\limits_{0\leq k\leq n}\big|y_k\big|=\max\limits_{0\leq t\leq 1} \big|h(\ul{y})(t)\big|
		\end{aligned}$
		\item $\begin{aligned}
			\ul{y},\ul{z}\in\R^{n+1}\implies h\big(\ul{y}+\ul{z}\big)=h\big(\ul{y}\big)+h\big(\ul{z}\big)
		\end{aligned}$
	\end{enumerate}
\end{lemma}

\begin{proof}
	\underline{Zeige (1):}
	Gemäß der Zwei-Punkte-Formel gilt:
	\begin{align}\label{eqProof7.17Stern}\tag{$\ast$}
		h\big(\ul{y}\big)(t)
		&=y_{k-1}+\big(n\cdot t-(k-1)\big)\cdot\big(y_k-y_{k-1}\big)
		\qquad\forall t\in\left[\frac{k-1}{n},\frac{k}{n}\right],\forall 1\leq k\leq n
	\end{align}
	Da
	\begin{align*}
		h\big(\ul{y}\big)\left(\frac{k}{n}\right)\overset{\text{Def}}{=}y_k\qquad\forall 0\leq k\leq n
	\end{align*}
	gilt in (1) und (2) in jedem Fall "$\leq$". Umgekehrt sei $t\in[0,1]$ beliebig. Dann:
	\begin{align*}
		\exists 1\leq k\leq n:t\in\left[\frac{k-1}{n},\frac{k}{n}\right]
	\end{align*}
	\underline{Fall 1:} $y_k\geq y_{k-1}$\\
	Dann ist $h\big(\ul{y}\big)$ monoton wachsend auf $\left[\frac{k-1}{n},\frac{k}{n}\right]$ und es gilt
	\begin{align*}
		h\big(\ul{y}\big)(t)\leq h\big(\ul{y}\big)\left(\frac{k}{n}\right)=y_k\leq\max\limits_{0\leq k\leq n} y_k
	\end{align*}
	\underline{Fall 2:} $y_k<y_{k-1}$\\
	Dann ist $h\big(\ul{y}\big)$ monoton fallend und es gilt
	\begin{align*}
		h\big(\ul{y}\big)(t)\leq h\big(\ul{y}\big)\left(\frac{k-1}{n}\right)=y_{k-1}\leq\max\limits_{0\leq k\leq n}y_k
	\end{align*}

	\underline{Zeige (2):} In Fall 1 gilt:
	\begin{align*}
		h\big(\ul{y}\big)(t)&\leq y_k\leq\big|y_k\big|\leq\max\limits_{0\leq k\leq n}\big|y_k\big|\\
		h\big(\ul{y}\big)(t)&\geq y_{k-1}\geq-\big|y_{k-1}\big|\geq-\max\limits_{0\leq k\leq n}\big|y_k\big|\\
		\implies
		\Big|h\big(\ul{y}\big)(t)\Big|&\leq\max\limits_{0\leq k\leq n}\big| y_k\big|
	\end{align*}
	Fall 2 analog. Somit folgt (2).\nl
	\underline{Zeige (3):}\\
	Folgt aus \eqref{eqProof7.17Stern}.
\end{proof}

Es folgt aus dem Lemma \ref{lemma7.17} (1) (mit $y_k=S_k$):
\begin{align*}
	T_n:&=\max\limits_{0\leq k\leq n} S_k
	\overset{\ref{lemma7.17}}=
	\max\limits_{0\leq t\leq 1}\underbrace{h\big(S_0,\ldots,S_n\big)}_{\stackrel{\text{Def}}{=}X_n=\text{ Partialsummenproz.}}(t)\\
	T_n
	&=\sup\limits_{0\leq t\leq 1} X_n(t)=M(X_n),\text{ wobei}\\
	M&:C\big([0,1]\big)\to\R,\qquad M(f):=\sup\limits_{0\leq t\leq 1} f(t),\qquad\forall C\big([0,1]\big)
\end{align*}
Da 
\begin{align*}
	\Big|M(f)-M(g)\Big|\overset{(!)}{\leq}\sup\limits_{0\leq t\leq 1}\Big|f(t)-g(t)\Big|=d(f,g)
\end{align*}
ist $M$ stetig auf $C\big([0,1]\big)$ folgt mit Donsker \ref{bemerkung7.16Einhalb} und dem CMT \ref{satz4.10ContinuousMappingTheorem}:
\begin{align*}
	T_n=M(X_n)\stackrel{\L}{\longrightarrow} M(B)=\sup\limits_{0\leq t\leq 1} B(t),~\text{\ul{falls} } H_0\text{ gilt}
\end{align*}
Es gilt (siehe z.B. Borodin + Salminen):
\begin{align*}
	\sup\limits_{0\leq t\leq 1}B(t)
	\overset{\L}&=
	\big|\mathcal{N}(0,1)\big|\quad\Big(=|Z|,~Z\sim\mathcal{N}(0,1)\Big)\\
	&\implies
	\underbrace{\P_{H_0}\big(T_n>x\big)}_{=1-\P\big(T_n\leq x\big)}\stackrel{n\to\infty}{\longrightarrow}\P\big(|Z|>x\big)
	=1-\P\big(|Z|\leq x\big)\qquad\forall x\in\R,
\end{align*}
da Verteilungsfunktion von $|Z|$ stetig auf ganz $\R$, denn:
\begin{align*}
	\P\big(|Z|>x\big)
	\stackrelnew{Z\stackeq{\L}-Z}{\text{Sym}}{=}
	2\cdot\big(1-\Phi(x)\big)
\end{align*}
Also folgt
\begin{align*}
	\P\big(T_n>k_\alpha\big)\stackrel{n\to\infty}{\longrightarrow}2\cdot\big(1-\Phi(k_\alpha)\big)\stackeq{!}\alpha\\
	2\cdot\big(1-\Phi(k_\alpha)\big)=\alpha
	\Longleftrightarrow 1-\frac{\alpha}{2}=\Phi(k_\alpha)\\
	\Longleftrightarrow k_\alpha=\Phi^{-1}\left(1-\frac{\alpha}{2}\right)=: u_{1-\frac{\alpha}{2}}
\end{align*}
Es folgt: Der Test $H_0$ verwerfen $:\Longleftrightarrow T_n> u_{1-\frac{\alpha}{2}}$\\ %\alpha/2-Quantil der Standardnormalverteilung
ist ein \textbf{asymptotischer Niveau-$\alpha$-Test für $H_0$}, d.h.
\begin{align*}
	\limn\P_{H_0}\big(H_0\text{ wird verworfen}\big)=\alpha\qquad\forall \alpha\in(0,1)
\end{align*}

Falls nur bekannt, dass $\nu\neq\mu$, so modifiziere obigen Test zu\\
$H_0$ verwerfen $:\Longleftrightarrow\hat{T}_n:=\max\limits_{0\leq k\leq n}\big|S_k\big|>c_\alpha$\\
Aus \ref{lemma7.17} (2) folgt:
\begin{align*}
	\hat{T}_n=\sup\limits_{0\leq t\leq 1}\big|X_n(t)\big|=\Vert X_n\Vert_\infty=:\hat{M}(X_n)
\end{align*}
Da $\hat{M}$ stetig auf $C\big([0,1]\big)$, folgt analog wie oben
\begin{align*}
	\hat{T}_n=\hat{M}(X_n)\stackrel{\L}{\longrightarrow}\hat{M}(B)=\sup\limits_{0\leq t\leq 1}\big|B(t)\big|
\end{align*}
Es gilt (vergleiche z.B. Shorak und Wellner (1986) \textit{Empirical Processes with applications to statistics}):
\begin{align*}
	H(x):&=\P\left(\sup\limits_{0\leq t\leq 1}\big|B(t)\big|\leq x\right)
	=\left\lbrace\begin{array}{cl}
		\frac{4}{\pi}\cdot\sum\limits_{k\in\N_0}\frac{(-1)^k}{2\cdot k+1}\cdot\exp\left(-\frac{(2\cdot k+1)^2\cdot\pi^2}{8\cdot x^2}\right), &\falls x>0\\
		0, &\falls x\leq 0
	\end{array}\right.
\end{align*}
Ferner ist $H$ stetig auf $\R$ und streng monoton wachsend auf $(0,\infty)$. Somit erhalten wir (analog wie oben)
\begin{align*}
	\limn\P_{H_0}\Big(H_0\text{ wird verworfen}\Big)=\alpha\qquad\forall\alpha\in(0,1)
\end{align*}
\ul{falls} man $c_\alpha:=H^{-1}(1-\alpha)$ wählt.\\
Natürlich ist die Annahme $\mu$ und $\sigma^2$ beide bekannt sehr restriktiv. 
Falls beide Parameter unbekannt, so ersetze sie durch Schätzer (sogenannte \textbf{Plug-in-Methode}).
\begin{align*}
	\overline{X}_n:=\frac{1}{n}\cdot\sum\limits_{i=1}^n X_i,\qquad 
	\hat{\sigma}_n^2:=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big(X_i-\overline{X}_n\big)^2
\end{align*}
Wir wissen
\begin{align*}
	\big(\overline{X}_n,\sigma_n^2\big)\stackrel{n\to\infty}{\longrightarrow}\big(\mu,\hat{\sigma}^2\big)\qquad\P_{H_0}\text{-f.s.}
\end{align*}
Beachte: Es gibt auch ein starkes Gesetz der großen Zahlen (SGGZ bzw. auf englisch SLLN) für Dreiecksschemata (arrays), 
siehe z.B. Chow und Teicher (1997) \textit{Probability Theory - Independence, Interchangeability, Martingales}\nl
Das Ersetzen von $\mu$ und $\sigma^2$ in der Statistik
\begin{align*}
	T_n=\max\limits_{0\leq k\leq n}\frac{1}{\sqrt{n}}\left|\sum\limits_{i=1}^k\frac{X_i-\mu}{\sigma}\right|
\end{align*}
liefert
\begin{align*}
	T_n^\ast
	:&=\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\frac{1}{\sqrt{n}}\cdot\left|\sum\limits_{i=1}^k \big(X_{i,n}-\overline{X}_n\big)\right|
	\mit \hat{\sigma}=\sqrt{\hat{\sigma}^2}
\end{align*}

\begin{satz}\label{satz7.18}
	Unter $H_0$ gilt:
	\begin{align*}
		T_n^\ast\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1}\big|B(t)-t\cdot B(1)\big|
	\end{align*}
\end{satz}

\begin{proof}
	\begin{align*}
		X_i-\overline{X}_n
		&=X_i-\frac{1}{n}\cdot\sum\limits_{j=1}^n X_j
		=\sigma\bigg(\underbrace{\frac{X_i-\mu}{\sigma}}_{=:Z_i}-\underbrace{\frac{1}{n}\cdot\sum\limits_{j=1}^n\frac{X_j-\mu}{\sigma}}_{=:\overline{Z}_n}\bigg)\\
		\implies 
		T_n^\ast&=\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left|\frac{1}{n\sqrt{n}}\cdot\sum\limits_{i=1}^k\big(Z_i-\overline{Z}_n\big)\right|\\
		&=\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left|S_k-\frac{k}{n}\cdot S_n\right|\\
		\overset{\ref{lemma7.17}(2)+(3)}&=
		\frac{\sigma}{\hat{\sigma}_n}\cdot\max\limits_{0\leq t\leq 1}\Big|\underbrace{X_n(t)-t\cdot X_n(1)}_{=:Y_n(t)}\Big|
	\end{align*}
	wobei $X_n$ der Partialsummenprozess zu $Z_1,\ldots,Z_n$ ist (= Polygonzug durch $\left(\frac{k}{n},S_k\right)$) und 
	$S_k:=\frac{1}{\sqrt{n}}\cdot\sum\limits_{i=1}^k Z_i$.\nl
	Betrachte die Abbildung
	\begin{align*}
		&h:C\big([0,1]\big)\to C\big([0,1]\big),\qquad f \mapsto h(f):[0,1]\to\R,\qquad t\mapsto h(f)(t):=f(t)-t\cdot f(1)\\
		&\implies Y_n=h(X_n)
	\end{align*}
	Wir zeigen nun, dass $h$ stetig ist.
	\begin{align*}
		\big|h(f)(t)-h(g)(t)\big|
		&=\Big|f(t)-t\cdot f(1)-\big(g(t)-t\cdot g(1)\big)\Big|\\
		&=\Big|f(t)-g(t)-t\cdot\big(f(1)-g(1)\big)\Big|\\
		\overset{\text{DU}}&\leq
		\big|f(t)-g(t)\big|+\underbrace{|t|}_{\leq 1}\cdot\big|f(1)-g(1)\big|\\
		&\leq
		2\cdot d(f,g)\qquad\forall t\in[0,1]\\
		\implies d\big(h(f),h(g)\big)&\leq 2\cdot d(f,g)\qquad\forall f,g\in C\big([0,1]\big)
	\end{align*}
	Also ist $h$ stetig auf $C\big([0,1]\big)$. Dann folgt aus Bemerkung  \ref{bemerkung7.16Einhalb} + CMT \ref{satz4.10ContinuousMappingTheorem}:
	\begin{align*}
		Y_n=h(X_n)\stackrel{\L}{\longrightarrow} h(B)=:B_0\text{ in }C\big([0,1]\big)
	\end{align*}
	wobei $B_0(t)\overset{\text{Def}}{=}B(t)-t\cdot B(1)$.
	\begin{align*}
		\max\limits_{0\leq t\leq 1}\big|Y_n(t)\big|=\hat{M}(Y_n)=\Vert Y_n\Vert_\infty
		\stackrelnew{\ref{satz4.10ContinuousMappingTheorem}}{\L}{\longrightarrow}
		\Vert B_0\Vert=\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|
	\end{align*}
	Da $\frac{\sigma}{\hat{\sigma}_n}\stackrel{\P}{\longrightarrow}1$, liefert \ref{beisp4.18}(3):
	\begin{align*}
		T_n^\ast\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|
	\end{align*}
\end{proof}

\begin{definition} %7.19
	Sei $B$ ein Brownsche Bewegung aud $[0,1]$. Der stochastische Prozess
	\begin{align*}
		B_0(t):=B(t)-t\cdot B(1)\qquad\forall t\in[0,1]
	\end{align*}
	heißt \textbf{Brownsche Brücke}.
\end{definition}

%TODO Hier könnte man eine Skizze einfügen
%B_0(0)=B_0(1)=0
%Zwischen 0 und 1 gezitterter Sinus
%Ferger: "Also über die Brücke würde ich jetzt nicht gehen. Kein Ahnung, warum man Brücke dazu sagt."

%T_n^\ast ist Supremumsnorm von Partialsummenprozess?+

Es gilt (vergleiche Shorak und Wellner 1986 \textit{empirical processes}, Seite 34)
\begin{align*}
	H_0(x)&:=\P\left(\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|\leq x\right)
	\overset{\text{Doob}}{=}
	1-\sum\limits_{k\geq 1}(-1)^{k+1}\cdot\exp\big(-2\cdot k^2\cdot x^2\big)&\forall x>0\\
	H_0(x)&~=0 &\forall x\leq 0
\end{align*}
$H_0$ ist stetig auf $\R$ uns streng monoton auf $[0,\infty)$. 
Folglich ist der Test
\begin{align*}
	H_0\text{ verwerfen }:\Longleftrightarrow T_n^\ast>H_0^{-1}(1-\alpha)
\end{align*}
ist ein asymptotischer Niveau-$\alpha$-Test.\\
$H_0$ ist die Verteilungsfunktion der \textbf{Kolmogorov-Smirnov-Verteilung}.\\
Falls $\mu,\sigma^2$ unbekannt, aber $\nu>\mu$, so betrachte 
\begin{align*}
	\tilde{T}_n&:=\frac{1}{\sqrt{n}}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k\leq n}\left(\sum\limits_{i=1}^kX_i-\overline{X}_n\right)
\end{align*}
Vollkommen analog folgt: $\tilde{T}_n\stackrel{\L}{\longrightarrow}\sup\limits_{0\leq t\leq 1} B_0(t)$ unter $H_0$. 
Es gilt (vergleiche z.B. Gänssler + Stute (1977) \textit{Wahrscheinlichkeitstheorie}, Seite 322)
\begin{align*}
	H_0^+(x)&:=\P\left(\sup\limits_{0\leq t\leq 1}B_0(t)\leq x\right)=1-\exp\left(-2\cdot x^2\right) &\forall x\geq0\\
	H_0^+(x)&:=0 &\forall x<0
\end{align*}
Da 
\begin{align*}
	(H_0^+)^{-1}(1-\alpha)&=\left(-\frac{1}{2}\cdot\log(\alpha)\right)^{\frac{1}{2}}
\end{align*}
folgt, dass der (einseitige) Test
\begin{align*}
	H_0\text{ verwerfen}:\Longleftrightarrow\tilde{T}_n>\left(-\frac{1}{2}\cdot\log(\alpha)\right)^{\frac{1}{2}}
\end{align*}
ein asymptotischer Niveau-$\alpha$-Test für $H_0$ ist.\nl
Anstelle von
\begin{align*}
	T_n^\ast=\frac{1}{\sqrt{n}}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{0\leq k \leq n}\bigg|\underbrace{\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)}_{=:C_k}\bigg|
\end{align*}
(die $C_k$ heißen \textbf{kumulierte Summe}) betrachten Csörgő und Horváth (1977) in \textit{Limit Theorems in Change-point Analysis} \ul{gewichtete} kumulierte Summen:
\begin{align*}
	U_n^\ast&:=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\max\limits_{1\leq k\leq n-1}\frac{|C_k|}{\sqrt{k\cdot(n-k)}}
\end{align*}

\begin{thm}[Csörgő und Horváth]\label{theoremCH} %noNumber
	Seien
	\begin{align*}
		A_n&:=\sqrt{2\cdot\log\big(\log(n)\big)}\\
		D_n&:=2\cdot\log\big(\log(n)\big)+\frac{1}{2}\cdot\log\Big(\log\big(\log(n)\big)\Big)-\frac{1}{2}\cdot\log(\pi)\qquad\forall n\geq n_0
	\end{align*}
	Dann gilt unter $H_0$:
	\begin{align}\label{eqTheoremCsorgoHorvath}\tag{1}
		\limn\P\Big(A_n\cdot U_n^\ast-D_n\leq t\Big)=\exp\big(-2\cdot\exp(-t)\big)=:G(t)
		\qquad\forall t\in\R
	\end{align}
	$G$ ist die sogenannte \textbf{Gumbel-Verteilung}.
\end{thm}

Damit Konstruktion eines asymptotischen Niveau-$\alpha$-Tests für $H_0$ wie folgt möglich: Sei
\begin{align*}
	t_\alpha&:=G^{-1}(1-\alpha)=-\log\left(-\frac{1}{2}\cdot\log(1-\alpha)\right)
\end{align*}
Klarerweise gilt:
\begin{align*}
	A_n\cdot U_n^\ast-D_n>t_\alpha\implies U_n^\ast>\frac{t_\alpha+D_n}{A_n}:=d_n(\alpha)
\end{align*}
Somit ist 
\begin{align*}
	H_0\text{ verwerfen}:\Longleftrightarrow U_n^\ast>d_n(\alpha)
\end{align*}
ein asymptotischer Niveau-$\alpha$-Test, d.h.
\begin{align}\label{eqUnderTheoremCsorgoHarcath}\tag{2}
	\limn\P_{H_0}\big(U_n^\ast>d_n(\alpha)\big)=\alpha
\end{align}
\textbf{Problem:} Die Konvergenz in \eqref{eqTheoremCsorgoHorvath} bzw. in \eqref{eqUnderTheoremCsorgoHarcath} ist sehr sehr sehr laaaaaangsaaaam. %genauso stand das an der Tafel
%Ferger: "[...] Grottenschlecht."
Andererseits führen die Gewichte in $\big(k\cdot(n-k)\big)^{-\frac{1}{2}}$ in $U_n^\ast$ zu einer verbesserten Sensitivität des \textbf{$U_n^\ast$-Test} unter der Alternativen, \underline{insbesondere}, falls $\tau_n$ nahe bei $1$ bzw. oder $n-1$ (am Rand) liegt.\nl
\textbf{Idee:} Weiterhin gewichten, aber durch die \ul{zufällige} Größe $\sqrt{\hat{k}_n\cdot(n-\hat{k}_n)}$ wobei 
\begin{align*}
	\hat{k}_n&:=\min\left\lbrace 1\leq k\leq n:\big|C_k\big|=\max\limits_{1\leq j\leq n-1}\big|C_j\big|\right\rbrace\\
	C_j&:=\sum\limits_{i=1}^j \big(X_i-\overline{X}_n\big)
\end{align*}
d.h. $\hat{k}_n$ ist die kleinste Maximalstelle von $|C_k|,k\in\lbrace1,\ldots,n-1\rbrace$. Erhalte:
\begin{align*}
	R_n=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\frac{\max\limits_{1\leq k\leq n-1}\left|\sum\limits_{i=1}^k\big)(X_i-\overline{X}_n\big)\right|}{\sqrt{\hat{k}_n\cdot(n-\hat{k}_n)}}
\end{align*}

\begin{thm}[Dietmar Ferger, 2018]\label{theoremFerger2018} %nonumber
	Sei $(X_i)_{i\in\N}$ eine Folge von i.i.d. Zufallsvariablen mit positiver endlicher Varianz. Dann gilt:
	\begin{align}\label{eqTheoremFerger2018}\tag{3}
		R_n&\stackrel{\L}{\longrightarrow} R
		\qquad\text{wobei}\qquad
		R=\frac{M}{\sqrt{T\cdot(1-T)}}\mit\\
		M&:=\sup\limits_{0\leq t\leq 1}\big|B_0(t)\big|\nonumber,\qquad
		T:=\argmax\limits_{0<t<1}\big|B_0(t)\big|,\qquad
		B_0=\text{ Brownsche Brücke}\nonumber
	\end{align}
\end{thm}

In diesem Theorem \ref{theoremFerger2018} wird die Verteilungsfunktion $H$ von $R$ explizit bestimmt ($H$ hat Reihendarstellung).
%Ferger: "Sie müssen immer eine Anwendung haben. Am besten Sie retten die Welt."
Falls $r_\alpha:=H^{-1}(1-\alpha)$, so liefert \eqref{eqTheoremFerger2018} für den Test
\begin{align*}
	H_0\text{ verwerfen}:\Longleftrightarrow R_n>r_\alpha
\end{align*}
dass gilt:
\begin{align*}
	\limn\P_{H_0}\big(R_n>r_\alpha\big)=\alpha
\end{align*}

\begin{bemerkung}
	\eqref{eqTheoremFerger2018} gilt für $X_{1,n},\ldots,X_{n,n}$ i.i.d. $\forall n\in\N$.\\
	In Simulationsstudien zeigt der $R_n$-Test eine deutlich bessere Güte als der $U_n^\ast$-Test von Csörgő und Horvath.\\
	Falls bekannt ist, dass $\nu>\mu$, so betrachte
	\begin{align*}
		R_n^+&:=\sqrt{n}\cdot\frac{1}{\hat{\sigma}_n}\cdot\frac{\max\limits_{1\leq k\leq n-1}\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)}{\sqrt{\hat{k}_n^+\cdot\big(n-\hat{k}_n^+\big)}}\\
		\hat{k}_n^+&:=\min\left\lbrace 1\leq k\leq n-1:C_k=\max\limits_{1\leq j\leq n-1}C_j\right\rbrace
	\end{align*}
\end{bemerkung}

Es gilt:
\begin{align*}
	R_n^+\stackrel{\L}{\longrightarrow} R^+=\frac{M^+}{\sqrt{T^+\cdot(1-T^+)}},\qquad
	M^+=\sup\limits B_0(t),\qquad
	T^*=\argmax\limits B_0(t)\\
	H^+(x):=\P\big(R^+\leq x\big)=2\cdot\Phi(x)-\sqrt{\frac{2}{\pi}}\cdot x\cdot\exp\left(-\frac{1}{2}\cdot x^2\right)-1\qquad x\geq0~(=0, x<0)
\end{align*}
Hierbei ist $\Phi$ die Verteilungsfunktion der Standardnormalverteilung. Ferner gilt:\\
Die Zufallsgrößen $R^+$ und $T^+$ sind stochastisch unabhängig!
\begin{align*}
	\hat{\tau}_n-\tau_n\stackrel{\L}{\longrightarrow}V=\\
	\hat{\tau}_n:=\argmax\limits_{1\leq k\leq n-1}\left|\sum\limits_{i=1}^k\big(X_i-\overline{X_n}\big)\right|
\end{align*}

\subsection{Verteilungskonvergenz in \texorpdfstring{$C(\R)$}{C(R)}}
Häufig hat man mit stochastischen Prozessen zu tun, deren Pfade in
\begin{align*}
	C(\R):=\Big\lbrace f:\R\to\R:f\text{ stetig}\Big\rbrace
\end{align*}
liegen (siehe Beispiel Median ganz am Anfang). Die bisherige Theorie für $C(I)$, $I$\\ \underline{kompaktes} Intervall deckt das nicht ab. Wir versehen $C(\R)$ mit der Metrik
\begin{align*}
	d(f,g)&:=\sum\limits_{j\geq 1} 2^{-j}\cdot\frac{d_j(f,g)}{1+d_j(f,g)} &\forall f,g\in C(\R)\\
	d_j(f,g)&:=\sup\limits_{-j\leq t\leq j}\Big|f(t)-g(t)\Big| &\forall f,g\in C(\R)
\end{align*}
Es zeigt sich, das $\big(C(\R),d\big)$ ein vollständiger separabler metrischer Raum ist. Ferner gilt:
\begin{align*}
	d(f_n,f)\stackrel{n\to\infty}{\longrightarrow}0
	\Longleftrightarrow \forall j\in\N: d_j(f_n,f)\stackrel{n\to\infty}{\longrightarrow}0
	\Longleftrightarrow\text{ glm. Konvergenz auf Kompakta}
\end{align*}
Seien $\pi_t:C(\R)\to R$ und $\pi_T:C(\R)\to\R^{|T|}$ die Projektionsabbildungen, d.h. z.B.
\begin{align*}
	\pi_T(f)=\big(f(t)\big)_{t\in T}=\Big(f(t_1),\ldots,f(t_k)\Big)\qquad
	T=\lbrace t_1,\ldots, t_k\rbrace
\end{align*}

\begin{theorem}\label{theorem7.20}
	\begin{align*}
		\B_d\big(C(\R)\big)&=\sigma\Big(\pi_t:t\in\R\Big)=\sigma\Big(\pi_T:T\subseteq\R\text{ endlich}\Big)
	\end{align*}
\end{theorem}

\begin{proof}
	Die Argumente im Beweis von Satz \ref{satz7.2} lassen sich problemlos übertragen.
\end{proof}

Ebenfalls analog beweist man:

\begin{satz}\label{satz7.21}
	 Sei $(\Omega,\A)$ messbarer Raum und $C:=C(\R)$ versehen mit $\B_d(D)$.\\
	 Dann gilt für eine Abbildung $X:\Omega\to C(\R):$
	 \begin{align*}
	 	X~\A\text{-}\B_d(C)\text{-messbar}
	 	\Longleftrightarrow\forall t\in\R:
	 	\pi_t\circ X~\A\text{-}\B(\R)\text{-messbar }
	 \end{align*}
\end{satz}

Konsequenz aus Satz \ref{satz7.21}: Jeder stetige stochastische Prozess indiziert nach $\R$ kann aufgefasst werden als Zufallsvariable in $(C,d)$.

\begin{satz}\label{satz7.22}
	Seien $X,Y$ Zufallsvariablen in $\big(C(\R),d\big)$. Dann gilt:
	\begin{align*}
		X\stackeq{\L}Y\Longleftrightarrow
		\Big(X(t_1),\ldots,X(t_k)\Big)\stackeq{\L}\Big(Y(t_1),\ldots,Y(t_k)\Big)
	\end{align*}
	für jede Wahl von Punkten $t_1<\ldots<t_k$ aus $\R$.
\end{satz}

\begin{proof}
	Siehe Theorem \ref{theorem7.20} + Maßeindeutigkeitssatz.
\end{proof}

Für $f\in C(\R)$ sei
\begin{align*}
	f^{(j)}:=f|_{[-j,j]},\qquad I_j:=[-j,j]\text{ d.h.}\\
	f^{(j)}:I_j\to\R,\qquad f^{(j)}(t):=f(t)\qquad\forall t\in I_j
\end{align*}

\begin{satz}\label{satz7.23}
	Seien $X,X_n,n\in\N$ Zufallsvariablen in $\big(C(\R),d\big)$. Dann gilt:
	\begin{align*}
		X_n\stackrel{\L}{\longrightarrow}X\text{ in }\big(C(\R),d\big)
		\Longleftrightarrow\forall j\in\N:
		X_n^{(j)}\stackrel{\L}{\longrightarrow} X^{(j)}\text{ in }\big(C(I_j),d_j\big)
	\end{align*}
\end{satz}

Satz \ref{satz7.23} geht auf Whitt (1970) zurück. Ein (relativ) einfacher Beweis findet sich in Kallenberg (1997), \textit{Foundations of modern probability}, Seite 260.

\begin{bemerkungnr}\label{bemerkung7.24}\
	\begin{enumerate}[label=(\arabic*)]
		\item Die Resultate in \ref{theorem7.20},\ref{satz7.21},\ref{satz7.22} und \ref{satz7.23} gelten analog für $C\big([0,\infty)\big)$ mit $I_j$ ersetzt durch $[0,j]$.
		\item Satz \ref{satz7.23} ermöglicht es insbesondere, auf die Konvergenz-Kriterien in \ref{satz7.9} und \ref{satz7.11MomentenkriteriumVonKolmogoroff} zurückzugreifen.
	\end{enumerate}
\end{bemerkungnr}

\begin{beispiel}\label{beispiel7.25}
	 Seien $(\xi_i)_{i\in\N}$ i.i.d. mit $\E[\xi_1]=0$, $\Var(\xi_1)=1$ und 
	 \begin{align*}
	 	S_k&:=\sum\limits_{j=1}^k\xi_j\qquad\forall k\in\N_0\\
	 	X_n(t)&:=\frac{1}{\sqrt{n}}\cdot S_{\lceil n\cdot t\rceil}+\frac{1}{\sqrt{n}}\cdot\big(n\cdot t-\lceil n\cdot t\rceil\big)\cdot\xi_{\lceil n\cdot t\rceil+1}\qquad\forall t\in[0,\infty)
	 \end{align*}
	 D.h. $X_n$ ist Polygonzug durch $\left(\frac{k}{n},\frac{1}{\sqrt{n}\cdot S_n}\right),k\in\N_0$.
	 %TODO Hier Skizze einfügen
	 Aus Satz \ref{satz7.16Donsker} folgt:
	 \begin{align*}
	 	&X_n^{(j)}\stackrel{\L}{\longrightarrow}B^{(j)}\qquad\forall j\in\N\\
	 	&\overset{\ref{bemerkung7.24}(1)+\ref{satz7.23}}&\Longleftrightarrow
	 	X_n\stackrel{\L}{\longleftarrow} B\text{ in }\Big(C\big([0,\infty)\big),d\Big)
	 \end{align*}
\end{beispiel}

\section{Argmin-Theoreme in \texorpdfstring{$C(\R)$}{C(R)}}
Erinnere an folgende Probleme, vergleiche  1.2 und 1.5 (Median)\\ %TODO
Wann überträgt sich die Konvergenz (f.s. oder in Verteilung) von stetigen stochastischen Prozessen auf deren Minimalstellen?

\begin{definition}\label{definition8.1}
	Sei $f\in C(\R)$.
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			A(f)=\argmin(f):=\left\lbrace t\in\R:f(t)=\inf\limits_{s\in\R} f(s)\right\rbrace
		\end{aligned}$ = Menge aller Minimalstellen von $f$
		\item $\tau\in A(f)$ heißt \textbf{wohl-separiert} 
		\begin{align*}
			:\Longleftrightarrow\inf\Big\lbrace f(t):|t-\tau|\geq\varepsilon\Big\rbrace>f(\tau)\qquad\forall 0<\varepsilon\in\Q
		\end{align*}
	\end{enumerate}
\end{definition}

\begin{bemerkungnr}\label{bemerkung8.2}\
	\begin{enumerate}[label=(\arabic*)]
		\item $A(f)\neq\emptyset$ ist natürlich nicht ausgeschlossen, aber in jedem Fall ist $A(f)$\\ \underline{abgeschlossen} in $\R$, denn:\\
		Sei $(t_n)_{n\in\N}\subseteq A(f)$ mit $t_n\stackrel{n\to\infty}{\longrightarrow}t.$ Dann gilt:
		\begin{align*}
			f(t)&=\limn\underbrace{f(t_n)}_{=\inf\limits_{s\in\R}f(s)}=\inf\limits_{s\in\R}f(s)
			\implies t\in A(f)
		\end{align*}
		\item $\tau$ wohl-separariert $\implies\tau$ eindeutig, denn:\\
		Sei $\tilde{\tau}\in A(f)$ und $\tilde{\tau}\neq\tau$. Also:
		\begin{align}\label{eqBemerkung8.2Stern}\tag{$\ast$}
			\exists 0&<\varepsilon\in\Q:\big|\tau-\tau|>\varepsilon\\\nonumber
			\inf\limits_{s\in\R}f(s)
			\overset{\tilde{\tau}\in A(f)}&=
			f(\tau)
			\overset{\eqref{eqBemerkung8.2Stern}}{\geq}
			\inf\Big\lbrace f(t):|t-\tau|\geq\varepsilon\Big\rbrace
			\overset{\tau\text{ wohl-sep}}{\geq}
			f(\tau)
			\overset{\tau\in A(f)}{=}
			\inf\limits_{s\in\R} f(s)
		\end{align}
		Dies ist ein Widerspruch!
	\end{enumerate}
\end{bemerkungnr}

%TODO 2x Skizze einfügen
%links: $\tau$ ist wohl-separiert
%rechts: $\tau$ eindeutig, aber nicht wohl-separiert

\begin{satz}\label{satz8.3}
	Seien $f,f_n,n\in\N$ aus $C(\R)$, $\tau_n\in A(f_n)\neq\emptyset~\forall n\geq N_0\in\N$ 
	(also $\tau_n$ ist eine Minimalstelle, ab einem gewissen Zeitpunkt $N_0$.)
	und $\tau\in A(f)$ sei wohl-separiert.
	Falls
	\begin{align}\label{eqSatz8.3_1}\tag{1}
		\big\Vert f_n-f\big\Vert\overset{\text{Def}}{=}\sup\limits_{t\in\R}\big|f_n(t)-f(t)\big|\overset{n\to\infty}{\longrightarrow}0
	\end{align}
	so folgt
	\begin{align*}
		\tau_n\overset{n\to\infty}{\longrightarrow}\tau
	\end{align*}
\end{satz}

\begin{proof}
	Sei $0<\varepsilon\overset{\text{oBdA}}{\in}\Q$. Es folgt:
	\begin{align*}
		s(\varepsilon)&:=\inf\limits\big\lbrace f(t):|t-\tau|\geq\varepsilon\big\rbrace
		\overset{\text{Def wohl-sep}}{>}
		f(\tau)\\
		\implies
		\delta&:=\delta(\varepsilon):=\frac{1}{3}\cdot\big(s(\varepsilon)-f(\tau)\big)>0\\
		\overset{\eqref{eqSatz8.3_1}}{\implies}
		\exists n_0&=n_0(\delta)\in\N:\forall n\geq n_0:
		\Vert f_n-f\Vert\leq\delta
	\end{align*}
	Für alle $n\geq\max\limits(N_0,n_0)$ gilt:\\
	Falls $t\in\R$ mit $|t-\tau|\geq\varepsilon$, so folgt:
	\begin{align*}
		f_n(t)-f_n(\tau)
		&=\underbrace{f_n(t)-f(t)}_{\geq\underbrace{-\underbrace{\Vert f_n-f\Vert}_{\leq\delta}}_{\geq-\delta}}+\underbrace{\underbrace{f(t)}_{\geq s(\varepsilon)}-f(\tau)}_{\geq s(\varepsilon)-f(\tau)=3\delta}+\underbrace{f(\tau)-f_n(\tau)}_{\geq-\delta}\\
		&\geq-\delta+3\cdot\delta-\delta\\
		&=\delta>0
	\end{align*}
	Es folgt
	\begin{align}\label{eqProofSatz8.3Stern}\tag{$\ast$}
		f_n(t)>f_n(\tau)\qquad\forall t\in\R\mit|t-\tau|\geq\varepsilon
	\end{align}
	Folglich gilt $|\tau_n-\tau|<\varepsilon$, denn sonst folgte aus \eqref{eqProofSatz8.3Stern}, dass
	$f_n(\tau_n)>f_n(\tau)$ im Widerspruch zu $\tau_n$ ist Minimalstelle von $f_n$.\\
	Somit gezeigt:
	\begin{align*}
		\forall\varepsilon\in\Q\cap(0,\infty):\exists m_0=m_0(\varepsilon):=\max\limits(N_0,n_0):\forall n\geq m_0:|t_n-\tau|<\varepsilon
	\end{align*}
\end{proof}

Satz 8.3 lässt sich problemlos von $\R$ auf offene Intervalle $I=(a,b)$ übertragen.\nl
Für kompakte Intervalle $I=[a,b]$, $a<b\in\R$ muss $\tau$ nur eindeutig sein.
Es gilt:

\begin{satz}\label{satz8.4}
	Sei also $I=[a,b]$ kompaktes Intervall und seien $f,f_n,n\in\N$ aus $C(I)$. 
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			A(f_n)\neq\emptyset
		\end{aligned}$
		\item Falls $\tau$ \underline{eindeutige} Minimalstelle von $f$ ist und falls
		\begin{align*}
			\Vert f_n-f\Vert_I:=\sup\limits_{t\in I}\big|f_n(t)-f(t)\big|\overset{n\to\infty}{\longrightarrow}0
		\end{align*}
		so gilt für \underline{jede} Auswahl $\tau_n\in A(f)$:
		$\tau_n\overset{n\to\infty}{\longrightarrow}\tau$
	\end{enumerate}
\end{satz}

%Ferger: "In irgendeinem der Seminarräumen ist mal eine Tafel von der Wand gefallen. Und ich möchte jetzt nicht unter so einer Tafel begraben liegen.
% Vielleicht ist das für einen Professor ein schöner Tod. Aber bitte jetzt noch nicht."
% "Ich bin ja von Natur aus etwas ängstlich. Es gab da mal so ein lustiges Lied und da singt der Sänger: "Leute seid nicht feige, lasst mich ...""

\begin{proof}
	\underline{Zeige (1):}\\
	Folgt, da jede stetige Funktion auf einem Kompaktum das Infimum annimmt.\nl
	\underline{Zeige (2):}\\
	$\tau$ ist wohl-separiert auf $I$, denn: 
	Angenommen, es wäre nicht so, also angenommen
	\begin{align*}
		\exists0<\varepsilon\in\Q:\inf\limits\big\lbrace f(t):t\in I:|t-\tau|\geq\varepsilon\big\rbrace=f(\tau)
	\end{align*}
	Die Menge $K_\varepsilon:=\lbrace t\in I:|t-\tau|\geq\varepsilon\rbrace$ ist kompakt.  
	Weil $f$ stetig ist, nimmt $f$ auf $K_\varepsilon$ ihr Infimum an, d.h.
	\begin{align*}
		\exists\sigma\in I:|\sigma-\tau|\geq\varepsilon\mit f(\sigma)=\inf\limits
		\big\lbrace f(t):t\in I:|t-\tau|\geq\varepsilon\big\rbrace
		=f(\tau)
	\end{align*}
	Also ist $\sigma$ eine \underline{weitere} Minimalstelle von $f$ (denn $\sigma$ und $\tau$ haben positiven Abstand zueinander) im Widerspruch zur Eindeutigkeit von $\tau$.\\
	Jetzt weiter wie im Beweis von Satz \ref{satz8.3}.
\end{proof}

Es ergeben sich nun mühelos Argmin-Theoreme für fast sichere Konvergenz:

\begin{satz}\label{satz8.5}
	Seien $M,M_n,n\in\N$ stochastische Prozesse über $(\Omega,\A,\P)$ mit Pfaden in $C(\R)$.
	Es gelte:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\tau\in A(M)
		\end{aligned}$ f.s. für eine Zufallsvariable $\tau$
		\item $\begin{aligned}
			\inf\limits\big\lbrace M(t):|t-\tau|\geq\varepsilon\big\rbrace>M(\tau)\text{ f.s.}\qquad\forall 0<\varepsilon\in\Q
		\end{aligned}$
		\item $\begin{aligned}
			\big\Vert M_n-M\big\Vert\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}
		\end{aligned}$
	\end{enumerate}
	Dann gilt für jede Folge $(\tau_n)_{n\in\N}$ von Zufallsvariablen mit $\tau_n\in A(M_n)$ f.s.:
	$\tau_n\overset{n\to\infty}{\longrightarrow}\tau$ f.s.
\end{satz}

\begin{proof}
	Setze
	\begin{align*}
		\Omega_0:=\underbrace{\Big\lbrace\omega\in\Omega:\tau(\omega)\in A\big(M(\cdot,\omega)\big)\Big\rbrace}_{=:\big\lbrace\tau\in A(M)\big\rbrace\text{ Einsmenge wg (1)}}
		&\cap\bigcap\limits_{0<\varepsilon\in\Q}
		\underbrace{\Big\lbrace\inf\limits\big\lbrace M(t):|t-\tau|\geq\varepsilon\big\rbrace>M(\tau)\Big\rbrace}_{\text{Einsmenge wg. (2)}}\\
		&\cap\underbrace{\Big\lbrace\Vert M_n-M\Vert\overset{n\to\infty}{\longrightarrow}0\Big\rbrace}_{\text{Einsmenge wg (3)}}
		\cap\bigcap\limits_{n\geq1}\underbrace{\Big\lbrace\tau_n\in A(M_n)\Big\rbrace}_{\text{Einsmenge nach Vor.}}
	\end{align*}
	Erinnerung: Abzählbare Schnitte von Einsmengen sind Einsmengen.\\
	Dann gilt $\P(\Omega_0)=1$. 
	Da $\Omega_0\overset{\ref{satz8.3}}{\subseteq}\big\lbrace\tau_n\overset{n\to\infty}{\longrightarrow}\tau\big\rbrace$, folgt die Behauptung.
\end{proof}

Analog erhält man mit Satz \ref{satz8.4}:

\begin{satz}\label{satz8.6}
	Seien $M,M_n,n\in\N$ stochastische Prozesse über $(\Omega,\A,\P)$ mit Pfaden in $C(I)$,
	wobei $I$ kompaktes Intervall ist.
	Es gelte:
	\begin{enumerate}[label=(\arabic*)]
		\item Es gibt eine Zufallsvariable $\tau$, die f.s. eindeutige Minimalstelle von $M$ ist.
		\item $\begin{aligned}
			\Vert M_n-M\Vert_I\overset{n\to\infty}{\longrightarrow}0
		\end{aligned}$ f.s.
	\end{enumerate}
	Dann gilt für jede Folge $(\tau_n)_{n\in\N}$ von Zufallsvariablen mit $\tau_n\in A(M_n)$ f.s.:
	$\tau_n\overset{n\to\infty}{\longrightarrow}\tau$ f.s.
\end{satz}

\begin{proof}
	Setze
	\begin{align*}
		 \Omega_0&:=\lbrace\tau\text{ ist eindeutige Minimalst. v. }M\rbrace
		 \cap\Big\lbrace\Vert M_n-M\Vert_I\overset{n\to\infty}{\longrightarrow}0\Big\rbrace
		 \cap\bigcap\limits_{n\geq1}\Big\lbrace\tau_n\in A(M_n)\Big\rbrace\\
		 &\implies\P(\Omega_0)=1
	\end{align*}		
	Da $\Omega_0\overset{\ref{satz8.4}}{\subseteq}\Big\lbrace\tau_n\overset{n\to\infty}{\longrightarrow}\tau\Big\rbrace$, folgt die Behauptung.
\end{proof}

\subsection{Anwedung in der Statistik} %nonumber
Maximum-Likelihood-Schätzung in 1-parametrigen Exponentialfamilien
Seien $(X_n)_{n\geq1}$ i.i.d. Kopien von Zufallsvariablen $X$, (d.h. $X_i\sim X$ bzw. $X_i\overset{\L}{=}X$)
mit Werten im Messraum $(\X,\F)$ und mit $\mu$-Dichte 
\begin{align*}
	f_\theta(x)&=c(\theta)\cdot h(x)\cdot\exp\big(q(\theta)\cdot T(\theta)\big)
	\qquad\forall x\in\X,\theta\in\Theta\subseteq\R
\end{align*}

% Ferger:
% In der Statistik werden Beobachtungen als Realisierungen von Zufallsvariablen aufgefasst, also X(\omega)=Beobachtung für ein \omega\in\Omega
% X_i kann das Befragungsergebnis des $i$-ten Probanden sein
% Kann man die $X_i$ in der Theorie als Unabhhängigkeit annehmen? -> siehe Zeitreihenanalyse
% Natürlich ist es nicht immer gegeben, unabhängige Daten zu betrachten. Mann muss sich in jeder Situation fragen, ob diese Modellierung sinnvoll ist.

% Ferger zu mir: "Schreiben Sie auch meine Späße mit?"
% Ich: "Ja, natürlich!"
% Ferger: "Das ist bedenklich..."

Erinnere an Maximum-Likelihood-Schätzer (MLS /MLE):
\begin{align*}
	\ul{X}_n:=\big(X_1,\ldots,X_n\big)
\end{align*}
hat \textbf{Likelihood-Funktion}
\begin{align*}
	L_n(\theta,\ul{X}_n)&:=\prod\limits_{i=1}^n L\big(\theta,X_i\big)
	\qquad\mit\qquad
	L(\theta,x):=f_\theta(x)
\end{align*}
Zugehörige \textbf{$\log$-Likelihood-Funktion} ist
\begin{align*}
	l_n\big(\theta,\ul{X}_n\big)&:=\log\Big(L_n\big(\theta,\ul{X}_n\big)\Big)
	=\sum\limits_{i=1}^n\log\big(\theta,X_i\big)
	=\sum\limits_{i=1}^n l\big(\theta,X_i\big)\qquad\mit
\\
l(\theta,x)&:=\log\big(L(\theta,x)\big)=\log\big(f_\theta(x)\big)
\end{align*}
Der MLS für $\theta$ ist definiert durch
\begin{align*}
	\hat{\theta}_n
	:=\argmax\limits_{\theta\in\Theta}L_n\big(\theta,\ul{X}_n\big)
	%\overset{x\mapsto\log(x)\text{ streng monoton wachsend}}{=}
	\overset{(\ast)}{=}
	\argmax\limits_{\theta\in\Theta} l_n\big(\theta,\ul{X}_n\big)
	=\argmin\limits_{\theta\in\Theta}\underbrace{-\frac{1}{n}\cdot  l_n\big(\theta,\ul{X}_n\big)}_{=:S_n(\theta)}
\end{align*}
Die $(\ast)$-Gleichheit gilt, weil $x\mapsto\log(x)$ streng monoton wachsend und stetig ist.
\begin{align*}
	l(\theta,x)&=\log\big(c(\theta)\big)+\log\big(h(x)\big)+q(\theta)\cdot T(x)\\
	\implies S_n(\theta)&=-\log\big(c(\theta)\big)\underbrace{-\frac{1}{n}\cdot\sum\limits_{i=1}^n\log\big(h(X_i)\big)}_{\text{hängt \ul{nicht} von $\theta$ ab}}-q(\theta)\cdot\underbrace{\frac{1}{n}\cdot\sum\limits_{i=1}^n T(X_i)}_{=:\overline{T}_n}\\
	&=\underbrace{-\Big(\log\big(c(\theta)\big)+q(\theta)\cdot\overline{T}_n\Big)}_{=:M_n(\theta)}-\frac{1}{n}\cdot\sum\limits_{i=1}^n\log\big(h(X_i)\big)\\
	\implies\hat{\theta}_n
	&=\argmin\limits_{\theta\in\Theta} M_n(\theta)
\end{align*}

% Ferger zu den Schülern von Uni-Live: "Ich mach' auch sonst keine Späße. Und die Studenten sind auch immer leise."
%Ferger: "Ich komme aus einem 400-Einwohner Dorf."

\textbf{Annahme:}
\begin{enumerate}[label=(\arabic*)]
	\item $\Theta\subseteq\R$ ist kompaktes Intervall
	\item $q$ ist stetig auf $\Theta$ ($\implies c$ stetig, $c>0$).
	\item Sei $\theta_0$ der \textbf{wahre} Parameter, d.h. $X_i\sim f_{\theta_0}$
\end{enumerate}

Ziel: Zeige
\begin{align*}
	\hat{\theta}_n\overset{n\to\infty}{\longrightarrow}\theta_0
	\quad\P\text{-fast sicher}
	\qquad\forall \theta_0\in\Theta
\end{align*}
Das ist die sogenannte \textbf{starke Konsistenz} der Schätzerfolge $(\hat{\theta}_n)_{n\in\N}$.

% Ferger zu den Schülern: "Liebe Kinder, bitte nicht rauchen! Rauchen ist ungesund."
%Ferger: "Ich rauche in meinem Zimmer. Das ist verboten. Da ist ziviler ungehorsam. Ich warte immer noch, dass jemand zu mir kommt und sagt, dass das verboten. Und jetzt? Werfen Sie mich jetzt raus?
% Ich habe auch einen Hund in meinem Zimmer. Ist auch verboten."

Gemäß SGGZ gilt:
\begin{align}\label{eqUnderStarkeKonsistenz}
	\overline{T}_n\overset{n\to\infty}{\longrightarrow}&\E_{\theta_0}\big[T(X)\big]\quad\P\text{-fast sicher}\\
	\E_{\theta_0}\big[T(X)\big]
	&=\int\limits_\Omega T(X)\d\P_{\theta_0}
	\overset{\eqref{eqTrafo}}{=}
	\int\limits_{\X}T(x)\underbrace{\P_{\theta0}\circ X^{-1}}_{\text{hat $\mu$-Dichte}}(\d x)	
	\int\limits_{\X}T(x)\cdot f_{\theta_0}(x)~\mu(\d x)\nonumber
\end{align}

Erinnerung: 
\begin{align*}
	X_i:\big(\Omega,\A,\P_\theta)\to\big(\X,\F)
	\qquad
	X_i(\omega)\in\X\overset{\text{z.B.}}{=}\R
	\qquad
	\underline{X}_n:=\big(X_1,\ldots,X_n\big):\Omega\to\X^n
\end{align*}

Aus \eqref{eqUnderStarkeKonsistenz} folgt:
\begin{align*}
	M_n(\theta)\overset{n\to\infty}&{\longrightarrow}
	\underbrace{-\Big(\log\big(c(\theta)\big)+q(\theta)\cdot\E_{\theta_0}\big[T(X)\big]\Big)}_{=:M(\theta)=:M_{\theta_0}(\theta)}
	\quad\P\text{-f.s.}
	\qquad\forall\theta_0\in\Theta\\
	\Big|M_n(\theta)-M_{\theta_0}(\theta)\Big|
	&=\bigg|q(\theta)\cdot\Big(\overline{T}_n-\E_{\theta_0}\big[T(X)\big]\Big)\bigg|\\
	\implies
	\sup\limits_{\theta\in\Theta}\Big|M_n(\theta)-M_{\theta_0}(\theta)\Big|
	&=\underbrace{\left(\sup\limits_{\theta\in\Theta}\big|q(\theta)\big|\right)}_{=:c<\infty}
	\cdot\underbrace{\bigg|\Big(\overline{T}_n-\E_{\theta_0}\big[T(X)\big]\Big)\bigg|}_{\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}}
\end{align*}
Die Bedingung (2) in Satz \ref{satz8.6} ist also erfüllt.

% Ferger hat am 18.01. Geburtstag

Falls $\theta_0$ eindeutige Minimalstelle von $M_{\theta_0}$ ist, so liefert Satz \ref{satz8.6} die starke Konsistenz von $(\hat{\theta}_n)_{n\in\N}$.
Zusammenfassung in:

\begin{satz}\label{satz8.7}
	 Im Modell (Exponential-Familie) sei $\Theta\subseteq\R$ kompaktes Intervall,
	 $q$ sei stetig und der wahre Parameter $\theta_0$ sei eindeutige Minimalstelle der Funktion
	 \begin{align*}
	 	M_{\theta_0}(\theta)
	 	&=-\log\big(c(\theta)\big)-q(\theta)\cdot c(\theta)\cdot\int\limits T(x)\cdot h(x)\cdot\exp\big(q(\theta_0)\cdot T(x)\big)~\mu(\d x)
	 	\qquad\forall\theta\in\Theta
	 \end{align*}
	 (Das ist eine implizite Forderung an die Verteilungsannahmen (VA).)
	 Und sei 
	 \begin{align*}
	 	\hat{\theta}_n=\argmin\limits_{\theta\in\Theta} M_n(\theta)
	 	\qquad\mit\qquad
	 	M_n(\theta)=-\log\big(c(\theta)\big)-q(\theta)\cdot\frac{1}{n}\cdot\sum\limits_{i=1}^n T(X_i)
	 \end{align*}
	 Dann gilt:
	 \begin{align*}
	 	\hat{\theta}_n\overset{n\to\infty}{\longrightarrow}
	 	\theta_0\quad\P\text{-f.s.}\qquad\forall\theta_0\in\Theta
	 \end{align*}
\end{satz}

\begin{proof}
	Folgt aus Satz \ref{satz8.6}.
\end{proof}

%Ferger: "Stellen wir uns mal konkret einen Hilbertraum vor."

Vom theoretischen Standpunkt aus ist die Kompaktheitsannahme an $\Theta$ sehr stark.
Allerdings ist dies für den Anwender keine signifikante Einschränkung.
Dafür aber neben der Stetigkeit von $q$ \underline{keine} weitere "Glattheitsannahmen".
Sind solche Glattheitsannahmen aber gegeben, so lässt sich zeigen, dass $\theta_0$ tatsächlich eindeutiger (und im Fall $\Theta=\R$ wohl-separierte) Minimalstelle von $M_{\theta_0}$ ist. 

\subsection{Anwendung in der Change-Point-Analyse} %8.2
Betrachten des Change-Point-Modell aus Kapitel 7.1:
$X_{1,n},\ldots,X_{n,n},n\in\N$ seien unabhängige reelle Zufallsvariablen mit 
\begin{align*}
	\left\lbrace\begin{array}{ll}
		X_{i,n}\text{ i.i.d }\sim(\mu,\sigma^2), &\falls 1\leq i\leq\tau_n\\
		X_{i,n}\text{ i.i.d }\sim(\nu,\tau^2), &\falls \tau_n< i\leq\tau_n\\
	\end{array}\right.,\qquad\mu\neq\nu
\end{align*}
Mit $\tau_n\in\big\lbrace1,\ldots,n-1\big\rbrace$ unbekannter \textbf{Wechselzeitpunkt}.
\begin{align*}
	\underbrace{X_1,\ldots,X_{\tau_n}}_{\text{pre-change variables}},\underbrace{X_{\tau_n+1},\ldots,X_n}_{\text{post-change variables}}
\end{align*}

\underline{Ziel:} Schätzung von $\tau$\\
Schreibe im Folgenden: $X_i\equiv X_{i,n}$
\begin{align*}
	S_k&:=\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big) &\forall& 0\leq k\leq n\\
	\overline{X}_n&:=\frac{1}{n}\cdot\sum\limits_{i=1}^n X_i &\forall& n\in\N\\
	\theta_n&:=\frac{\tau_n}{n} &\forall& n\in\N
\end{align*}

Eine elementare Rechnung zeigt (zur Übung):
\begin{align}\label{eqUnder8.2_1}\tag{1}
	\E[S_k]
	&=\left\lbrace\begin{array}{cll}
		k\cdot(1-\theta_n)\cdot(\mu-\nu), &\falls 0\leq k\leq\tau_n &\text{monoton wachsend}\\
		(n-k)\cdot\theta_n\cdot(\mu-\nu), &\falls\tau_n<k\leq n &\text{monoton fallend}
	\end{array}\right.\\\nonumber
	\big|\E[S_k]\big|
	&=\left\lbrace\begin{array}{cll}
		k\cdot(1-\theta_n)\cdot|\mu-\nu|, &\falls 0\leq k\leq\tau_n &\text{monoton wachsend}\\
		(n-k)\cdot\theta_n\cdot|\mu-\nu|, &\falls\tau_n<k\leq n &\text{monoton fallend}
	\end{array}\right.\\
	&\implies\tau_n=\argmax\limits_{0\leq k\leq n}\big|\E[S_k]\big|\nonumber
\end{align}
%TODO Hier könnte man eine Skizze einfügen
% monoton linear wachsend bis zu einem punkt, dann fallend, sieht aus wie ein Dreieck

Dies motiviert folgenden Schätzer (ersetze unbekannte $\big|\E[S_k)\big]$ durch bekannte $|S_k|$):
\begin{align*}
	\hat{\tau}_n:=\argmax\limits_{0\leq k\leq n}\big|S_k\big|
\end{align*}
Im Folgenden sei $\tau_n=\lfloor n\cdot\theta\rfloor,\theta\in(0,1)$.
Wir zeigen:
\begin{align*}
	\hat{\theta}_n:=\frac{\hat{\tau}_n}{n}\overset{n\to\infty}{\longrightarrow}\theta\qquad\P\text{-f.s.}
\end{align*}

\textbf{Annahme:} Wir nehmen die sogenannte \textbf{Momentenbedingung an}, d.h.
\begin{align*}
	\mu_p:=\E\Big[\big|X_{\tau_n}\big|^p\Big]<\infty,
	\qquad
	\nu_p:=\E\Big[\big|X_{\tau_n+1}\big|^p\Big]<\infty
\end{align*}

Sei $M_n:=$ Polygonzug durch $\left(\frac{k}{n},\frac{1}{n}\cdot S_k\right)$, $0\leq k\leq n$.
Dann folgt aus Lemma \ref{lemma7.17}:
\begin{align*}
	\hat{\theta}_n&:=\argmax\limits_{0\leq t\leq 1}\big|M_n(t)\big|\\
	M_n(t)&~=\frac{1}{n}\cdot S_{\lfloor n\cdot t\rfloor}+\big(n\cdot t-\lfloor w\cdot t\rfloor\big)\cdot\Big(S_{\lfloor n\cdot t\rfloor+1}-S_{\lfloor n\cdot t\rfloor+1}\Big) &\forall 0\leq t\leq 1\\
	\E\big[M_n(t)\big]&~=\frac{1}{n}\cdot\E\Big[ S_{\lfloor n\cdot t\rfloor}\Big]+\big(n\cdot t-\lfloor w\cdot t\rfloor\big)\cdot
	\Big(\E\Big[S_{\lfloor n\cdot t\rfloor+1}\Big]-\E\Big[S_{\lfloor n\cdot t\rfloor+1}\Big]\Big) &\forall 0\leq t\leq 1\\
\end{align*}

Sei $\overline{M}_n(t):=\E\big[M_n(t)\big),0\leq t\leq 1$. 
Dann ist $\overline{M}_n$ ein Polygonzug durch die Punkte $\left(\frac{k}{n},\frac{1}{n}\cdot\E[S_k]\right)$ $0\leq k\leq n$.
Da wegen \eqref{eqUnder8.2_1} 
\begin{align*}
	\overline{M}_n\left(\frac{k}{n}\right)
	&=\frac{1}{n}\cdot\E\big[S_k\big]
	\overset{\eqref{eqUnder8.2_1}}&=
	\left\lbrace\begin{array}{cl}
		\frac{k}{n}\cdot(1-\theta_n)\cdot(\mu-\nu) &\falls \frac{k}{n}\leq\frac{\tau_n}{n}=\theta_n\\
		\left(1-\frac{k}{n}\right)\cdot\theta_n\cdot(\mu-\nu), & \falls \frac{k}{n}>\theta_n
	\end{array}\right.
\end{align*}
folgt
\begin{align*}
	\overline{M}_n\left(t\right)
	&=\left\lbrace\begin{array}{cl}
		t\cdot(1-\theta_n)\cdot(\mu-\nu) &\falls t\leq\theta_n\\
		\left(1-t\right)\cdot\theta_n\cdot(\mu-\nu), & \falls \frac{k}{n}>\theta_n
	\end{array}\right.
\end{align*}
Beachte $\theta-\frac{1}{n}<\theta_n=\frac{\lfloor n\cdot\theta\rfloor}{n}=\theta$.
Eine Fallunterscheidung ($t\leq\theta_n$; $\theta_n<t\leq\theta$; $t>\theta$) liefert
\begin{align}\label{eqUnder8.2_2}\tag{2}
	\big\Vert\overline{M}_n-M\big\Vert
	&=\sup\limits_{0\leq t\leq 1}\Big|\overline{M}_n(t)-M(t)\Big|
	\leq 2\cdot|\mu-\nu|\cdot n^{-1}\overset{n\to\infty}{\longrightarrow}0\text{ wobei}\\\nonumber
	M(t)&=\left\lbrace\begin{array}{cl}
		t\cdot(1-\theta)\cdot(\mu-\nu) ,&\falls 0\leq t\leq\theta\\
		(1-t)\cdot\theta\cdot(\mu-\nu), &\falls \theta<t\leq 1
	\end{array}\right.\\\nonumber
	|M(t)|&=\left\lbrace\begin{array}{cl}
		t\cdot(1-\theta)\cdot|\mu-\nu| ,&\falls 0\leq t\leq\theta\\
		(1-t)\cdot\theta\cdot|\mu-\nu|, &\falls \theta<t\leq 1
	\end{array}\right.
\end{align}

%TODO Evtl. Skizze einfügen, plot(\overline{M}) auf [0,1]
% Wieder so ein Zelt mit Maximum in \theta

Somit ist dieses $\theta$ charakterisierbar als Maximalstelle:
\begin{align*}
	\theta=\argmax\limits_{0\leq t\leq 1}\big|M(t)\big|
\end{align*}

Zeige:
\begin{align*}
	\big\Vert M_n-M\big\Vert
	&=\sup\limits_{0\leq t\leq 1}\big|M_n(t)-M(t)\big|\overset{n\to\infty}{\longrightarrow}0\quad\P\text{-f.s.}
\end{align*}
Da
\begin{align}\label{eqUnder8.2_3}\tag{3}
	0\leq\big\Vert M_n-M\big\Vert\leq\big\Vert M_n-\overline{M}_n\big\Vert+\underbrace{\overline{M}_n-M\big\Vert}_{\stackrelnew{\eqref{eqUnder8.2_2}}{n\to\infty}{\longrightarrow}0}
\end{align}

Betrachte den ersten Summanden
\begin{align}\label{eqUnder8.2_4}\tag{4}
	\big\Vert M_n-\overline{M}_n\big\Vert
	=\sup\limits_{0\leq t\leq 1}&\big|M_n(t)-\overline{M}_n(t)\big|
	\overset{\ref{lemma7.17}}{=}
	\max\limits_{0\leq k\leq n}\bigg|\underbrace{M_n\left(\frac{k}{n}\right)}_{=\frac{1}{n}\cdot S_k}-\underbrace{\overline{M}_n\left(\frac{k}{n}\right)}_{=\frac{1}{n}\cdot\E[S_k]}\bigg|\\\nonumber
	\frac{1}{n}\cdot S_k
	&=\frac{1}{n}\cdot\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)
	=\frac{1}{n}\cdot\sum\limits_{i=1}^k X_i-\frac{k}{n^2}\cdot\sum\limits_{i=1}^n X_i\\\nonumber
	\implies
	M_n\left(\frac{k}{n}\right)-\overline{M}_n\left(\frac{k}{n}\right)
	\overset{\text{Lin}}&=
	\frac{1}{n}\cdot\sum\limits_{i=1}^n\underbrace{\Big(X_i-\E\big[X_i\big]\Big)}_{=:Z_i}-\frac{k}{n^2}\cdot\sum\limits_{i=1}^n\underbrace{\Big(X_i-\E\big[X_i\big]\Big)}_{=Z_i}\\\nonumber
	\implies
	\left|M_n\left(\frac{k}{n}\right)-\overline{M}_n\left(\frac{k}{n}\right)\right|
	\overset{\text{DU}}&\leq
	\frac{1}{n}\cdot\underbrace{\bigg|\sum\limits_{i=1}^n\underbrace{\Big(X_i-\E\big[X_i\big]\Big)}_{=:Z_i}\bigg|}_{
		\leq\max\limits_{0\leq k\leq n}\left|\sum\limits_{i=1}^k Z_i\right|
	}+\underbrace{\frac{k}{n^2}}_{\leq\frac{1}{n}}\underbrace{\cdot\bigg|\sum\limits_{i=1}^n\underbrace{\Big(X_i-\E\big[X_i\big]\Big)}_{=Z_i}\bigg|}_{
		\leq\max\limits_{0\leq k\leq n}\left|\sum\limits_{i=1}^k Z_i\right|
	}\\\nonumber
	&\leq2\cdot\frac{1}{n}\cdot\max\limits_{0\leq k\leq n}\left|\sum\limits_{i=1}^k Z_i\right|\\
	\implies\label{eqUnder8.2_5}\tag{5}
	&\max\limits_{0\leq k\leq n}\left|M_n\left(\frac{k}{n}\right)-\overline{M}_n\left(\frac{k}{n}\right)\right|
	\leq 2\cdot\frac{1}{n}\cdot\max\limits_{0\leq k\leq n}\left|\sum\limits_{i=1}^k Z_i\right|
\end{align}

Es ist
\begin{align*}
	T_k=\sum\limits_{i=1}^k Z_i\qquad 0\leq k\leq n
\end{align*}
ein Martingal bzgl. Filtration 
\begin{align*}
	\F_k:=\sigma\big(Z_1,\ldots,Z_k\big) &\qquad\forall 1\leq k\leq n,\qquad \F_0:=\lbrace\emptyset,\Omega\rbrace
\end{align*}
Ist Martingal, weil
\begin{align*}
	\E\Big[T_{k+1}~\Big|~\F_k\Big]
	&=\E\left.\left[\sum\limits_{i=1}^k Z_i+Z_{k+1}~\right|~\F_k\right]
	=\underbrace{\E\bigg[\overbrace{\sum\limits_{i=1}^k Z_i}^{\text{ist $\F_K$-messbar}}~\bigg|~\F_k\bigg]}_{=T_k}+\underbrace{\E\Big[ Z_{k+1}~\Big|~\F_k\Big]}_{=\E[Z_{k+1}]=0}
\end{align*}

Es folgt aus der bedingten Jensenungleichung, dass $\left(|T_k|^p,\F_k\right)_{0\leq k\leq n}$ ein nicht-negatives Submartingal ist.
\begin{align}\nonumber
	\P\Big(\big\Vert M_n-\overline{M}_n\big\Vert\geq\varepsilon\Big)
	\overset{\eqref{eqUnder8.2_4}+\eqref{eqUnder8.2_5}}&{\leq}
	2\cdot\frac{1}{n}\cdot\max\limits_{0\leq k\leq n}\big|T_k\big|\\\nonumber
	&\leq\P\left(\max\limits_{0\leq k\leq n}\big|T_k\big|\geq\frac{1}{2}\cdot n\cdot\varepsilon\right)\\\nonumber
	\overset{u\mapsto u^p\uparrow}&\leq
	\P\left(\left(\max\limits_{0\leq k\leq n}\big|T_k\big|\right)^p\geq\left(\frac{1}{2}\cdot n\cdot\varepsilon\right)^p\right)\\\nonumber
	&=\P\left(\max\limits_{0\leq k\leq n}\big|T_k\big|^p\geq\left(\frac{1}{2}\cdot n\cdot\varepsilon\right)^p\right)\\\nonumber
	\implies
	\P\Big(\big\Vert M_n-\overline{M}_n\big\Vert\geq\varepsilon\Big)
	&\leq\P\bigg(\max\limits_{0\leq k\leq n}\underbrace{\big|T_k\big|^p}_{\text{Submartingal}}\geq\underbrace{2^{-p}\cdot n^p\cdot\varepsilon^p}_{=:y>0}\bigg)\\\nonumber
	\overset{\text{Doob-Ungl}}&\leq
	y^{-1}\cdot\E\left[\big|T_n\big|^p\right]\\
	&=2^p\cdot n^{-p}\cdot\varepsilon^{-p}\cdot\E\left[\left|\sum\limits_{i=1}^n Z_i\right|^p\right]\label{eqUnder8.2_6}\tag{6}
\end{align}

Es gilt folgende \textbf{Momenten-Ungleichung} (vergleiche (1.6) in Ferger (2014), \textit{Optimal constants in the Marcinkiewicz-Zygmund-inequalities, statistics and propability letters} Ausgabe 84, Seiten 96 bis 101)

\begin{align*}
	\E\left[\left|\sum\limits_{i=1}^n Z_i\right|^p\right]
	\overset{(1.6)}{\leq}
	C_p\cdot n^{\frac{p}{2}}\cdot\sum\limits_{i=1}^n\E\Big[|Z_i|^p\Big]
\end{align*}

Erinnerung:
\begin{align*}
	Z_i&=X_i-\E[X_i]\\
	\implies
	|Z_i|^p&=\big|X_i-\E[X_i]\big|^p\\
	\implies
	\big|X_i-\E[X_i]\big|^p
	\overset{C_r\text{ Ungl}}&\leq
	c_p\cdot\Big(|X_i|^p+\underbrace{\big|\E[X_i]\big|^p}_{\overset{\text{Jensen}}{\leq}\E\big[|X_i|^p\big]}\Big)\\
	\implies
	\E\Big[|Z_i|^p\Big]
	&\leq c_p\cdot\left(\E\big[|X_i|^p\big]+\E\big[|X_i|^p\big]\right)\\
	&\leq 2\cdot c_p\cdot\underbrace{\E\Big[|X_i|^p\Big]}_{\leq\max\lbrace\mu_p,\nu_p\rbrace}
\end{align*}
Somit gibt es eine Konstante $D_p$ so, dass
\begin{align*}
	\E\left[\left|\sum\limits_{i=1}^n Z_i\right|^p\right]
	\overset{(1.6)}&{\leq}
	C_p\cdot n^{-\frac{p}{2}}\cdot\sum\limits_{i=1}^n\underbrace{\E\Big[|Z_i|^p\Big]}_{\leq D_p}\\
	\implies
	\E\left[\left|\sum\limits_{i=1}^n Z_i\right|\right]
	&\leq\tilde{c}_p\cdot n^{\frac{p}{2}}\\
	\overset{\eqref{eqUnder8.2_6}}{\implies}
	\P\Big(\big\Vert M_m-\overline{M}_n\big\Vert\geq\varepsilon\Big)
	&\leq\overline{c}_p\cdot\varepsilon^{-p}\qquad\forall\varepsilon>0
\end{align*}

Nun bilden wir die Reihe dieser Wahrscheinlichkeiten und erhalten Konvergenz für $p>2$:
\begin{align*}
	\overset{p>2}{\implies}
	\sum\limits_{n=1}^\infty\P\Big(\big\Vert M_m-\overline{M}_n\big\Vert\geq\varepsilon\Big)<\infty\qquad\forall\varepsilon>0
\end{align*}

Es gilt ganz allgemein für eine Folge $(\xi_n)_{n\in\N}$ von Zufallsvariablen (folgt aus dem Borel-Cantelli-Lemma):
\begin{align*}
	\left(\forall\varepsilon>0:\sum\limits_{n=1}^\infty\P\Big(|\xi_n|\geq\varepsilon\Big)<\infty\right)\implies\xi_n\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}
\end{align*}

Also erhalten wir
\begin{align*}
	\big\Vert M_n-\overline{M}_n\big\Vert\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}\\
	\overset{\eqref{eqUnder8.2_2}+\eqref{eqUnder8.2_3}}{\implies}
	\big\Vert M_n-M\big\Vert\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}
\end{align*}

Da
\begin{align*}
	\Big\Vert\big|M_n\big|-\big|M\big|\Big\Vert\leq\big\Vert M_n-M\big\Vert
\end{align*}
folgt
\begin{align*}
	\Big\Vert\big|M_n\big|-\big|M\big|\Big\Vert\overset{n\to\infty}&{\longrightarrow}0\text{ f.s.}\\
	\implies	
	\Big\Vert-\big|M_n\big|-\big(-\big|M\big|)\Big\Vert\overset{n\to\infty}&{\longrightarrow}0\text{ f.s.}\\
	\implies
	\hat{\theta}_n
	&=\argmax\limits_{0\leq t\leq 1}\big|M_n(t)\big|
	=\argmin\limits_{0\leq t\leq 1}-\big|M_n(t)\big|
\end{align*}
Also hat $-|M|$ eindeutige Minimalstelle $\theta$.
Aus Satz \ref{satz8.6} folgt nun $\hat{\theta}\overset{n\to\infty}{\longrightarrow}\theta$ f.s.\\
(In Satz \ref{satz8.6} verwende: $M_n\hat{=}-|M_n|$, $M\hat{=}|M|$, $\tau\hat{=}\theta$)

\begin{bemerkung}
	Betrachte eine endliche Beobachtungsfolge:\\
	$X_1,X_2,\ldots,X_\tau,X_{\tau+1},X_{\tau+},\ldots,X_n\qquad n>\tau$\\
	Hier braucht man keinen doppelten Index.
	Vergrößert man aber nun $n$, dann geht der Anteil an der Gesamtstichprobe gegen 0, also $\frac{\tau}{n}\longrightarrow0$.
\end{bemerkung}

Zusammenfassung unserer Überlegungen in
\begin{satz}\label{satz8.8}
	Sei $\theta\in(0,1),\mu\neq\nu,\mu_p<\infty,\nu_p<\infty$ für ein $p>2$.
	Dann gilt:
	\begin{align*}
		\hat{\theta}_n
		&=\frac{1}{n}\cdot\underbrace{\argmax\limits_{0\leq k\leq n}\left|\sum\limits_{i=1}^k\big(X_i-\overline{X}_n\big)\right|}_{=\hat{\tau}_n}\overset{n\to\infty}{\longrightarrow}0\text{ f.s.}
	\end{align*}
\end{satz}

\begin{bemerkung}
	 Es gilt \underline{nicht}, dass $\hat{\tau}_n-\tau_n\overset{n\to\infty}{\longrightarrow}0$ f.s.
	 Tatsächlich gilt:
	 \begin{align*}
	 	\hat{\tau}_n-\tau_n\overset{\L}{\longrightarrow} T
	 \end{align*}
	 wobei $T$ die f.s. eindeutige Maximalstelle einer \textit{2-seitigen Irrfahrt / Random Walk} auf $\Z$ ist
	 (vergleiche D.F. (1994) \textit{Asymptotic distribution theory of change-point estimators} veröffentlicht in \textit{Mathematical methods in statics 3}, Seiten 362 bis 378).
\end{bemerkung}

Nächstes Ziel: Unter welchen Bedingungen gilt
\begin{align*}
	Z_n\overset{L}{\longrightarrow}Z\text{ in }\big(C(\R),d\big)
	\implies\argmin\limits_{t\in\R}Z_n(t)\overset{\L}{\longrightarrow}\argmin\limits_{t\in\R}Z(t)
\end{align*}

Folgender Satz gibt Antwort.

\begin{satz}\label{satz8.9}
	Seien $Z,Z_n,n\in\N$ stochastische Prozesse mit Pfaden in $C(\R)$ über $(\Omega,\A,P)$,
	wobei $A(Z)$ 
	\footnote{$A(Z)$ ist eine \textit{random closed set (RCS)}} 
	und $A(Z_n)$ für alle $n\in\N$ nichtleer sind.
	Ferner sei $\sigma_n$ eine Zufallsvariable mit $\sigma_n\in A(Z_n)$.
	Dann gilt: Falls
	\begin{align}\label{eqSatz8.9_1}\tag{1}
		Z_n\overset{L}{\longrightarrow} Z\text{ in }\big(C(\R),d\big)
	\end{align}
	so folgt
	\begin{align}\label{eqSatz8.9_a}\tag{a}
		\limsup\limits_{n\to\infty}\P\big(\sigma_n\in K\big)\leq\mu^\ast(K)\qquad\forall K\in\mathcal{K}:=\big\lbrace K\subseteq\R:K\text{ kompakt}\big\rbrace
	\end{align}
	wobei $\mu^\ast$
	\footnote{$\mu^\ast$ ist die \textit{Kapazitätsfunktion von $A(Z)$} und damit insbesondere eine \textit{Choquet-Kapazität}.
	Beachte, dass $\mu^\ast$ i.d.R. \underline{kein} W-Maß.}	
	die sogenannte \textbf{hitting-probability} ist:
	\begin{align*}
		\mu^\ast:\mathcal{K}\to[0,1],\qquad
		\mu^\ast(K)=\P\big(A(Z)\cap K\neq\emptyset\big)
	\end{align*}
	Gilt zusätzlich, dass $(\sigma_n)_{n\in\N}$ \textbf{stochastisch beschränkt} ist, d.h. 
	\begin{align}\label{eqSatz8.9_2}\tag{2}
		\lim\limits_{j\to\infty}\limsup\limits_{n\to\infty}\P\Big(\big|\sigma_n\big|>j\Big)=0
	\end{align}
	so folgt
	\begin{align}\label{eqSatz8.9_b}\tag{b}
		\limsup\limits_{n\to\infty}\P\big(\sigma_n\in F\big)\leq\mu^\ast(F)\qquad\forall F\in\F(\R):=\big\lbrace F\subseteq\R:F\text{ abgeschlossen}\big\rbrace
	\end{align}
	Falls zusätzlich $A(Z)=\lbrace\sigma\rbrace$ für eine Zufallsvariable $\sigma$
	(d.h. $Z$ hat f.s. eine eindeutige Minimalstelle, nämlich $\sigma$), dann gilt
	\begin{align}\label{eqSatz8.9_c}\tag{c}
		\sigma_n\overset{L}{\longrightarrow}\sigma\text{ in }\R.
	\end{align}
\end{satz}

%Ferger: "Ich weiß nicht mehr wie ich drauf gekommen bin. Aber ich bin darauf gekommen. Da habe ich mich gefreut wie so ein Brötchen."
%Ferger: "Müssen Sie nicht mitschreiben, ich will jetzt hier nur zu einer didaktischen Meisterleistung auflaufen."

\begin{proof}
	\underline{Zeig \eqref{eqSatz8.9_a}:}\\
	Sei $K$ kompakt. Dann gilt:
	\begin{align}\label{eqProofSatz8.9_i}\tag{i}
		\big\lbrace\sigma_n\in K\big\rbrace
		&\subseteq\left\lbrace\inf\limits{t\in K} Z_n(t)\leq\inf\limits_{t\not\in K} Z_n(t)\right\rbrace\\
		&\subseteq\left\lbrace\inf\limits_{t\in K} Z_n(t)\leq\inf\limits_{t\in K^C\cap I_j}Z_n(t)\right\rbrace\label{eqProofSatz8.9_ii}\tag{ii}
	\end{align}
	wobei $I_j:=[-j,j],j\in\N$.\nl
	\underline{Zeige \eqref{eqProofSatz8.9_i}:}\\
	Sei $\sigma_n\in K$ und angenommen
	\begin{align}\label{eqProofSatz8.9Stern}\tag{$\ast$}
		\inf\limits_{t\in K} Z_n(t)>\inf\limits_{t\in K^C} Z_n(t)
	\end{align}
	Dann gilt:
	\begin{align*}
		Z_n(\sigma_n)
		\overset{\sigma_n\in K}&\geq
		\inf\limits_{t\in K} Z_n(t)
		\overset{\eqref{eqProofSatz8.9Stern}}>
		\inf\limits_{t\in K^C} Z_n(t)
		\overset{K^C\subseteq\R}\geq
		\inf\limits_{t\in\R} Z_n(t)
		\overset{\sigma_n\in A(Z_n)}=
		Z(\sigma_n)
	\end{align*}
	Das ist ein Widerspruch, weshalb \eqref{eqProofSatz8.9_i} folgt.\nl
	Gleichung \eqref{eqProofSatz8.9_ii} folgt aus 
	\begin{align*}
		\inf\limits_{t\in K^C} Z_n(t)\leq\inf\limits_{t\in K^C\cap I_j}Z_n(t).
	\end{align*}
	Sei $K_j:=[-k_j,k_j]$ mit $k_j\in\N$ so, dass $K_j\supseteq K\cup I_j$.
	Für jedes $M\subseteq K_j$ gilt:
	Die Abbildung $f\mapsto\inf\limits_{t\in M} f(t)$ ist stetig auf $\big(C(K_j),\Vert\cdot\Vert_{K_j}\big)$,
	denn:
	\begin{align*}
		\left|\inf\limits_{t\in M} f(t)-\inf\limits_{t\in M}g(t)\right|
		&\leq \Vert f-g\Vert_M
		\leq\Vert f-g\Vert_{K_j}
	\end{align*}
	Daraus folgt, dass die Abbildung
	\begin{align*}
		H_j:\left(C\big(K_j\big),\Vert\cdot\Vert_{K_j}\right)\to\R,\qquad
		f\mapsto\inf\limits_{t\in K} f(t)-\inf\limits_{t\in K^C\cap I_j} f(t)
	\end{align*}
	stetig ist, denn $K^C\cap I_j\subseteq I_j\subseteq K\cup I_j\subseteq K_j$.
	Somit folgt aus Satz \ref{satz7.23} und aus dem CMT \ref{satz4.10ContinuousMappingTheorem}:
	\begin{align}\label{eqProofSatz8.9_iii}\tag{iii}
		H_j(Z_n)\overset{\L}{\longrightarrow} H_j(Z)\qquad\forall j\in\N
	\end{align}
	Aus \eqref{eqProofSatz8.9_i} und \eqref{eqProofSatz8.9_ii} folgt:
	\begin{align}\nonumber
		\P\Big(\big\lbrace\sigma_n\in K\big\rbrace\Big)
			&\leq\P\Bigg(\Bigg\lbrace\underbrace{\inf\limits_{t\in K} Z_n(t)-\inf\limits_{t\in K^C\cap I_j}Z_n(t)}_{=H_j(Z_n)}\leq0\Bigg\rbrace\Bigg)\\\nonumber
		\implies		
		\limsup\limits_{n\to\infty}\P\big(\sigma_n\in K\big)
		\overset{\eqref{eqProofSatz8.9_i}+\eqref{eqProofSatz8.9_ii}}&\leq
		\limsup\limits_{n\to\infty}\P\Big(H_j(Z_n)\in\underbrace{(-\infty,0]}_{\in\F(\R)}\Big)\\\nonumber
		\overset{\eqref{eqProofSatz8.9_iii}+\text{Portmannteau}}&\leq
		\P\Big(H_j(Z)\in(-\infty,0]\Big)\\
		\overset{\text{Def }H_j}&=
		\P\bigg(\underbrace{\bigg\lbrace\inf\limits_{t\in K}Z(t)\leq\inf\limits_{t\in K^C\cap I_j}Z(t)\bigg\rbrace}_{=:E_j}\qquad\forall j\in\N\label{eqProofSatz8.9_iv}\tag{iv}
	\end{align}
	Da $E_j$ monoton fallend ist, folgt (mit $\sigma$-Stetigkeit von oben):
	\begin{align*}
		\lim\limits_{j\to\infty}\P(E_j)
		&=\P\left(\bigcap\limits_{j\in\N}E_j\right)\\
		&=\P\left(\inf\limits_{t\in K}Z(t)\leq\inf\limits_{t\in K^C\cap I_j}Z(t)~\forall j\in\N\right)\\
		&\leq \P\bigg(\inf\limits_{t\in K}Z(t)\leq\underbrace{\inf\limits_{j\in\N}\inf\limits_{t\in K^C\cap I_j}Z(t)}_{=\inf\limits_{\bigcup\limits_{j\in\N}\big(K^C\cap I_j\big)}Z(t)}\bigg)\\
		\overset{\bigcup\limits_j(K^C\cap I_j)=K^C\cap\bigcap\limits_j I_j=K^C\cap\R}&=
		\P\bigg(\underbrace{\bigg\lbrace\inf\limits_{t\in K} Z(t)\leq\inf\limits_{t\in K^C} Z(t)\bigg\rbrace}_{=:E}\bigg)
	\end{align*}
	Dahinter steckt die allgemeine Rechenregel
	\begin{align*}
		\inf\limits_{t\in A\cup B}f(t)=\inf\limits\left\lbrace\inf\limits_A f,\inf\limits_B f\right\rbrace.
	\end{align*}
	Grenzübergang $j\to\infty$ in \eqref{eqProofSatz8.9_iv} liefert:
	\begin{align}\label{eqProofSatz8.9Plus}\tag{+}
		\limsup\limits_{n\to\infty}\P\big(\sigma_n\in K\big)
		\leq\lim\limits_{j\to\infty}\P(E_j)\leq\P(E)
	\end{align}
	Schließlich gilt
	\begin{align}\label{eqProofSatz8.9_v}\tag{v}
		E\subseteq\big\lbrace A(Z)\cap K\neq\emptyset\big\rbrace
	\end{align}
	denn:
	\begin{align*}
		&\inf\limits_{t\in K}Z(t)
		\leq\inf\limits_{t\in K^C} Z(t)\\
		&\implies
		\inf\limits_\R Z(t)=\inf\limits_{K\cup K^C}Z(t)=\min\left\lbrace\inf\limits_{t\in K} Z(t),\inf\limits_{t\in K^C}Z(t)\right\rbrace=\inf_{t\in K}Z(t)
		=Z(\tau)
	\end{align*}
	für ein $\tau\in K$, da $Z$ stetig und $K$ kompakt.
	Also ist $\tau$ eine Minimalstelle von $Z$, i.Z. $\tau\in A(Z)$.
	Da $\tau\in K$ ist, gilt
	\begin{align*}
		\tau\in A(Z)\cap K\\
		\implies A(Z)\cap K\neq\emptyset
	\end{align*}
	Wegen \eqref{eqProofSatz8.9_v} gilt, dass
	\begin{align*}
		\P(E)\leq\P\big(A(Z)\cap K\neq\emptyset\big)=\mu^\ast(K)
	\end{align*}
	und \eqref{eqSatz8.9_a} folgt aus \eqref{eqProofSatz8.9Plus}.\nl
	\underline{Zeige \eqref{eqSatz8.9_b}:}\\
	Sei $F\in\F$. Dann gilt:
	\begin{align}\nonumber
		\big\lbrace\sigma_n\in F\big\rbrace
		\overset{\text{Zerlegung}}&=
		\big\lbrace\sigma_n\in F,\sigma_n\in I_j\big\rbrace
		\dot{\cup}\big\lbrace\sigma_n\in F,\sigma_n\not\in I_j\big\rbrace\\\nonumber
		&=\big\lbrace\sigma_n\in \underbrace{F\cap I_j}_{\text{kompakt}}\big\rbrace\dot{\cup}\big\lbrace |\sigma_n|>j\big\rbrace&\forall& j\in\N\\\nonumber
		\implies
		\P\Big(\big\lbrace\sigma_n\in F\big\rbrace\Big)
		&\leq\P\Big(\big\lbrace\sigma_n\in F\cap I_j\big\rbrace\Big)+\P\Big(\big\lbrace |\sigma_n|>j\big\rbrace\Big)&\forall& j\in\N\\
		\limsup\limits_{n\to\infty}\P(\sigma_n\in F)
		&\leq\underbrace{\limsup\limits_{n\to\infty}\P\big(\sigma_n\in F\cap I_j)}_{
			\overset{\eqref{eqSatz8.9_a}}{\leq}\P\big(\underbrace{\big\lbrace A(Z)\cap(F\cap I_j)\neq\emptyset\big\rbrace}_{=:B_j}
		}+\underbrace{\limsup\limits_{n\to\infty}\P\big(|\sigma_n|>j\big)}_{
			\overset{j\to\infty}{\longrightarrow}\text{ wegen }\eqref{eqSatz8.9_2}
		} &\forall& j\in\N\label{eqProofSatz8.9PlusPlus}\tag{++}
	\end{align}
	Da
	\begin{align*}
		B_j\uparrow\big\lbrace A(Z)\cap F\neq\emptyset\big\rbrace
	\end{align*}
	 (zur Übung), liefert Grenzübergang $j\to\infty$ in \eqref{eqProofSatz8.9PlusPlus} und $\sigma$-Stetigkeit von unten und \eqref{eqSatz8.9_2} die Behauptung \eqref{eqSatz8.9_b}.\nl
	 \underline{Zeige \eqref{eqSatz8.9_c}:}
	 \begin{align*}
	 	&A(Z)=\lbrace\sigma\rbrace\text{ f.s.}\\
	 	&\implies\mu^\ast(F)=\P\big(A(Z)\cap F\neq\emptyset\big)
	 	=\P\Big(\big\lbrace\sigma\rbrace\cap F\neq\emptyset\big\rbrace\Big)
	 	=\P(\sigma\in F)\\
	 	\overset{\eqref{eqSatz8.9_b}}&\implies
	 	\limsup\limits_{n\to\infty}\P\big(\sigma_n\in F\big)\leq\mu^\ast(F)
	 	=\P(\sigma\in F)\qquad\forall F\in F\\
	 	\overset{\text{Portmannteau}}&{\implies}
	 	\sigma_n\overset{\L}{\longrightarrow}\sigma
	 \end{align*}
\end{proof}

%Ferger: "Ich war früher so ein richtiger Sport-Paul. Aber jetzt bin ich einfach zu faul. Ist nicht so, dass ich keine Zeit habe.

\subsection*{Anwendung}

Sei $X_1,\ldots,X_n$ i.i.d. Kopien von $X$ mit $\mu$-Dichte 
\begin{align*}
	f_\theta(x)&=c(\theta)\cdot h(x)\cdot\exp\big(q(\theta)\cdot T(x)\big)\qquad\forall x\in\X,\forall\theta\in\Theta=\R
\end{align*}
Sei $\theta_0$ der wahre Parameter.
Dann gilt:

\begin{align}\label{eqAnwendung_1}\tag{1}
	\text{MLS $\hat{\theta}_n$ für $\theta$ ist }\hat{\theta}_n&=\argmin\limits_{t\in\R}M_n(t)\qquad\mit\\
	M_n(t)&=-\log\big(c(t)\big)-q(t)\cdot\overline{T}_n,\qquad
	\overline{T}_n=\frac{1}{n}\cdot\sum\limits_{i=1}^nt(X_i)\nonumber
\end{align}
Ziel: Nachweis von
\begin{align*}
	\sqrt{n}\cdot\big(\hat{\theta}_n-\theta_0\big)\overset{\L}{\longrightarrow}\xi
\end{align*}
und Identifizierung von $\xi$.\nl
Dazu: Sei 
\begin{align*}
	Z_n(t):=n\cdot\left(M_n\left(\theta_0+\frac{t}{\sqrt{n}}\right)-M_n\big(\theta_0\big)\right)\qquad\forall t\in\R
\end{align*}
der \textbf{reskalierte Prozess} zu $M_n$.
Offenbar ist $Z_n$ stetiger stochastischer Prozess \underline{und}
\begin{align}\label{eqAnwendung_2}\tag{2}
	\underbrace{\sqrt{n}\cdot\big(\hat{\theta}_n-\theta\big)}_{=:\sigma_n}
	&=\argmin\limits_{t\in\R} Z_n(t)
\end{align}
denn:
Zu zeigen: 
\begin{align*}
	Z_n(\sigma_n)\leq Z_n(t)\qquad\forall t\in\R
\end{align*}
Dazu nutze Äquivalenzen:
\begin{align*}
	&Z_n(\sigma_n)\leq Z_n(t) &\forall& t\in\R\\
	\overset{t=\sigma_n}&\Longleftrightarrow
	n\bigg(M_n\cdot\bigg(\underbrace{\theta_0+\frac{\sigma_n}{\sqrt{n}}}_{
		\begin{subarray}{c}
			=\theta_0+\frac{\sqrt{n}\big(\hat{\theta}_n-\theta_n\big)}{\sqrt{n}}\\
			=\theta_n
		\end{subarray}
	}\bigg)-M_n(\theta_0)\bigg)
	\leq n\cdot\left(M_n\left(\theta_0+\frac{t}{\sqrt{n}}\right)-M_n(\theta_0)\right)&\forall& t\in\R\\
	&\Longleftrightarrow n\cdot\Big(M_n\big(\hat{\theta}_n\big)-M_n\big(\theta_0\big)\Big)
	\leq n\cdot\left(M_n\left(\theta_0+\frac{t}{\sqrt{n}}\right)-M_n(\theta_0)\right) &\forall&t\in\R\\
	&\Longleftrightarrow M_n\big(\hat{\theta}_n\big)
	\leq M_n\left(\theta_0+\frac{t}{\sqrt{n}}\right) &\forall&t\in\R\\
	&\Longleftrightarrow M_n\big(\hat{\theta}_n\big)
	\leq M_n\left(\theta_0+\frac{t}{\sqrt{n}}\right) &\forall& t\in\R\\
	&\Longleftrightarrow M_n\big(\hat{\theta}_n\big)\leq M_n(u) &\forall& u\in\R\\
	\overset{\eqref{eqAnwendung_1}}&\Longleftrightarrow \text{ True}
\end{align*}
Wegen \eqref{eqAnwendung_2} versuche Satz \ref{satz8.9} anzuwenden:\\
Man kann zeigen:
\begin{align}\label{eqAnwendung_i}\tag{i}
	Z_n\overset{\L}{\longrightarrow} Z\text{ in }\big(C(\R),d\big)
\end{align}
wobei
\begin{align*}
	Z(t)
	&=-q'(\theta_0)\cdot N\cdot t+\frac{1}{2}\cdot\big(q'(\theta_0)\big)^2\cdot\sigma^2(\theta_0)\cdot t^2 \qquad\forall t\in\R
\end{align*}
wobei wir fordern:
\begin{align*}
	\sigma^2(\theta_0):=\Var_{\theta_0}\big(T(X)\big)\overset{\text{Forderung}}&>0\\
	q'(\theta_0)\overset{\text{Forderung}}&\neq0\\
	N&\sim\Nor\big(0,\sigma^2(\theta_0)\big)
\end{align*}
Also ist $Z$ eine zufällige nach oben geöffnete Parabel.

\begin{align*}
	0
	&=Z'(t)
	=-q'(\theta_0)\cdot N+\big(q'(\theta_0)\big)^2\cdot\sigma^2(\theta_0)\cdot t\\
	\xi
	&=\frac{1}{q'(\theta_0)\cdot\sigma^2(\theta_0}\cdot N
	\text{ ist die eindeutige Minimalstelle von }Z.
\end{align*}
Man kann zeigen:
\begin{align*}
	\limsup\limits_{n\to\infty}\P\Big(\sqrt{n}\cdot\big|\hat{\theta}_n-\theta_0\big|\geq j\Big)
	\leq\frac{4}{\big(q'(\theta_0)\big)^2\cdot\sigma^2(\theta_0)}\cdot\frac{1}{j^2}\qquad\forall n\in\N
\end{align*}
Grenzübergang $j\to\infty$ liefert, dass zweite Bedingung \eqref{eqSatz8.9_2} in Satz \ref{satz8.9} erfüllt ist.
Somit liefert Satz \ref{satz8.9}:
\begin{align*}
	\sqrt{n}\cdot\big(\hat{\theta}_n-\theta_0\big)\overset{\L}{\longrightarrow}\xi
\end{align*}

\section{Verteilungskonvergenz im Raum konvexer Funktionen} %noNumber

\begin{definition}\label{def9.1}\
	\begin{enumerate}[label=(\arabic*)]
		\item Eine Menge $\emptyset\neq O\subseteq\R$ heißt \textbf{konvex}
		\begin{align*}
			:\Longleftrightarrow x,y\in O\implies\forall\lambda\in(0,1):\lambda\cdot x+(1-\lambda)\cdot y\in O
		\end{align*}
		\item Sei $O$ konvex. Eine Funktion $f\colon O\to\R$ heißt \textbf{konvex}
		\begin{align*}
			:\Longleftrightarrow 
			f\big(\lambda\cdot x+(1-\lambda)\cdot y\big)\leq\lambda\cdot f(x)+(1-\lambda)\cdot f(y)\qquad\forall x,y\in,\forall\lambda\in(0,1)
		\end{align*}
		\item $\begin{aligned}
			C_c(O):=\big\lbrace f\colon O\to\R:f\text{ konvex}\big\rbrace
		\end{aligned}$
		\item Ein stochastischer Prozess
		\begin{align*}
			X=\big\lbrace X(t):t\in O\big\rbrace
		\end{align*}				
		über $(\Omega,\A)$ mit Pfaden in $C_c(O)$ heißt \textbf{konvexer stochastischer Prozess} auf $O$.
	\end{enumerate}
\end{definition}

Im Folgenden sei $O=(a,b)$ mit $-\infty\leq a<b\leq\infty$ und $d$ die Metrik der gleichmäßigen Konvergenz auf $C_c(O)$.
Bekanntlich ist $C_c(O)\subseteq C(O)$, also $\big(C_c(O),d\big)$ als Unterraum von $\big(C(O),d\big)$ ebensfalls separabel.
Seien $\pi_t$ und $\pi_T$ die Projektionen eingeschränkt auf $C_c(O)$.

\begin{satz}\label{satz9.2}
	\begin{align*}
		\B_d\big(C_c(O)\big)=\sigma\big(\pi_t:t\in O\big)=\sigma\big(\pi_T:T\subseteq O\text{ endlich}\big)
	\end{align*}
\end{satz}

\begin{proof}
	Analog zu Satz \ref{satz7.2}.
\end{proof}

%Ferger: "Ich bin zu gut für diese Welt." (also er einen Satz nochmals anschrieb, weil einige Stundenten nicht hinterher kamen)

Wir erhalten folgende Korollare:

\begin{korollar}\label{korollar9.3}
	 Sei $X$ konvexer stochastischer Prozess über $(\Omega,\A)$ auf $O$.\\
	 Dann ist die Pfadabbildung
	 \begin{align*}
	 	X\colon(\Omega,\A)\to\Big(\C_c(O),\B_d\big(C_c(O)\big)\Big)
	 \end{align*}
	 messbar.
\end{korollar}

\begin{korollar}\label{korollar9.4} %9.3 in VL
	Seien $X,Y$ Zufallsvariablen in $\big(C_c(O),d\big)$.
	Dann gilt:
	\begin{align*}
		X\overset{\L}{=} Y\Longleftrightarrow
		\big(X(t_1),\ldots,X(t_k)\big)
		\overset{\L}{=}
		\big(Y(t_1),\ldots,Y(t_k)\big)
		\qquad\forall t_1,\ldots,t_k\in O,\forall k\in\N
	\end{align*}
	(Die endlich dimensionalen Randverteilungen sind gleich.)
\end{korollar}

\begin{beispiel}\label{beispiel9.5}\
	\begin{enumerate}[label=(\arabic*)]
		\item $f(x):=|x|,x\in\R$ ist konvex.
		\begin{proof}
			Folgt aus Dreiecksungleichung.
		\end{proof}
		\item Sei $f\colon\R\to\R$ konvex und $a\in\R$.
		Dann ist auch $f_a(x):=f(x-a),x\in\R$ konvex.
		\begin{proof}
			Nachrechnen.
		\end{proof}
	\end{enumerate}
\end{beispiel}

\begin{lemma}\label{lemma9.6}
	Sei $F$ Verteilungsfunktion auf $\R$ mit
	\begin{align*}
		\int\limits_\R |x|~F(\d x)<\infty
	\end{align*}
	Dann ist die Funktion
	\begin{align*}
		M\colon\R\to\R,\qquad M(t):=\int\limits_\R|x-t|~F(\d x)\qquad\forall t\in\R
	\end{align*}
\end{lemma}

\begin{proof}
	Folgt aus \ref{beispiel9.5} und der Monotonie und Linearität des Integrals.
\end{proof}

Erinnere an:\\
$m$ ist \textbf{Median} von $F$
\begin{align*}
	:\Longleftrightarrow m\in A(M)
\end{align*}
$\hat{m}_n$ ist \textbf{empirischer Median} zu $X_1,\ldots,X_n$
\begin{align*}
	:\Longleftrightarrow&\hat{m}_n\in A(M_n)
	\qquad\mit\qquad
	\hat{M}_n(t):=\int\limits_\R|x-t| F_n(\d x)
\end{align*}
wobei $F_n$ die \textit{empirische Verteilungsfunktion} zu $X_1,\ldots,X_n$ ist.
Also:
\begin{align*}
	M_n(t)=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big|X_i-t\big|\qquad\forall t\in\R
\end{align*}
Gemäß \ref{korollar9.3} sind $M_n,n\in\N$ und $M$ als	 konvexe stochastische Prozesse Zufallsvariablen in $\big(C_r(\R),d\big)$.

\begin{satz}\label{satz9.7}
	Sei $D\subseteq O$ dicht in $O=(a,b)\subseteq\R$ (z. B. $D=\Q\cap O$) und $K\subseteq O$ kompakt.
	Dann existieren eine universelle Konstante $c$ und Punkte $b_1,\ldots,b_m\in D$ so, dass für jede Konvexe Funktion $f\colon O\to\R$ gilt:
	\begin{align*}
		\big|f(x)-f(y)\big|\leq c\cdot\sum\limits_{i=1}^n\big|f(b_i)\big|\cdot|x-y|\qquad\forall x,y\in O
	\end{align*}
\end{satz}

\begin{proof}
	Siehe Liese und Mischka (2008) \textit{Statistical decision theory} Seite 402 bis 403.
\end{proof}

% Ferger: "Da sehe ich mich noch Samstagsmittags nach einem gut gekühlten Weißwein und blättere in dem Buch und stoße auf dieses Ergebnis. Da bin ich erst mal vor Freude hin und her gehopst."

\begin{theorem}\label{theorem9.8}
	Seien $f_n,n\in\N$ konvexe Funktionen auf $O$, $D\subseteq O$ dicht in $O$.
	Sei $f\colon O\to\R$ eine Funktion.
	Falls 
	\begin{align}\label{eqtheorem9.8Stern}\tag{$\ast$}
		f_n(x)\overset{n\to\infty}{\longrightarrow} f(x)\qquad\forall x\in D
	\end{align}
	gilt, so gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			f(x)=\limn f_n(x)\text{ existiert }\qquad\forall x\in O
		\end{aligned}$
		\item $f$ ist konvex.
		\item $\begin{aligned}
			\big\Vert f_n-f\big\Vert_K\overset{n\to\infty}{\longrightarrow}0\qquad\forall K\subseteq O
		\end{aligned}$
	\end{enumerate}
\end{theorem}

% Ferger kommt aus Westerwald, Westerburg

\begin{proof}
	Siehe Rockafellar (1972) \textit{Convex Analysis} Theorem 10.8.
\end{proof}

\begin{satz}\label{satz9.9}
	Seien $X_n,n\in\N$ konvexe stochastische Prozesse und sei $X$ stetiger stochastischer Prozess auf $\R$ über $(\Omega,\A,\P)$.
	Dann sind (1) $\gdw$ (2) $\implies$ (3):
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			X_n\overset{\text{fd}}{\longrightarrow}X\qquad\text{Konvergenz der fidis}
		\end{aligned}$
		\item $\begin{aligned}
			X_n\overset{\L}{\longrightarrow} X\text{ in }\big(C(\R),d\big)
		\end{aligned}$
		\item In jedem Fall ist $X$ ein \underline{konvexer} stochastischer Prozess.
	\end{enumerate}
\end{satz}

\begin{proof}
	\underline{Zeige (1) $\implies$ (2):}\\
	Gemäß Satz \ref{satz7.23} ist zu zeigen:
	\begin{align}\label{eqProofSatz9.9Stern}\tag{$\ast$}
		X_n^{(j)}\overset{\L}{\longrightarrow} X^{(j)}\text{ in }\big(C(I_j),d_j\big)\qquad\forall j\in\N\mit I_j:=[-j,j]
	\end{align}		
	Nachweis von \eqref{eqProofSatz9.9Stern} mit Satz \ref{satz7.9}.
	Voraussetzung (1) in Satz \ref{satz7.9} ist nach Voraussetzung erfüllt.
	Betrachte den \textbf{Oszillationsmodul}
	\begin{align*}
		q\left(X_n^{(ij)},\delta\right)
		\overset{\text{Def}}&=
		\sup\limits\Big\lbrace\big|X_n(s)-X_n(t)\big|:s,t\in I_j,|s-t|\leq\delta\Big\rbrace\\
		&\leq \underbrace{c\cdot\sum\limits_{i=1}^m\big|X_n(b_i)\big|}_{=:Z_n}\cdot\delta\mit c>0\text{ und }b_1,\ldots,b_m\in I_j
	\end{align*}
	
	Wegen (1) gilt 
	\begin{align*}
		X_n(b_1),\ldots,X_n(b_m)\big)\overset{\L}&{\longrightarrow}\big(X(b_1),\ldots,X(b_m)\big)\\
		\overset{\ref{satz4.10ContinuousMappingTheorem}}\implies
		Z_n\overset{\L}{\longrightarrow} Z&=c\cdot\sum\limits_{i=1}^m\big|X(b_i)\big|\\
		\implies
		0&\leq\limsup\limits_{n\to\infty}\P\Big(\underbrace{w\Big(X_n^{(j)},\delta_k\Big)}_{\leq Z_n\cdot\delta_k}\geq\varepsilon\Big)\\
		&\leq\limsup\limits_{n\to\infty}\P\left(Z_n\geq\frac{\varepsilon}{\delta_k}\right)\\
		\overset{\text{Portmannteau}}&\leq
		\P\left(Z\geq\frac{\varepsilon}{\delta_k}\right)
		\qquad\forall\varepsilon>0,\forall(\delta_k)_{k\in\N}\subseteq(0,\infty)
	\end{align*}
	Für $\delta_k\downarrow0$ folgt $\frac{\varepsilon}{\delta_k}\overset{k\to\infty}{\longrightarrow}+\infty$, also
	\begin{align*}
		\P\left(Z\geq\frac{\varepsilon}{\delta_k}\right)\overset{k\to\infty}{\longrightarrow}0
	\end{align*}
	Somit ist Voraussetzung (2) in Satz \ref{satz7.9} erfüllt. 
	Daraus folgt (2).\nl
	\underline{Zeige (2) $\implies$ (1):}
	Folgt direkt aus dem CMT \ref{satz4.10ContinuousMappingTheorem}.\nl
	\underline{Zeige (2) $\implies$ (3):} 
	Ohne Beweis.
\end{proof}

\begin{lemma}[Subspace-Lemma]\label{lemma9.10SubspaceLemma}\enter
	Sei $U\subseteq\S$ ein Unterraum des metrischen Raumes $(\S,d)$.
	Seien $Z,Z_n,n\in\N$ Zufallsvariablen in $\big(U,\B_d(U)\big)$.
	Dann gilt:
	\begin{align*}
		Z_n\overset{\L}{\longrightarrow} Z\text{ in }(\S,d)
		\Longleftrightarrow
		Z_n\overset{\L}{\longrightarrow} Z\text{ in }(U,d)
	\end{align*}
\end{lemma}

\begin{proof}
	Siehe Kallenberg (1997) \textit{Foundations of modern probability}, Lemma 3.2
\end{proof}

\begin{korollar}\label{korollar9.11}
	Seien $X,X_n,n\in\N$ konvexe stochastische Prozesse über $(\Omega,\A)$ auf $\R$ nach $\R$ mit 
	\begin{align*}
		X_n\overset{\text{fd}}{\longrightarrow}X
	\end{align*}
	Dann folgt
	\begin{align*}
		X_n\overset{\L}{\longrightarrow} X\text{ in }\big(C_c(\R),d\big)
	\end{align*}
\end{korollar}

\begin{proof}
	Folgt aus Satz \ref{satz9.9} und Lemma \ref{lemma9.10SubspaceLemma}.
\end{proof}

\section{Argmin-Theoreme in \texorpdfstring{$C_c(\R)$}{C\_c(R)}}

\begin{lemma}\label{lemma10.1}
	Seien $f\colon\R^d\to\R$ konvex, $g\colon\R^d\to\R$ stetig, $A(f)\neq\emptyset$, $A(g)=\lbrace\sigma\rbrace$.\\
	Für $\varepsilon>0$ seien
	\begin{align*}
		\overline{B}(\sigma,\varepsilon)&:=\big\lbrace t\in\R^d:|t-\sigma|\leq\varepsilon\big\rbrace\\
		m(\varepsilon)&:=\inf\limits\big\lbrace g(t9:|t-\sigma|=\varepsilon\big\rbrace\\
		b(\varepsilon)&:=\frac{1}{2}\cdot\big(m(\varepsilon)-g(\sigma)\big)
	\end{align*}
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			%\exists\varepsilon_0>0:\forall\varepsilon\in(0,\varepsilon_0]:b(\varepsilon)>0 %wurde gestrichen
			\forall\varepsilon>0:b(\varepsilon)>0
		\end{aligned}$
		%\item Für $\varepsilon\in(0,\varepsilon_0]$ gilt: %wurde geändert
		\item Für alle $\varepsilon>0$ gilt:
		\begin{align*}
			\Vert f-g\Vert_{\overline{B}(\sigma,\varepsilon)}<b(\varepsilon)
			\implies|\tau-\sigma|\leq\varepsilon\qquad\forall\tau\in A(f)
		\end{align*}
	\end{enumerate}
\end{lemma}

\begin{proof}
	\underline{Zeige (1):}\
		Sei $\varepsilon>0$ und angenommen $b(\varepsilon)=0$.
		Dann:
		\begin{align*}
			g(\sigma)=m(\varepsilon)=g(t_\varepsilon)
			\text{ für ein }t_\varepsilon\in
			\big\lbrace t\in\R^d:|t-\sigma|=\varepsilon\big\rbrace=:S(\sigma,\varepsilon),
		\end{align*}
		da $S(\sigma,\varepsilon)$ kompakt und $g$ stetig (stetige Funktionen nehmen auf kompakten Mengen ihr Infimum an!).
		Somit ist $t_\varepsilon\in A(g)$ und $t_\varepsilon=\sigma$ im Widerspruch zu $t_\varepsilon\in S(\sigma,\varepsilon)$.\nl
	\underline{Zeige (2):}
	Sei $t\not\in\overline{B}(\sigma,\varepsilon)$ und $\lambda:=\frac{\varepsilon}{\Vert t-\sigma\Vert}\in(0,1)$.
	Für
	\begin{align*}
		s:=(1-\lambda)\cdot\sigma+\lambda\cdot t
		=\sigma+\lambda\cdot(t-\sigma)
	\end{align*}
	folgt
	\begin{align}\label{eqProofLemma10.1Stern}\tag{$\ast$}
		\Vert S-\sigma\Vert=\lambda\cdot\Vert t-\sigma\Vert
		=\varepsilon
	\end{align}
	Da $f$ konvex, folgt
	\begin{align}\nonumber
		f(s)
		&\leq(1-\lambda)\cdot f(\sigma)+\lambda\cdot f(t)\\\nonumber
		&=f(\sigma)+\lambda\cdot\big(f(t)-f(\sigma)\big)\\\nonumber
		&=\lambda\cdot\big(f(t)-f(\sigma)\big)\\\nonumber
		&\geq f(s)-f(\sigma)\\\nonumber
		&=g(s)-g(\sigma)+\underbrace{f(s)-g(s)}_{
			\overset{s\in\overline{B}(\sigma,\varepsilon)}{\geq}-\Vert f-g\Vert_{\overline{B}(\sigma,\varepsilon)}
		}-\underbrace{\big(f(\sigma)-g(\sigma)\big)}_{
			\leq\Vert f-g\Vert_{\overline{B}(\sigma,\varepsilon)}
		}\\\nonumber
		&\geq \underbrace{\underbrace{g(s)}_{
			\overset{s\in S(\sigma,\varepsilon)}{\geq}m(\varepsilon)
		}-g(\sigma)}_{
			\geq m(\varepsilon)-g(\sigma)=2\cdot b(\varepsilon)
		}-2\cdot\Vert f-g\Vert_{\overline{B}(\sigma,\varepsilon)}\\\nonumber
		&\geq 2\cdot\underbrace{\Big(b(\varepsilon)-\Vert f-f\Vert_{\overline{B}(\sigma,\varepsilon)}}_{
			>0\text{ nach Annahme}
		}\Big)\\
		\implies f(t)>f(\sigma)\qquad\forall t\not\in \overline{B}(\sigma,\varepsilon)\label{eqProofLemma10.1Plus}\tag{+}
	\end{align}
	Somit folgt $\tau\in\overline{B}(\sigma,\varepsilon)$, da sonst wegen \eqref{eqProofLemma10.1Plus} folgen würde, dass $f(\tau)>f(\sigma)$.
	Dies ist aber ein Widerspruch, wann immer $\tau\in A(f)$.
	Schließlich folgt $|\tau-\sigma|\leq\varepsilon$.
\end{proof}

%Ferger: "Das schafft nicht einmal bis Chuck Norris."

Lemma \ref{lemma10.1} geht zurück auf Hjorth und Pollard (1993), Preprint.
Wir erhalten folgendes Argmin-Theorem für konvexe Funktionen:

\begin{satz}\label{satz10.2}
	Sei $D\subseteq\R^d$ dicht in $\R^d$ (z.B. $D=\Q^d$) und seien $f,f_n,n\in\N$ konvexe Funktionen auf $\R^d$, wobei $f$ eine eindeutige Minimalstelle $\tau$ besitze ($A(f)=\lbrace\tau\rbrace$).
	Seien $\tau_n\in A(f_n)\neq\Phi$ $\forall n\geq N_0\in\N$.
	Dann gilt:
	\begin{align}\label{eqSatz10.2Stern}\tag{$\ast$}
		\Big(f_n(t)\overset{n\to\infty}{\longrightarrow} f(t)\qquad\forall t\in D\Big)\implies \tau_n\overset{n\to\infty}{\longleftarrow}\tau
	\end{align}
\end{satz}

\begin{proof}
	Konvexe Funktionen sind immer stetig, also ist $f$ stetig.
	Lemma \ref{lemma10.1} mit $f\hat{=}f_n$, $g\hat{=}f$ liefert:
	\begin{align*}
		b(\varepsilon)>0\qquad\forall\varepsilon>0
	\end{align*}
	Gemäß Theorem \ref{theorem9.8} folgt aus \eqref{eqSatz10.2Stern}:
	\begin{align*}
		\big\Vert f_n-f\big\Vert_{\overline{B}(\tau,\varepsilon)}\overset{n\to\infty}{\longrightarrow}0
	\end{align*}
	(weil $\overline{B}(\tau,\varepsilon)$ kompakt.)
	Somit folgt:
	\begin{align*}
		&\forall\varepsilon>0:\exists n_0=n_0(\varepsilon):\forall n\geq n_0(\varepsilon):
		\underbrace{\big\Vert f_n-f\big\Vert_{\overline{B}(\tau,\varepsilon)}<b(\varepsilon)}_{\overset{\ref{lemma10.1}(2)}{\implies}|\tau_n-\tau|\leq\varepsilon}\\
		&\implies \tau_n\overset{n\to\infty}{\longrightarrow}\tau
	\end{align*}
\end{proof}

Satz \ref{satz10.2} liefert Argmin-Theorem für konvexe stochastische Prozesse bzgl. fast sicherer Konvergenz.

\begin{satz}\label{satz10.3}
	Seien $M,M_n,n\in\N$ konvexe stochastische Prozesse auf $\R^d$ über $(\Omega,\A,\P)$.
	Es gelte:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			A(M)=\lbrace\tau\rbrace
		\end{aligned}$ f.s. für eine Zufallsvariable $\tau$.
		\item $\begin{aligned}
			M_n(t)\overset{n\to\infty}{\longrightarrow} M(t)~\P\text{-f.s.}\qquad\forall t\in\R^d
		\end{aligned}$
	\end{enumerate}
	Seien $\tau_n$ Zufallsvariablen mit $\tau_n\in A(M_n)$ f.s. $\forall n\in\N$.
	Dann gilt:
	\begin{align*}
		\tau_n\overset{n\to\infty}{\longrightarrow}\tau~~\P\text{-f.s.}\text{ in }\R^d
	\end{align*}
\end{satz}

\begin{proof}
	Seien
	\begin{align*}
		\Omega_1&:=\big\lbrace A(M)=\tau\big\rbrace\\
		\Omega_2&:=\bigcap\limits_{t\in\Q^d}\Big\lbrace M_n\overset{n\to\infty}{\longrightarrow}M(t)\Big\rbrace\\
		\Omega_3&:=\bigcap\limits_{n\in\N}\big\lbrace\tau_n\in A(M_n)\big\rbrace\\
		\Omega_0&:=\Omega_1\cap\Omega_2\cap\Omega_3
	\end{align*}
	Aus den Voraussetzungen folgt, dass $\Omega_0$ eine Einsmenge ist (abzählbarer Schnitt von Einsmengen ist Einsmenge), d.h. $\P(\Omega_0)=1$.
	Aus Satz \ref{satz10.2} folgt ($f\hat{=}M_n(\omega,\cdot)$, $f\hat{=}M(\omega,\cdot)$, $\tau\hat{=}\tau(\omega)$, $\tau_n(\omega)\hat{=}\tau_n$):
	\begin{align*}
		\Omega_0\subseteq\Big\lbrace\tau_n\overset{n\to\infty}{\longrightarrow}\tau\Big\rbrace\\
		\implies \P\Big(\tau\overset{n\to\infty}{\longrightarrow}\tau\Big)=1
	\end{align*}
\end{proof}

\subsection{Anwendung: Starke Konsistenz des empirischen Medians} %noNumber

\begin{align*}
	M_n(t)&:=\frac{1}{n}\cdot\sum\limits_{i=1}^n\big|X_i-t\big|&\forall& t\in\R\\
	\overset{\text{SGGZ}}{\implies}
	M_n(t)\overset{n\to\infty}&{\longrightarrow}\E\Big[\big|X_1-t\big|\Big]
	\overset{\text{Def}}= M(t)\text{ f.s.} &\forall& t\in\R^d
\end{align*}
Falls der Median $m$ von $F$ \underline{eindeutig}, d.h. $A(M)=\lbrace m\rbrace$ (Beachte: hier ist $M$ deterministisch, also konstanter stochastischer Prozess), so folgt aus Satz \ref{satz10.3} sofort für jede messbare Auswahl $\hat{m}_n\in A(M_n)$:
\begin{align*}
	\hat{m}_n\overset{n\to\infty}{\longrightarrow} m~~\P\text{-f.s.}
\end{align*}
Zum Beispiel $\hat{m}_n:=F^{-1}\left(\frac{1}{2}\right)$.\nl
Das Argmin-Theorem für Verteilungskonvergenz ist:

\begin{satz}\label{satz10.4}
	Seien $Z,Z_n,n\in\N$ konvexe stochastische Prozesse auf $\R$ über $(\Omega,\A,\P)$.
	Es gelte:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			A(Z)=\lbrace\sigma\rbrace
		\end{aligned}$ f.s. für eine Zufallsvariable $\sigma$.
		\item $\begin{aligned}
			Z_n\overset{\text{fd}}{\longrightarrow} Z
		\end{aligned}$
	\end{enumerate}
	Dann gilt für jede messbare Auswahl $\sigma_n\in A(Z_n)$:
	\begin{align*}
		\sigma_n\overset{\L}{\longrightarrow}\sigma
	\end{align*}
\end{satz}

\begin{proof}[Beweisskizze]
	\begin{align*}
		(2) &\implies Z_n\overset{n\to\infty}{\longrightarrow}Z\text{ in }\big(C(\R),d\big)\\
		&\implies\limsup\limits_{n\to\infty}\P\big(\sigma_n\in K\big)\leq\P\big(\lbrace\sigma\rbrace\cap K\neq\emptyset\big\rbrace
		=\P\big(\sigma\in K\big)\qquad\forall K\in \mathcal{K}
	\end{align*}
\end{proof}