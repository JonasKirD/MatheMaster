% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\chapter{Zentrale Grenzwertsätze (ZGS)} %9
\ul{\textbf{Botschaft / Kernaussage:}}\nl
Summen von unabhängigen Zufallsvariablen konvergieren bei passender Normalisierung (Verschiebung + Skalierung) in Verteilung gegen eine 
(Standard-)Normalverteilte Zufallsvariable, wenn:
\begin{itemize}
	\item Alle Zufallsvariablen endliche Varianzen $\sigma_j^2$ haben.
	\item Keine der $\sigma_j^2$ zu stark "dominiert".
\end{itemize}

Wir behandeln ZGS von 
\begin{itemize}
	\item De Moivre-Laplace (Alle $\sigma_j^2$ sind gleich)
	\item Lindeberg (unterschiedliche $\sigma_j^2$)
\end{itemize}

Beweismethode: Charakteristische Funktionen und Stetigkeitssatz von Lévy \ref{theorem8.1StetigkeitssatzVonLevy}

\begin{vorüberlegung}
	Taylor-Entwicklung der Exponentialfunktion mit Rekursion für Restglied
	\begin{align}\label{eqVorueberlegungChapter9Exp}\tag{ExpTay}
		\exp(i\cdot x)=\sum\limits_{k=0}^n\frac{(i\cdot x)^k}{k!}+g_n(x)
	\end{align}
	Es gilt:
	\begin{align*}
		\int\limits_0^x g_n(y)\d y
		&=\int\limits_0^x\left(\exp(i\cdot y)-\sum\limits_{k=0}^n\frac{(i\cdot y)^k}{k!}\right)\d y\\
		&=(-i)\cdot\left(\big(\exp(i\cdot x)-1\big)-\sum\limits_{k=0}^n\frac{(i\cdot x)^{k+1}}{(k+1)!}\right)\\
		\overset{\text{Indexshift}}&=
		(-i)\cdot\underbrace{\left(\exp(i\cdot x)-\sum\limits_{k=0}^{n+1}\frac{(i\cdot x)^k}{k!}\right)}_{=g_{n+1}(x)}\\
		&=-i\cdot g_{n+1}(x)
	\end{align*}
\end{vorüberlegung}

\begin{lemma}\label{lemma9.1}
	Sei $g_n(x)$ das Restglied in \eqref{eqVorueberlegungChapter9Exp}.
	Dann gilt:
	\begin{align*}
		\big|g_n(x)\big|\leq\min\left\lbrace\frac{2\cdot|x|^n}{n!},\frac{|x|^{n+1}}{(n+1)!}\right\rbrace
	\end{align*}
	Die linke Ungleichung ist besser für großes $|x|$ und die rechte Ungleichung ist besser für $|x|$ klein.
\end{lemma}

\begin{proof}
	Induktion nach $n$.\\
	\underline{IA:} $n=0$:
	\begin{align*}
		\big|g_0(x)\big|
		&=\exp(i\cdot x)-1
		\leq\left\lbrace\begin{array}{cl}
			\big|\exp(i\cdot x)|+|1|&\leq 2\\
			\left|\int\limits_0^x\exp(i\cdot y)\d y\right|&\leq\int\limits_0^{|x|}\big|\exp(i\cdot y)\big|\d y\leq |x|
		\end{array}\right.\\
		\implies
		\big|g_0(x)\big|&\leq\min\big\lbrace 2,|x|\big\rbrace
	\end{align*}
	\underline{IS:} $n\to n+1$:
	\begin{align*}
		\big|g_n(x)\big|
		\overset{\text{VÜ}}&=
		\left|\int\limits_0^x g_n(y)\d y\right|
		\leq\int\limits_0^{|x|}\big|g_n(y)\big|\d y
		\overset{\text{IV}}\leq
		\left\lbrace\begin{array}{cl}
			\int\limits_0^{|x|}\frac{2\cdot y^{n}}{n!}\d y &=\frac{2\cdot|y|^{n+1}}{(n+1)!}\\
			\int\limits_0^{|x|}\frac{y^{n+1}}{(n+1)!}\d y &=\frac{|x|^{n+2}}{(n+2)!}
		\end{array}\right.\\
		&=\min\left\lbrace\frac{2\cdot|x|^{n+1}}{(n+1)!},\frac{|x|^{n+2}}{(n+2)!}\right\rbrace
	\end{align*}
\end{proof}

\begin{bemerkung}
	 Sei $X\in L_2(\d\P)$, $u\in\R$.
	 Dann folgt aus Lemma \ref{lemma9.1}
	 \begin{align}\label{eqBemerkungUnder9.1A}\tag{A}
	 	\Big|\E\big[g_n(u\cdot X)\big]\Big|
	 	&\leq u^2\cdot\E\left[\min\left\lbrace X^2,\frac{|X|^3\cdot|u|}{6}\right\rbrace\right]
	 \end{align}
	 Diese Abschätzung wird im Beweis der ZGS verwendet.
	 Beachte außerdem, dass die dritten Momente nicht existieren müssen, damit diese Gleichung gilt.
\end{bemerkung}

\begin{lemma}\label{lemma9.2}
	Sei $(a_n)_{n\in\N}\subseteq\R$ mit $\limn a_n=a$.
	Dann gilt
	\begin{align*}
		\limn\left(1-\frac{a_n}{n}\right)^n=\exp(-a)
	\end{align*}
\end{lemma}

\begin{proof}
	Logarithmieren liefert:	
	\begin{align*}
		\log\left(\left(1-\frac{a_n}{n}\right)^n\right)
		&=n\cdot\log\left(1-\frac{a_n}{n}\right)
		=\underbrace{a_n}_{\longrightarrow a}\cdot\frac{\overbrace{\log\left(1-\frac{a_n}{n}\right)}^{\longrightarrow0}}{\underbrace{\frac{a_n}{n}}_{\longrightarrow0}}\\
		\lim\limits_{x\to0}\frac{\log(1-x)}{x}="\frac{0}{0}"
		\overset{\text{L'Hospital}}&=
		\lim\limits_{x\to0}\frac{1}{x-1}=-1\\
	\end{align*}
	Insgesamt:
	\begin{align*}
		\limn\log\left(\left(1-\frac{a_n}{n}\right)^n\right)&=-a\\
		\implies \limn\left(1-\frac{a_n}{n}\right)^n&=\exp(-a)
	\end{align*}
\end{proof}

\begin{theorem}[ZGS von Moivre-Laplace]\label{theorem9.3ZGSMoivreLaplace}\enter
	% 1733 gezeigt von De Moivre für die Binomialverteilung
	% 1820 gezeigt von Laplace allgemeiner bewiesen
	Sei $(X_j)_{j\in\N}$ eine Folge unabhängiger reellwertiger Zufallsvariablen mit der gleichen Verteilung (i.i.d.)
	und $\E[X_1]=0$, $\Var(X_1)=\sigma^2\in(0,\infty)$.
	Dann gilt:
	\begin{align*}
		G_n&:=\frac{1}{\sigma\cdot\sqrt{n}}\cdot\big(X_1+\ldots+X_n\big)
	\end{align*}
	konvergiert in Verteilung gegen eine standardnormalverteilte ZV $G\sim\Nor(0,1)$.
\end{theorem}

\begin{proof}
	Berechne die charakteristische Funktion von $G_n$:
	\begin{align}\nonumber
		\Phi_{G_n}(u)
		&=\E\Big[\exp(i\cdot u\cdot G_n)\Big]\\\nonumber
		&=\E\left[\exp\left(\frac{i\cdot u}{\sigma\cdot\sqrt{n}}\cdot\big(X_1+\ldots+X_n\big)\right)\right]\\\nonumber
		\overset{\text{iid}}&=
		\E\left[\exp\left(\frac{i\cdot u}{\sigma\cdot\sqrt{n}}\cdot X_1\right)\right]\\\nonumber
		\overset{\eqref{eqVorueberlegungChapter9Exp}}&{=}
		\bigg(1+\frac{i\cdot u}{\sigma\cdot\sqrt{n}}\cdot\underbrace{\E[X_1]}_{=0}-\frac{u^2}{2\cdot\sigma^2\cdot n}\cdot\underbrace{\E\big[X_1^2\big]}_{=\Var(X_1)=\sigma^2}+\E\left[g_n\left(\frac{u}{\sigma\cdot\sqrt{n}}\cdot X_1\right)\right]\bigg)^n\\
		&=\Bigg(1-\underbrace{\frac{u^2}{2\cdot n}\cdot\bigg(1+\frac{2\cdot n}{u^2}\cdot\E\left[g_n\left(\frac{u}{\sigma\cdot\sqrt{n}}\cdot X_1\right)\right]\bigg)}_{=:\frac{a_n}{n}}\Bigg)^n\label{eqProof9.3Stern}\tag{$\ast$}
	\end{align}
	
	\begin{align*}
		\left|\frac{n}{u^2}\cdot\E\left[g_n\left(\frac{u}{\sigma\cdot\sqrt{n}}\cdot X_1\right)\right]\right|
		\overset{\eqref{eqBemerkungUnder9.1A}}{\leq}
		\frac{1}{\sigma^2}\cdot\underbrace{\E\left[\min\left\lbrace X_1^2,\left|\frac{u}{\sigma\cdot\sqrt{n}}\right|\cdot\frac{|X_1|^3}{6}\right\rbrace\right]}_{\overset{n\to\infty}{\longrightarrow}0\text{ mit dom. Konv.}}
	\end{align*}
	Also gilt $a_n\overset{n\to\infty}{\longrightarrow}\frac{u^2}{2}$.
	Aus Lemma \ref{lemma9.2} folgt:
	\begin{align*}
		\limn\Phi_{a_n}(u)
		&=\limn\left(1-\frac{a_n}{n}\right)
		=\exp\left(-\frac{u^2}{2}\right)
	\end{align*}
	Die Funktion $\exp\left(-\frac{u^2}{2}\right)$ ist die charakteristische Funktion von $\Nor(0,1)$ und ist stetig in 0.
	Aus dem Stetigkeitssatz von Lévy \ref{theorem8.1StetigkeitssatzVonLevy} folgt nun
	\begin{align*}
		G_n\overset{\d}{\longrightarrow} G\sim\Nor(0,1)
	\end{align*}
\end{proof}

\begin{bemerkung}
	Folgendes Beispiel zeigt was passieren kann, wenn $\E[X_1^2]=\infty$ (also die Varianz nicht endlich ist). 
\end{bemerkung}

\begin{beisp}
	Sei $(X_j)_{j\in\N}$ iid mit $X_1\sim\Cauchy$ 
	(d.h. mit Dichte $f(x)=\frac{1}{\pi}\cdot\frac{1}{1+x^2}$ und CF $\Phi(u)=\exp\big(-|u|\big)$).
	Setze
	\begin{align*}
		H_n:=\frac{1}{n}\cdot\big(X_1+\ldots+X_n\big)\qquad\text{(Beachte das im Nenner nun $\frac{1}{n}$ ohne Wurzel steht.)}
	\end{align*}
	Es gilt:
	\begin{align*}
		\Phi_{H_n}(u)
		&=\E\left[\exp\left(\frac{i\cdot u}{n}\cdot\big(X_1+\ldots+X_n\big)\right)\right]\\
		\overset{\text{iid}}&=
		\E\left[\exp\left(\frac{i\cdot u\cdot X_1}{n}\right)\right]^n\\
		&=\exp\left(-n\cdot\left|\frac{u}{n}\right|\right)\\
		&=\exp\big(-|u|\big)\\
		\implies H_n&\sim\Cauchy
	\end{align*}
	Insbesondere $H_n\overset{\d}{\longrightarrow} H\sim\Cauchy$.
	Also kann 
	\begin{align*}
		G_n=\frac{1}{\sqrt{n}}\cdot\big(X_1+\ldots+ X_n\big)=\sqrt{n}\cdot H_n
	\end{align*}
	nicht in Verteilung konvergieren.
	Der ZGS gilt also \underline{nicht}.
\end{beisp}

Nächstes Ziel: Abschwächen der Annahme der gleichen Verteilungen

\begin{theorem}[ZGS von Lindeberg-Feller]\label{theorem9.4ZGSLindebergFeller}\enter
	% 1920 von Lindeberg gezeigt
	% Feller hat DInge vereinfacht
	Sei $(X_j)_{j\in\N}$ unabhängiger reellwertiger Zufallsvariablen mit $\E[X_j]=0$ und \\$\Var(X_j)=\sigma_j^2\in(0,\infty)$.
	Setze $s_n^2:=\sigma_1^2+\ldots+\sigma_n^2$.
	Wenn die \textbf{Lindeberg-Bedingung}
	\begin{align}\label{eqTheorem9.4LindebergBedingungL}\tag{L}
		\limn\frac{1}{s_n^2}\cdot\sum\limits_{j=1}^n\E\left[X_j^2\cdot\indi_{\big\lbrace|X_j|\geq\varepsilon\cdot s_n\big\rbrace}\right]=0\qquad\forall\varepsilon>0
	\end{align}
	erfüllt ist, dann gilt:
	\begin{align*}
		G_n:=\frac{1}{s_n^2}\cdot\big(X_1+\ldots+X_n\big)\overset{\d}{\longrightarrow}G\sim\Nor(0,1)
	\end{align*}
\end{theorem}

\begin{bemerkung}
	 Die Bedingung \eqref{eqTheorem9.4LindebergBedingungL} drückt aus, dass kein $\sigma_j^2$ zu stark "dominiert".
	 Konkret gilt:
\end{bemerkung}

\begin{lemma}\label{lemma9.5}
	Die Lindebergbedingung \eqref{eqTheorem9.4LindebergBedingungL} impliziert die \textbf{Feller-Bedingung}
	\begin{align}\label{eqLemma9.5FellerbedingungF}\tag{F}
		\limn\frac{1}{s_n^2}\cdot\max\limits_{1\leq j\leq n}\sigma_j^2=0
	\end{align}
	\underline{Interpretation:}
	$s_n^2=\sigma_1^2+\ldots+\sigma_n^2$ wächst echt schneller als $\max\limits_{1\leq j\leq n}\sigma_j^2$.\nl
	Es gilt: \eqref{eqLemma9.5FellerbedingungF} + (ZGS) $\implies$ \eqref{eqTheorem9.4LindebergBedingungL}
\end{lemma}

\begin{proof}
	Sei $\varepsilon>0,n\in\N$. Für $j\in\lbrace 1,\ldots,n\rbrace$ gilt:
	\begin{align*}
		\sigma_j^2
		&=\Var(X_j)
		=\E\bigg[\underbrace{X_j^2\cdot\indi_{\big\lbrace|X_j|<\varepsilon\cdot s_n\big\rbrace}}_{\leq\varepsilon^2\cdot s_n^2}+X_j^2\cdot\indi_{\big\lbrace|X_j|\geq\varepsilon\cdot s_n\big\rbrace}\bigg]\\
		&\implies 
		\frac{\sigma_j^2}{s_n^2}
		\leq\varepsilon^2+\frac{1}{s_n^2}\cdot\E\left[X_j^2\cdot\indi_{\big\lbrace|X_j|\geq\varepsilon\cdot s_n\big\rbrace}\right]
		\overset{\text{grob}}\leq
		\varepsilon^2+\frac{1}{s_n^2}\cdot\sum\limits_{j=1}^n\E\left[X_j^2\cdot\indi_{\big\lbrace |X_j|\geq\varepsilon\cdot s_n\big\rbrace}\right]\\
		&\implies
		\frac{1}{s_n^2}\cdot\max\limits_{1\leq j\leq n}\sigma_j^2
		\leq\varepsilon^2+\underbrace{\frac{1}{s_n^2}\cdot\sum\limits_{j=1}^n\E\left[X_j^2\cdot\indi_{\big\lbrace|X_j|\geq\varepsilon\cdot s_n\big\rbrace}\right]}_{\overset{n\to\infty}{\longrightarrow}0\text{ wegen \eqref{eqTheorem9.4LindebergBedingungL}}}\\
		&\implies
		\limn\frac{1}{s_n^2}\cdot\max\limits_{1\leq j\leq n}\sigma_j^2
		\leq\varepsilon^2\qquad\forall\varepsilon>0\\
		&\implies\limn\frac{1}{s_n^2}\cdot\max\limits_{1\leq j\leq n}\sigma_j^2=0
	\end{align*}
\end{proof}

\begin{proof}[Beweis des ZGS von Lindeberg-Feller, Theorem \ref{theorem9.4ZGSLindebergFeller}]\enter
	Sei
\end{proof}

