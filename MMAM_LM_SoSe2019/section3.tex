% This work is licensed under the Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy
% of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/ or
% send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\section{Parameterschätzung in linearen Modellen}

\begin{definition}\label{def3.1}\
	\begin{enumerate}[label=(\arabic*)]
		\item Sei $Z:=\big(Z_{i,j}\big)_{\begin{subarray}{c}
			1\leq i\leq m\\
			1\leq j\leq n
		\end{subarray}}\in M(m\times n)$
		mit integrierbaren Komponenten $Z_{i,j}$, welche reelle Zufallsvariablen sind.
		Dann: \label{item:def3.1(1)}
		\begin{align*}
			\E[Z]:=\Big(\E\big[Z_{i,j}\big]\Big)_{\begin{subarray}{c}
			1\leq i\leq m\\
			1\leq j\leq n
		\end{subarray}}
		\end{align*}
		Speziell für 
		\begin{align*}
			Z=\big(Z_1,\ldots,Z_m\big)'
		\end{align*}
		ist 
		\begin{align*}
			\E[Z]=\begin{pmatrix}
				\E\big[Z_1\big]\\
				\vdots\\
				\E\big[Z_n\big]
			\end{pmatrix}=\Big(\E\big[Z_1\big],\ldots,\E\big[Z_n\big]\Big)'
		\end{align*}
		\item Seien
		\begin{align*}
			Z=\big(Z_1,\ldots,Z_m\big)',\qquad
			Y=\big(Y_1,\ldots,Y_n\big)'.
		\end{align*}		 
		Dann, falls existent
		\begin{align*}
			\Cov(Y,Z)&:=\E\Big(\big(Y-\E[Y]\big)\mal\big(Z-\E[Z]\big)'\Big)\\
			\overset{\ref{item:def3.1(1)}}&{~=}
			\klammern[\bigg]{\E\Big[\big(Y_i-\E[Y_i]\big)\mal\big(Z_j-\E[Z_j]\big)\Big]}_{\begin{subarray}{c}
			1\leq i\leq m\\
			1\leq j\leq n
		\end{subarray}}\\
		&~=\Big(\Cov(Y_i,Z_j)\Big)_{\begin{subarray}{c}
			1\leq i\leq m\\
			1\leq j\leq n
		\end{subarray}}
		\end{align*}
		Speziell ist\index{Kovarianzmatrix}
		\begin{align*}
			\Var(Y):=\Cov(Y,Y)=\Big(\Cov(Y_i,Y_j)\Big)_{\begin{subarray}{c}
			1\leq i\leq m\\
			1\leq j\leq n
		\end{subarray}}\in M(n\times n)
		\end{align*}
		die \define{Kovarianzmatrix} von $Y$.
	\end{enumerate}
\end{definition}

\begin{satz}\label{satz3.2}
	Seien $Y$ und $Z$ Zufallsvektoren in $\R^n$ bzw. $\R^m$, $A\in M(m\times n)$, $b\in\R^m$ beide deterministisch.
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item $\begin{aligned}
			\E\big[A\mal Y+Z\big]=A\mal\E[Y]+\E[Z]
		\end{aligned}\qquad$ ($\E$ ist linear)
		\label{item:satz3.2(1)}
		\item $\begin{aligned}
			\Var\big(A\mal Y+b\big)=A\mal\Var(Y)\mal A'
		\end{aligned}$
		\label{item:satz3.2(2)}
	\end{enumerate}
\end{satz}

\begin{proof}
	Nachrechnen! (Zur Übung, folgt aus Matrixmultiplikation + den Eigenschaften im reellen Fall)
	%\betone{Zeige \ref{item:satz3.2(1)}:}\\
	
	%\betone{Zeige \ref{item:satz3.2(2)}:}\\
\end{proof}

Wir betrachten jetzt das lineare Modell (vergleiche \eqref{eq:1.2}) mit 
\begin{align}\label{eq:3.1}\tag{3.1}
	Y&=X\mal\beta+\varepsilon\\
	\E[\varepsilon]&=0\label{eq:3.2}\tag{3.2}\\
	\Var(\varepsilon)&=\sigma^2\mal I_n\mit\sigma^2>0\label{eq:3.3}\tag{3.3}\\
	n&\geq p\label{eq:3.4}\tag{3.4}
\end{align}
Beachte ($\varepsilon=\big(\varepsilon_1,\ldots,\varepsilon_n\big)'$):
\begin{align*}
	\eqref{eq:3.3}&\iff\Cov(\varepsilon_i,\varepsilon_j)=\sigma^2\mal\delta_{i,j}
	&&\forall i,j\\
	&\iff\Cov\big(\varepsilon_i,\varepsilon_j\big)=0
	&&\forall i\neq j\und\Var(\varepsilon_i)=\sigma^2
\end{align*}
Also gilt: $\varepsilon_1,\ldots,\varepsilon_n$ iid $\implies\eqref{eq:3.3}$\\
($p$ ist die Anzahl der Modellparameter)
\begin{align*}
	\eqref{eq:3.1}\iff Y_i=\sum\limits_{j=1}^p X_{i,j}\mal\beta_j+\varepsilon_i\quad\forall i
\end{align*}

\subsection{Minimum-Quadrat-Schätzer}

Ziel: Schätzung von $\beta$.
Idee nach Gauß und Legendre: 
\define{Methode der kleinsten Quadrate}, nämlich:
Minimierung der \define{Fehlerquadratsumme}
\index{Fehlerquadratsumme}\index{Methode der kleinsten Quadrate}
\begin{align*}
	\sum\limits_{i=1}^n\varepsilon_i^2
	\overset{\Def}&{=}
	\norm{\varepsilon}^2
	\overset{\eqref{eq:3.1}}{=}
	\norm{Y-X\mal\beta}^2,
\end{align*}
d.h. finde $\hat{\beta}$ mit
\begin{align*}
	&\hspace{11.5mm}\norm{Y-X\mal\hat{\beta}}^2\leq\norm{Y-X\mal\beta}^2&&\forall\beta\in\R^p\\
	&\iff
	\norm{Y-X\mal\hat{\beta}}\leq\norm{Y-X\mal\beta}&&\forall\beta\in\R^p\\
	&\iff\hat{\beta}(\omega)\in\argmin\limits_{\beta\in\R^p}\norm{Y(\omega)-X\mal\beta}
\end{align*}
Die Lösung ist bekannt.
Es gilt

\begin{satz}\label{satz3.3}
	Es gelten \eqref{eq:3.1} und \eqref{eq:3.4}.
	Dann gilt:
	\begin{enumerate}[label=(\arabic*)]
		\item Die Minimalstelle $\hat{\beta}$ existiert und erfüllt die \define{Normalgleichung} \label{item:satz3.3(1)}
		\index{Normalgleichung}
		\begin{align}\label{eq:satz3.3Stern}\tag{$*$}
			X'\mal X\mal\hat{\beta}=X'\mal Y
		\end{align}
		und umgekehrt ist jede Lösung von \eqref{eq:satz3.3Stern} eine Minimalstelle.\\
		Bezeichnet $L:=\Bild(X)$, so gilt:
		\begin{align*}
			P_L(Y)=X\mal\hat{\beta}
		\end{align*}
		\item Falls $\Rg(X)=p$ (also Vollrang), so gilt:\label{item:satz3.3(2)}
		\begin{align*}
			\hat{\beta}&=\big(X'\mal X)^{-1}\mal X'\mal Y
			\qquad\und\qquad
			P_L=X\mal\big(X'\mal X\big)^{-1}\mal X'
		\end{align*}
		Der Schätzer $\hat{\beta}$ heißt \define{Minimum-Quadrat-Schätzer (MQS) /\\ Kleinst-Quadrate-Schätzer (KQS) / least squares estimator (lse)}.
		\index{Minimum-Qudrat-Schätzer}
		\index{Kleinst-Quadrate-Schätzer}
		\index{least squares estimator|see{Minimum-Quadrat-Schätzer}}
	\end{enumerate}
\end{satz}

\begin{proof}
	Nutze Satz \ref{satz2.22} mit $A\leftrightarrow X$, $x\leftrightarrow Y(\omega)$ und $y\leftrightarrow\beta$.
\end{proof}

\begin{figure}[H]
	\begin{center}
		\input{./tikz/orthoProjektionSec3}
		\caption{Minimum-Quadrat-Schätzer}
		\label{Abb:MinQuaSchätzer}
	\end{center}
\end{figure}

\begin{satz}\label{satz3.4}
	Angenommen es gelten \eqref{eq:3.1}, \eqref{eq:3.2}, \eqref{eq:3.3} und \eqref{eq:3.4}.
	Falls $\Rg(X)=p$, so gilt:
	\begin{align}\label{eq:3.5}\tag{3.5}
		\E\big[\hat{\beta}\big]&=\beta\qquad\forall\beta\in\R^p\\
		\Var\big(\hat{\beta}\big)&=\sigma^2\mal\big(X'\mal X\big)^{-1}\label{eq:3.6}\tag{3.6}
	\end{align}
\end{satz}

\begin{proof}
	\begin{align*}
		\E\big[\hat{\beta}\big]
		\overset{\ref{satz3.3}\ref{item:satz3.3(2)}}&{=}
		\E\Big[\underbrace{\big((X'\mal X)^{-1}\mal X'\big)}_{=:A}\mal Y\Big]\\
		\overset{\ref{satz3.2}\ref{item:satz3.2(1)}}&{=}
		\big(X'\mal X\big)^{-1}\mal X'\mal\E[Y]\\
		\overset{\eqref{eq:3.1}}&{=}
		\big(X'\mal X\big)^{-1}\mal X'\mal\underbrace{\E[X\mal\beta+\varepsilon]}_{
			=\underbrace{\E[X\mal\beta]}_{
				=X\mal\beta
			}+\underbrace{\E[\varepsilon]}_{
				\overset{\eqref{eq:3.2}}{=}0		
			}=X\mal\beta
		}\\
		&=\big(X'\mal X\big)^{-1}\mal\big(X'\mal X\big)\mal\beta\\
		&=I_p\beta\\
		&=\beta
	\end{align*}
	Zur Varianz:
	\begin{align*}
		\Var\big(\hat{\beta}\big)
		\overset{\ref{satz3.3}\ref{item:satz3.3(2)}}&{=}
		\Var\Big(\big(X'\mal X\big)^{-1}\mal X'\mal Y)\\
		\overset{\ref{satz3.2}\ref{item:satz3.2(2)}}&{=}
		\big(X'\mal X\big)^{-1}\mal X'\mal\Var(Y)\mal\Big(\big(X'\mal X\big)^{-1}\mal X'\Big)'\\
		&=\big(X'\mal X\big)^{-1}\mal X'\mal\underbrace{\Var(Y)}_{
			\overset{\eqref{eq:3.1}}{=}\Var(X\mal\beta+\varepsilon)
			\overset{\ref{satz3.2}\ref{item:satz3.2(2)}}{=}
			\Var(\varepsilon)
			\overset{\eqref{eq:3.3}}{=}
			\sigma^2\mal I_n
		}\mal X\mal\big(X'\mal X\big)^{-1}\\
		&=\sigma^2\mal\big(X'\mal X\big)^{-1}\mal\underbrace{\big(X'\mal X\big)\mal\big(X'\mal X\big)^{-1}}_{=I_p}\\
		&=\sigma^2\mal\big(X'\mal X\big)^{-1}
	\end{align*}
\end{proof}

Beachte, der Designmatrix $X$ kommt durch die \define{Varianzformel} \eqref{eq:3.6} eine statistische Bedeutung zu ($\leadsto$ design of experiments).
\index{Varianzformel}

\begin{beispiel}[Einfache lineare Regression]
\label{beisp3.5einfacheLineareRegression}
	\begin{align*}
		Y_i&=a+b\mal x_i+\varepsilon_i,\qquad\forall i\in\set{1,\ldots,n}
	\end{align*}

	\begin{figure}[H]
		\begin{center}
			\input{./tikz/beispiel3.5}
			\caption{Einfache lineare Regression}
			%\label{Abb:MinQuaSchätzer}
		\end{center}
	\end{figure}
	
	\begin{figure}[H]
		\begin{center}
			\input{./tikz/beispiel3.5_2}
			\caption{Einfache lineare Regression - Methode der kleinsten Fehlerquadrate}
			%\label{Abb:MinQuaSchätzer}
		\end{center}
	\end{figure}

	\begin{align*}
		\underbrace{\begin{pmatrix}
			Y_1\\
			\vdots\\
			\vdots\\
			Y_n
		\end{pmatrix}}_{=Y}
		&=\underbrace{\begin{pmatrix}
			1 & x_1\\
			\vdots & x_2\\
			\vdots & \vdots\\
			1 & x_n
		\end{pmatrix}}_{=X}\mal\underbrace{\begin{pmatrix}
			a\\
			b
		\end{pmatrix}}_{=\beta}+\underbrace{\begin{pmatrix}
			\varepsilon_1\\
			\vdots\\
			\vdots\\
			\varepsilon_n
		\end{pmatrix}}_{=\varepsilon}
	\end{align*}
	Angenommen, $\Rg(X)=2$ ($\overset{!}{\iff}\exists i\neq j:x_i\neq x_j$)
	\begin{align*}
		X'\mal X
		&=\begin{pmatrix}
			1 & \hdots & 1\\
			x_1 & \hdots & x_n
		\end{pmatrix}\mal\begin{pmatrix}
			1 & x_1\\
			\vdots &  \vdots\\
			1 & x_n
		\end{pmatrix}
		=\begin{pmatrix}
			n & \sum\limits_{i=1}^n x_i\\
			\sum\limits_{i=1}^n x_i & \sum\limits_{i=1}^n x_i^2
		\end{pmatrix}
		=\begin{pmatrix}
			n & n\mal\overline{x_n}\\
			n\mal\overline{x_n} & \sum\limits_{i=1}^n x_i^2
		\end{pmatrix}\\
		X'\mal Y
		&=\begin{pmatrix}
			1 & \hdots & 1\\
			x_1 & \hdots & x_n
		\end{pmatrix}\mal\begin{pmatrix}
			Y_1\\
			\vdots\\
			Y_n
		\end{pmatrix}
		=\begin{pmatrix}
			\sum\limits_{i=1}^n Y_i\\
			\sum\limits_{i=1}^n x_i\mal Y_i
		\end{pmatrix}
		=\begin{pmatrix}
			n\mal\overline{Y_n}\\
			\sum\limits_{i=1}^n x_i\mal Y_i
		\end{pmatrix}\\
		\mit\qquad \overline{x_n}&:=\frac{1}{n}\mal\sum\limits_{i=1}^n x_i
		\qquad\und\qquad
		\overline{Y_n}:=\frac{1}{n}\mal\sum\limits_{i=1}^n Y_i
	\end{align*}
	Normalgleichung \eqref{eq:satz3.3Stern}:
	\begin{align*}
		\begin{pmatrix}
			n & n\mal\overline{x_n}\\
			n\mal\overline{x_n} & \sum\limits_{i=1}^n x_i^2
		\end{pmatrix}\mal\begin{pmatrix}
			\alpha\\
			\beta
		\end{pmatrix}=\begin{pmatrix}
			n\mal\overline{Y_n}\\
			\sum\limits_{i=1}^n x_i\mal Y_i
		\end{pmatrix}\\
		\iff
		\left\lbrace\begin{array}{l}
			(1)~~
			n\mal\alpha+n\mal\overline{x_n}\mal\beta=n\mal\overline{Y_n}
			\iff\alpha=\overline{Y_n}-\overline{x_n}\mal\beta\\
			(2)~~n\mal\overline{x_n}\mal\alpha+\sum\limits_{i=1}^n x_i^2\mal\beta=\sum\limits_{i=1}^n x_i\mal Y_i
		\end{array}\right.
	\end{align*}
	Teilen durch $n$ in (2) und einsetzen von (1) in (2) liefert:
	\begin{align*}
		\overline{x_n}\mal \overline{Y_n}\underbrace{-\overline{x_n}^2\mal\beta+\frac{1}{n}\mal\sum\limits_{i=1}^n x_i^2\mal\beta}_{
			=\beta\mal\klammern{\frac{1}{n}\mal\sum\limits_{i=1}^n \big(x_i^2-\overline{x_n}^2\big)}
		}
		&=\frac{1}{n}\mal\sum\limits_{i=1}^n x_i\mal Y_i\\
		\implies
		\beta
		&=\frac{
			\frac{1}{n}\mal\sum\limits_{i=1}^n x_i\mal Y_i-\overline{x_n}\mal\overline{Y_n}
		}{
			\frac{1}{n}\mal\sum\limits_{i=1}^n \big(x_i^2-\overline{x_n}^2\big)
		}\\
		&=
		\frac{
			\frac{1}{n}\mal\klammern{\sum\limits_{i=1}^n x_i\mal Y_i-n\mal\overline{x_n}\mal\overline{Y_n}}
		}{
			\frac{1}{n}\mal\sum\limits_{i=1}^n \big(x_i^2-\overline{x_n}^2\big)
		}\\
		\overset{\mal\frac{n}{n-1}}&{=}
		\frac{
			\frac{1}{n-1}\mal\klammern{\sum\limits_{i=1}^n x_i\mal Y_i-n\mal\overline{x_n}\mal\overline{Y_n}}		
		}{
			\frac{1}{n-1}\mal\klammern{\sum\limits_{i=1}^n x_i^2-n\mal \overline{x_n}^2}
		}\\
		\overset{!}&{=}
		\frac{
			\frac{1}{n-1}\mal\sum\limits_{i=1}^n\big(x_i-\overline{x_n}\big)\mal\big(Y_i-\overline{Y_n}\big)
		}{
			\frac{1}{n-1}\mal \sum\limits_{i=1}^n\big(x_i-\overline{x_n}\big)^2
		}\\
		&=:\frac{s_{x,Y}}{s_x^2}
	\end{align*}
	Hierbei heißen $s_{x,Y}$ \define{empirische Kovarianz} und $s_x^2$ \define{empirische Varianz}.
	\index{empirische Varianz}\index{empirische Kovarianz}
	\begin{align*}
		\hat{\beta}=\begin{pmatrix}
			\hat{a}\\
			\hat{b}
		\end{pmatrix}\qquad\mit\qquad
		\hat{a}:=\overline{Y_n}-\overline{x_n}\mal\hat{b},\qquad
		\hat{b}:=\frac{s_{x,Y}}{s_x^2}
	\end{align*}
\end{beispiel}







